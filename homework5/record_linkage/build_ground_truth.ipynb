{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !DA CONTINUARE SOLO A NORMALIZZAZIONE COMPLETATA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rapidfuzz in /home/hermann/Desktop/IngegneriaDeiDatiProject/.venv/lib/python3.12/site-packages (3.12.1)\n",
      "Requirement already satisfied: python-Levenshtein in /home/hermann/Desktop/IngegneriaDeiDatiProject/.venv/lib/python3.12/site-packages (0.26.1)\n",
      "Requirement already satisfied: Levenshtein==0.26.1 in /home/hermann/Desktop/IngegneriaDeiDatiProject/.venv/lib/python3.12/site-packages (from python-Levenshtein) (0.26.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /home/hermann/Desktop/IngegneriaDeiDatiProject/.venv/lib/python3.12/site-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.12.1)\n"
     ]
    }
   ],
   "source": [
    "# deps\n",
    "!pip install rapidfuzz\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rapidfuzz\n",
    "`rapidfuzz` è una libreria Python per la **fuzzy matching**, simile a `fuzzywuzzy`, ma più veloce e più efficiente perché scritta in C++. Il **fuzzy matching** è una tecnica di matching approssimativo che confronta due stringhe per determinare quanto sono simili, anche se non sono esattamente uguali.\n",
    "\n",
    "Il Fuzzy Matching utilizza metriche di similarità testuale come:\n",
    "\n",
    "+ **Levenshtein Distance** (distanza di edit)\n",
    "+ **Jaro-Winkler Similarity**\n",
    "+ **Token-based Matching** (ignora l'ordine delle parole)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarità: 95.65217391304348%\n"
     ]
    }
   ],
   "source": [
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Distanza Levenshtein\n",
    "str1 = \"Google Inc.\"\n",
    "str2 = \"Gooogle Inc.\"\n",
    "\n",
    "similarity = fuzz.ratio(str1, str2)\n",
    "print(f\"Similarità: {similarity}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Sort Ratio: 100.0%\n"
     ]
    }
   ],
   "source": [
    "similarity = fuzz.token_sort_ratio(\"International Business Machines\", \"Machines International Business\")\n",
    "print(f\"Token Sort Ratio: {similarity}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Google Inc.', 83.07692307692308, 0)\n"
     ]
    }
   ],
   "source": [
    "#Trova la Migliore Corrispondenza in una Lista\n",
    "from rapidfuzz import process\n",
    "\n",
    "choices = [\"Google Inc.\", \"Amazon LLC\", \"Microsoft Corp.\", \"Apple Ltd.\"]\n",
    "query = \"Gooogle\"\n",
    "\n",
    "best_match = process.extractOne(query, choices)\n",
    "print(best_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Costruzione della Ground Truth**\n",
    "\n",
    "1. Selezionare coppie candidate per il matching\n",
    "2. Selezionare coppie di aziende che non corrispondono\n",
    "3. Bilanciare la distribuzione dei casi facili e difficili\n",
    "4. Validare manualmente un sottoinsieme delle coppie\n",
    "5. Salvare la ground-truth in un formato utilizzabile per il training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Selezionare Coppie Candidate per il Matching\n",
    "\n",
    "Dobbiamo creare un insieme di coppie di aziende che potrebbero essere la stessa entità.\n",
    "Utilizziamo una combinazione di blocking e similarità fuzzy su più attributi (le strategie di blocking non sono quelle definitive che useremo nella fase di record linkage)\n",
    "\n",
    "### Blocking\n",
    "- Matching su nomi\n",
    "- Matching su città\n",
    "### Similarità\n",
    "- Nomi simili, usando **Jaccard, Levenshtein, Jaro-Winkler**\n",
    "- Sede operativa\n",
    "- Partita iva o codici identificativi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6232/3544164557.py:7: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  companies_df = pd.read_csv(AZIENDE_CSV)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from rapidfuzz import fuzz\n",
    "from tqdm import tqdm\n",
    "\n",
    "AZIENDE_CSV = '../aziende_normalizzate.csv'\n",
    "companies_df = pd.read_csv(AZIENDE_CSV)\n",
    "\n",
    "# Drop duplicates and missing values in key fields\n",
    "companies_df = companies_df.dropna(subset=['company_name'])\n",
    "\n",
    "companies_df['city'] = companies_df['city'].fillna('unknown').str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Blocking by `city`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_city_df = companies_df[companies_df['city'] != 'unknown']\n",
    "unknown_city_df = companies_df[companies_df['city'] == 'unknown']\n",
    "# sample unknown city df to reduce space complexity\n",
    "unknown_city_df = unknown_city_df.sample(min(10000, len(unknown_city_df)), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate pairs after limited unknown-city matching: 81753\n"
     ]
    }
   ],
   "source": [
    "unknown_groups = unknown_city_df.groupby(unknown_city_df['company_name'].str[:3])\n",
    "\n",
    "candidate_pairs_unknown = []\n",
    "for _, group in unknown_groups:\n",
    "    if len(group) > 1:\n",
    "        company_pairs = list(combinations(group.itertuples(index=False), 2))\n",
    "        candidate_pairs_unknown.extend(company_pairs)\n",
    "print(f\"Candidate pairs after limited unknown-city matching: {len(candidate_pairs_unknown)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidate pairs after optimization: 522007\n"
     ]
    }
   ],
   "source": [
    "city_groups = valid_city_df.groupby('city')\n",
    "candidate_pairs = []\n",
    "\n",
    "for _, group in city_groups:\n",
    "    if len(group) > 1:\n",
    "        company_pairs = list(combinations(group.itertuples(index=False), 2))\n",
    "        candidate_pairs.extend(company_pairs)\n",
    "\n",
    "all_candidate_pairs = candidate_pairs + candidate_pairs_unknown\n",
    "print(f\"Total candidate pairs after optimization: {len(all_candidate_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Fuzzy Matching sul campo `company_name`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Similarities: 100%|██████████| 522007/522007 [00:17<00:00, 29328.16it/s]\n"
     ]
    }
   ],
   "source": [
    "scored_pairs = []\n",
    "for c1, c2 in tqdm(all_candidate_pairs, desc=\"Computing Similarities\"):\n",
    "    similarity = fuzz.token_sort_ratio(c1.company_name, c2.company_name)\n",
    "    \n",
    "    c1_dict = c1._asdict()\n",
    "    c2_dict = c2._asdict()\n",
    "\n",
    "    c1_dict = {f\"c1.{key}\": value for key, value in c1_dict.items()}\n",
    "    c2_dict = {f\"c2.{key}\": value for key, value in c2_dict.items()}\n",
    "\n",
    "    combined_record = {**c1_dict, **c2_dict, \"similarity_score\": similarity}\n",
    "    scored_pairs.append(combined_record)\n",
    "\n",
    "candidate_pairs_df = pd.DataFrame(scored_pairs)\n",
    "candidate_pairs_df = candidate_pairs_df.sort_values(by=\"similarity_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size: 162 (Target: 250)\n",
      "High confidence: 62\n",
      "Moderate confidence: 70\n",
      "Low confidence: 30\n"
     ]
    }
   ],
   "source": [
    "HIGH_SIMILARITY_THRESHOLD = 85\n",
    "MODERATE_SIMILARITY_THRESHOLD = 65\n",
    "TOTAL_SAMPLES = 250\n",
    "\n",
    "# easy cases\n",
    "high_confidence_matches = candidate_pairs_df[candidate_pairs_df['similarity_score'] >= HIGH_SIMILARITY_THRESHOLD]\n",
    "\n",
    "# medium cases\n",
    "moderate_confidence_matches = candidate_pairs_df[\n",
    "    (candidate_pairs_df['similarity_score'] >= MODERATE_SIMILARITY_THRESHOLD) &\n",
    "    (candidate_pairs_df['similarity_score'] < HIGH_SIMILARITY_THRESHOLD)\n",
    "]\n",
    "\n",
    "# hard cases\n",
    "low_confidence_matches = candidate_pairs_df[candidate_pairs_df['similarity_score'] < MODERATE_SIMILARITY_THRESHOLD]\n",
    "\n",
    "# Choosing sample size\n",
    "high_sample_size = min(150, len(high_confidence_matches))\n",
    "moderate_sample_size = min(70, len(moderate_confidence_matches))\n",
    "low_sample_size = min(30, len(low_confidence_matches))\n",
    "\n",
    "# sample\n",
    "high_confidence_samples = high_confidence_matches.sample(high_sample_size, random_state=19)\n",
    "moderate_confidence_samples = moderate_confidence_matches.sample(moderate_sample_size, random_state=42)\n",
    "low_confidence_samples = low_confidence_matches.sample(low_sample_size, random_state=99)\n",
    "\n",
    "\n",
    "# combine selected matches\n",
    "ground_truth_candidates = pd.concat([high_confidence_samples, moderate_confidence_samples ,low_confidence_samples])\n",
    "ground_truth_candidates[\"is_match\"] = 1\n",
    "\n",
    "print(f\"Final dataset size: {len(ground_truth_candidates)} (Target: {TOTAL_SAMPLES})\")\n",
    "print(f\"High confidence: {len(high_confidence_samples)}\")\n",
    "print(f\"Moderate confidence: {len(moderate_confidence_samples)}\")\n",
    "print(f\"Low confidence: {len(low_confidence_samples)}\")\n",
    "\n",
    "# store to csv\n",
    "ground_truth_candidates.to_csv('ground_truth.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching: 223\n",
      "rows: 505\n"
     ]
    }
   ],
   "source": [
    "ground_truth_df = pd.read_csv('ground_truth.csv')\n",
    "\n",
    "count_match = ground_truth_df['is_match'].eq(True).sum()\n",
    "print(f\"matching: {count_match}\")\n",
    "print(f\"rows: {len(ground_truth_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505\n",
      "504\n"
     ]
    }
   ],
   "source": [
    "all_cols = ground_truth_df.columns.tolist()\n",
    "all_cols.remove('is_match')  # remove the 'is_match' column\n",
    "\n",
    "print(len(ground_truth_df))\n",
    "ground_truth_df.drop_duplicates(subset=all_cols, inplace=True)\n",
    "print(len(ground_truth_df))\n",
    "ground_truth_df.to_csv('ground_truth.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

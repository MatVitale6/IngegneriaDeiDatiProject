{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os\n",
    "import random\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "JSON_DIR = '../lucenehw/urls_htmls_tables/all_tables'\n",
    "DEST_DIR = 'improved_json/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empty JSON remover\n",
    "This function deletes useless empty json file.\n",
    "It checks if the file is empty or if the only content is a field `INFO` that usually says `\"There are no tables in this article\"`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define JSON to sample here\n",
    "Random sampling\n",
    "\n",
    "Rerun multiple times this section to change sampled files, otherwise never run again this section if you're working with this first specific sampled files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2110.02177.json',\n",
       " '2110.02998.json',\n",
       " '2409.02064.json',\n",
       " '2211.00735.json',\n",
       " '2004.09817.json',\n",
       " '2007.05453.json',\n",
       " '2203.05931.json',\n",
       " '2403.03017.json',\n",
       " '2102.05561v1.json',\n",
       " '2405.15632.json',\n",
       " '2405.13560v1.json',\n",
       " '2303.11745.json',\n",
       " '2406.02488.json',\n",
       " '2105.05883.json',\n",
       " '2207.11325.json',\n",
       " '2009.12999.json',\n",
       " '2406.00431.json',\n",
       " '2210.00884.json',\n",
       " '2409.11169.json',\n",
       " '2409.13171.json',\n",
       " '2309.15659.json',\n",
       " '2103.02964.json',\n",
       " '2001.11359.json',\n",
       " '2410.10349.json',\n",
       " '2402.10280.json',\n",
       " '2006.14591.json',\n",
       " '2403.12666.json',\n",
       " '2202.03575.json',\n",
       " '2405.19883.json',\n",
       " '2402.01030.json',\n",
       " '2402.09795.json',\n",
       " '2310.20062.json',\n",
       " '2410.10594.json',\n",
       " '2410.05930.json',\n",
       " '2211.14115.json',\n",
       " '2112.02829.json',\n",
       " '2401.16788.json',\n",
       " '2402.07295.json',\n",
       " '2403.10000.json',\n",
       " '161_arXiv2406.13840.json',\n",
       " '2404.19205v1.json',\n",
       " '2409.15890.json',\n",
       " '2312.14219.json',\n",
       " '2307.02499v1.json',\n",
       " '2309.14176.json',\n",
       " '2309.09115.json',\n",
       " '2406.10718v1.json',\n",
       " '2405.13025v2.json',\n",
       " '2207.07444.json',\n",
       " '2208.08207.json']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = [file for file in os.listdir(JSON_DIR) if os.path.isfile(os.path.join(JSON_DIR, file))]\n",
    "\n",
    "sampled_files = random.sample(all_files, min(50, len(all_files)))\n",
    "sampled_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_empty_json(input_path):\n",
    "    try:\n",
    "        with open(input_path, 'r') as infile:\n",
    "            data = json.load(infile)\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            if not data or set(data.keys()) == {\"INFO\"}:\n",
    "                os.remove(input_path)\n",
    "                \n",
    "                print(f\"Deleted empty file: {input_path}\")\n",
    "                return \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Unexpected error in decoding JSON {input_path}, {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {input_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in {input_path}, {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS TO DESIRED FOLDER\n",
    "for filename in all_files:\n",
    "    delete_empty_json(os.path.join(JSON_DIR, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Fixxer \n",
    "This function correct the bad formatted json, some json has this problem:\n",
    "```json\n",
    "{\n",
    "    ... data here\n",
    "}\n",
    "{\n",
    "    ... data here\n",
    "}\n",
    "{\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "when the correct formatting should be:\n",
    "\n",
    "```json\n",
    "{\n",
    "    ... data here\n",
    "    ,\n",
    "    ... data here\n",
    "    ,\n",
    "    ... data here\n",
    "}\n",
    "```\n",
    "\n",
    "So the replacement from `'}{'` to `','` is exactly what the function does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to easy fix bad formatting of json file\n",
    "def fix_json(input_path, output_path):\n",
    "    try:\n",
    "        with open(input_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        fixed_content = content.replace('}{', ',')\n",
    "\n",
    "        with open(output_path, 'w') as file:\n",
    "            file.write(fixed_content)\n",
    "        \n",
    "        print(f\"Fixed JSON saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in {input_path}, {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in all_files:\n",
    "    input_file = os.path.join(JSON_DIR, filename)\n",
    "    output_file = os.path.join(JSON_DIR, filename)\n",
    "    try:\n",
    "        with open(input_file, 'rb') as i:\n",
    "            data = json.load(i)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"error decoding this JSON file: {filename}, {e} \")\n",
    "        fix_json(input_file, output_path=output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = r\"^(S|A)[a-zA-Z0-9]*\\d+\\.T[a-zA-Z0-9]*\\d+(\\..*)?$|^global_footnotes$\"\n",
    "pattern2 = r\"^id_table_\\d+$\"\n",
    "pattern3 = r'id=\"[SA][a-zA-Z0-9]*\\d+\\.T[a-zA-Z0-9]*\\d+.*?\"'\n",
    "pattern4 = r\"^PAPER'S NUMBER OF TABLES$\"\n",
    "\n",
    "# this function unrolls recursively lists, used for case when data[key]['table'] is a nested list\n",
    "def process_element(element):\n",
    "    if isinstance(element, str):\n",
    "        if not (re.match(pattern1, element) or re.findall(pattern3, element)):\n",
    "            return True\n",
    "    elif isinstance(element, list):\n",
    "        for sub_element in element:\n",
    "            if process_element(sub_element):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def process_json(data):\n",
    "    keys_to_remove = []\n",
    "\n",
    "    for key in data.keys():\n",
    "        if not re.match(pattern1, key) and not re.match(pattern2, key):\n",
    "            keys_to_remove.append(key)\n",
    "            \n",
    "        if re.match(pattern4, key):\n",
    "            keys_to_remove.append(key)\n",
    "\n",
    "        if re.match(pattern2, key):\n",
    "            if isinstance(data[key]['table'], str):\n",
    "                if not re.match(pattern1, data[key]['table']) and not re.findall(pattern3, data[key]['table']):\n",
    "                    keys_to_remove.append(key)\n",
    "\n",
    "            elif isinstance(data[key]['table'], list):\n",
    "                # same thing as before but you're searching in a list\n",
    "                if process_element(data[key]['table']):\n",
    "                    keys_to_remove.append(key)\n",
    "                    print(f\"key '{key}' added to removal list\")\n",
    "               \n",
    "    if len(keys_to_remove) > 0:\n",
    "        for key in keys_to_remove:\n",
    "            if key in data:\n",
    "                \n",
    "                if key == \"PAPER'S NUMBER OF TABLES\" or key == \"INFO\":\n",
    "                    del data[key]\n",
    "                    continue\n",
    "\n",
    "                if isinstance(data[key], dict):\n",
    "                    print(f\"Key to delete: {key}\")\n",
    "                    print(f\"Value: {data[key]['table']}\")\n",
    "                else:\n",
    "                    print(f\"Key to delete: {key}\")\n",
    "                    print(f\"Value: {data[key]}\")\n",
    "\n",
    "                input(\"Press enter to delete\")\n",
    "                del data[key]\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2110.02177.json\n",
      "2110.02998.json\n",
      "2409.02064.json\n",
      "2211.00735.json\n",
      "2004.09817.json\n",
      "2007.05453.json\n",
      "2203.05931.json\n",
      "2403.03017.json\n",
      "2102.05561v1.json\n",
      "2405.15632.json\n",
      "2405.13560v1.json\n",
      "2303.11745.json\n",
      "2406.02488.json\n",
      "2105.05883.json\n",
      "2207.11325.json\n",
      "2009.12999.json\n",
      "2406.00431.json\n",
      "2210.00884.json\n",
      "2409.11169.json\n",
      "2409.13171.json\n",
      "2309.15659.json\n",
      "2103.02964.json\n",
      "2001.11359.json\n",
      "2410.10349.json\n",
      "2402.10280.json\n",
      "2006.14591.json\n",
      "2403.12666.json\n",
      "Key to delete: p3.1\n",
      "Value: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key to delete: A4.EGx1\n",
      "Value: None\n",
      "Key to delete: S3.E4\n",
      "Value: None\n",
      "Key to delete: S4.E5\n",
      "Value: None\n",
      "2202.03575.json\n",
      "2405.19883.json\n",
      "2402.01030.json\n",
      "2402.09795.json\n",
      "2310.20062.json\n",
      "2410.10594.json\n",
      "2410.05930.json\n",
      "2211.14115.json\n",
      "2112.02829.json\n",
      "2401.16788.json\n",
      "2402.07295.json\n",
      "2403.10000.json\n",
      "161_arXiv2406.13840.json\n",
      "2404.19205v1.json\n",
      "2409.15890.json\n",
      "Key to delete: id_table_1\n",
      "Value: S3.E2\n",
      "Key to delete: id_table_2\n",
      "Value: S3.E3\n",
      "2312.14219.json\n",
      "2307.02499v1.json\n",
      "2309.14176.json\n",
      "2309.09115.json\n",
      "2406.10718v1.json\n",
      "2405.13025v2.json\n",
      "2207.07444.json\n",
      "2208.08207.json\n"
     ]
    }
   ],
   "source": [
    "for filename in sampled_files:\n",
    "    input_file = os.path.join(JSON_DIR, filename)\n",
    "    output_file = os.path.join(DEST_DIR, filename)\n",
    "    with open(input_file, 'rb') as i:\n",
    "        data = json.load(i)\n",
    "\n",
    "        if process_json(data):\n",
    "            if len(data.keys()) > 0:\n",
    "                with open(output_file, 'w') as o:\n",
    "                    json.dump(data, o, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S2.E1', 'S1.F1', 'A3.E5', 'A3.F4.E1']\n",
      "['id_figure_1']\n",
      "['id=\"S1.F1.5.1.1\"', 'id=\"S3.Ex3.m1.1.2.3.2.1\"']\n",
      "['<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle\" id=\"S1.F1.5\">\\n<tr class=\"ltx_tr\" id=\"S1.F1.5.1\">\\n<td class=\"ltx_td ltx_align_center\"', 'S4.E7', 'A6.EGx1']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "string1 = ['S2.E1', 'S1.F1', 'A3.E5', 'A3.F4.E1', 'S12.T1', 'S1.T3.1.2', 'A1.T1', 'Sx4.T5.1.m.2']\n",
    "string2 = ['id_table_1', 'id_figure_1', 'id_table_12']\n",
    "string3 = ['id=\"S1.T1\"', 'id=\"A1.T2\"', 'id=\"S2.T1.1\"', \"<table id=\\\"S5.T2.1\\\"\", \"id=\\\"Sx1.T1.4.4\\\"\", \"id=\\\"S1.F1.5.1.1\\\"\", \"id=\\\"S3.Ex3.m1.1.2.3.2.1\\\"\"]\n",
    "string4 = [\"<table class=\\\"ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle\\\" id=\\\"S1.F1.5\\\">\\n<tr class=\\\"ltx_tr\\\" id=\\\"S1.F1.5.1\\\">\\n<td class=\\\"ltx_td ltx_align_center\\\"\",\n",
    "           \"<table id=\\\"S1.T1.1\\\" class=\\\"ltx_tabular ltx_guessed_headers ltx_align_middle\",\n",
    "           \"S2.T1.1.1\", \"S4.E7\", \"A6.EGx1\"\n",
    "           ]\n",
    "\n",
    "matches1 = [s for s in string1 if not re.match(pattern1, s)]\n",
    "matches2 = [s for s in string2 if not re.match(pattern2, s)]\n",
    "matches3 = [s for s in string3 if not re.findall(pattern3, s)]\n",
    "matches4 = [s for s in string4 if not re.match(pattern1, s) and not re.findall(pattern3, s)]\n",
    "\n",
    "print(matches1)\n",
    "print(matches2)\n",
    "print(matches3)\n",
    "print(matches4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 'Simple Valid String' passed.\n",
      "Test 'Simple Invalid String' passed.\n",
      "Test 'Valid Nested List' passed.\n",
      "Test 'Nested List with Invalid Element' passed.\n",
      "Test 'Deeply Nested List with Invalid Element' passed.\n",
      "Test 'Valid Element Matching pattern3' passed.\n",
      "Test 'Mixed List with Invalid Element' passed.\n",
      "Test 'Empty List' passed.\n",
      "Test 'List with Only Valid Nested Lists' passed.\n",
      "Test 'List with Deeply Nested Invalid String' passed.\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    (\"Simple Valid String\", \"S5.T2.1\", False),\n",
    "    (\"Simple Invalid String\", \"<table id=\\\"A4.E2.2.m.11\\\"\", True),\n",
    "    (\"Valid Nested List\", [\"S5.T2.1\", \"S6.T3.1\"], False),\n",
    "    (\"Nested List with Invalid Element\", [\"S5.T2.1\", \"invalid_element\"], True),\n",
    "    (\"Deeply Nested List with Invalid Element\", [\"S5.T2.1\", [\"S6.T3.1\", \"invalid_element\"]], True),\n",
    "    (\"Valid Element Matching pattern3\", 'id=\"S5.T2.1\"', False),\n",
    "    (\"Mixed List with Invalid Element\", ['id=\"S5.T2.1\"', \"invalid_element\"], True),\n",
    "    (\"Empty List\", [], False),\n",
    "    (\"List with Only Valid Nested Lists\", [[\"S5.T2.1\", \"S6.T3.1\"], [\"S7.T4.2\", \"S8.T5.3\"]], False),\n",
    "    (\"List with Deeply Nested Invalid String\", [[\"S5.T2.1\", \"S6.T3.1\"], [\"S7.T4.2\", \"invalid_element\"]], True),\n",
    "]\n",
    "\n",
    "for name, element, expected in test_cases:\n",
    "    result = process_element(element)\n",
    "    assert result == expected, f\"Test '{name}' failed: expected {expected}, got {result}\"\n",
    "    print(f\"Test '{name}' passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty improved_json folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm improved_json/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

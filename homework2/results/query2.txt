Query: precision score on federated learning

Table: 2207.08869:S5.T2
Caption: Table 2: Averaged precision for each coarse-grained class. C, FL, PFL stands for centralized, federated and private federated learning. R and F stands for training from scratch and fine-tuning. Columns are sorted by decreasing order of class frequency.
References: Table 2 summarizes averaged precision scores on FLAIR test set for each class in the coarse-grained taxonomy.
The performances are different for different classes and there is a positive correlation between the frequency of the class and its performance.
Noticeably, the gaps between classes are enlarged if models are trained with federated learning and DP.
For instance, the gap between recreation and outdoor is about 68% in centralized setting while the gap increases to 81% in the federated setting and 96% in the federated setting with DP.
In other words, the decrease in performance is worse for classes that are less frequent in federated learning, especially when DP is applied. This observation was also noted in prior works [5, 38].

-----------------------------------------

Table: 2404.13318:S5.T3
Caption: Table 3: Federated Learning performance using our precision-based method and three baselines for selecting participating clients.
Results are derived from the average of three seeds and present the mean macro AUROC across 12 clinical tasks. The results are averaged across the four federated learning algorithms, FedAvg, FedProx, FedBN, and FedPxN. Bold values indicate instances where the performance aligns with full client participation (i.e., 5 clients) performance. Asterisk indicates instances where the metric performance aligns with best performance. 
References: As discussed earlier, increased number of clients in federated learning incurs elevated costs for the host.
Therefore, it is crucial to optimize client participation by excluding clients that may potentially contribute minimally or detrimentally to the host’s performance.
In this section, we investigate the feasibility of maintaining federated learning performance with fewer participating clients, specifically when appropriate clients are selected.
Furthermore, we employ our precision-based method for selecting participating clients, and compare it’s efficacy against other baseline metrics of measuring client distance. tab:baselines presents the federated learning results for each host based on the total number of participating clients, with clients selected using our precision-based method and other baseline metrics (i.e., cosine similarity, euclidean distance, and KL-divergence).
Cosine similarity and euclidean distance are computed as the averaged patient-wise similarity and distance using the host and subject patient latent vectors, aligned to that of  Elhussein and Gürsoy ( 2023 )  and  Huang et al. ( 2019 ) , respectively.
KL-divergence is measured between the softmax-normalized latent vectors of the host and subject.
When selecting participating clients, we exclude the top clients with lowest precision scores for precision-based selection and the top clients with furthest distance values for the other baseline metrics respectively.
Additionally, we perform federated learning across all possible client combinations, and present the  random  and  best  performance.
 Random  indicates the average performance across all combinations, implying the performance achievable when participating clients are randomly selected, and  best  indicates the highest performance obtained, indicating the upper-bound performance.
The precision scores for each host-subject pair are depicted in  \figureref fig:precision_scores. The results show that in 14 out of 15 cases (across 3 client count settings with 5 clients each), the  best  performance with reduced number of participating clients aligns with that of when all clients are involved (i.e., 5-client performance).
Such results demonstrate that in some cases, reducing the number of participants without compromising model performance is possible, which underscores the need for selecting participating clients.
Notably, among the 14 cases where  best  performance aligns with the full client participation performance, there are 11 cases in which the  random  performance is lower than and does not align with the full client performance.
Such results highlights the importance of carefully selecting suitable clients for the host. In evaluating the number of cases where client-selected performance aligns with the  best  performance, the results from employing precision, cosine similarity, euclidean distance, and KL-divergence for selection each results in 9, 3, 6, and 5 aligned cases respectively.
This indicates that our precision-based method outperforms other baseline metrics in aligning with the upper-bound performance, highlighting the efficacy of our method in selecting appropriate participating clients for the host.
Furthermore, when selecting participating clients with our precision-based method, 14 out of 15 cases achieve equal or higher performance compared to the full client participation performance.
Such results suggests that our precision-based method can maintain the performance achieved with full client engagement while reducing the number of participating clients. To ensure the robustness of our precision-based method across different datasets, we construct a new set of clients and compare the federated learning results when clients are selected using our precision-based method and the other baselines.
The results, detailed in  \appendixref apd:compare_precision, consistently corroborate our analysis for observations in  \tableref tab:baselines, confirming the necessity of client selection and the effectiveness of our precision-based method.

-----------------------------------------

Table: 2405.00418:S3.T3
Caption: Table 3: Classification Report of Federated Learning 
References: Table 3 shows the precision, recall, and F1-score for the CNN model using federated learning.
For precision and recall, the detection scores for normal and Ransomeware attacks achieved identical scores of 92% and 100%, respectively. Similarly, recall scores of 100% and 92%, respectively, are obtained for the normal and Ransomeware attacks. The F1-Score is 96%, which proves the effectiveness of the proposed CNN model using Federated learning.

-----------------------------------------

Table: 2107.00881:S4.T4
Caption: Table 4: Comparison of approaches per label score for n = 4 (FL refers to Federated Learning and Seg-FL refers to Segmented-Federated Learning)
References: The results obtained by 4 participants/workers through centralized, federated, and Segmented-FL approaches are shown in table 3. In table 4, we compare results obtained by the respective approaches per label across precision, recall and F1 score. For simplicity, we compare the average of the scores obtained by the workers across the metric.

-----------------------------------------

Table: 2101.07511:S4.T3
Caption: TABLE III: COMPARISON AGAINST THE TWO BASELINES IN TERMS OF PRECISION, RECALL, AND F1-SCORE. Promising results are obtained by CFL, outperforming the conventional FL while slightly lower performance is obtained compared to Central Baseline with the added advantage of improved privacy and data security. ∗A separate model is trained in federated learning settings for each modality.
References: Table III and Figure 4 provides the experimental results per class and overall (per dataset) results, respectively, in terms of precision, recall, and F1-Score. Since the data set is not balanced, so we believe, alone accuracy is not enough to evaluate the proposed method. For performance evaluation of the three experimental setups (i.e., the two baselines and CFL), we kept the similar experimental setup where we first train the baseline models with a batch size of 16 (for each modality) and then we train the same model in CFL fashion (i.e., using multi-modal settings) with 5 epochs of local training with a batch size of 16. Then we evaluated the collaboratively trained model with the test data from each cluster (modality), i.e., X-ray and Ultrasound. As can be seen in the Figure 4, overall comparable results are observed for multi-modal model trained using CFL compared with the specialized two models trained in a conventional FL environment using X-ray and Ultrasound imagery separately. On the other hand, we can see that CFL performance is considerably better than the performance of multi-modal model trained in a conventional federated learning environment. Moreover, it is evident from the figure that a collaboratively trained model is capable of recognizing the test of images from different modalities without having explicit knowledge about these modalities. Moreover, overall better results are obtained on ultrasound images (Figure 4(b)) compared to X-ray imagery (Figure 4(a)) for all models.

-----------------------------------------

Table: 2404.13318:A5.T9
Caption: Table 9: Federated Learning performance using our precision-based method and three baselines for selecting participating clients.
Results are derived from the average of three seeds and present the mean macro AUROC across 12 clinical tasks. The results are averaged across the four federated learning algorithms, FedAvg, FedProx, FedBN, and FedPxN. Bold values indicate instances where the performance aligns with full client participation (i.e., 5 clients) performance. Asterisk indicates instances where the metric performance aligns with best performance. 
References: This section evaluates our precision-based method on a newly constructed set of clients.
We construct five distinct clients using the northeast and midwest regions of the eICU dataset, each client representing a different hospital: eICU-264, eICU-300, eICU-338, eICU-420, and eICU-73.
The selection of data for each client follows the criteria outlined in Section  5.2 , with the demographics and label distributions of each client listed in  \appendixref apd:stat. tab:baselines_eicu presents the federated learning results for each host based on the total number of participating clients, with clients selected using our precision-based method and other baseline metrics, with the metrics aligned to that elaborated in  \sectionref refined.
The precision scores for each host-subject pair are depicted in  \figureref fig:precision_scores_eicu. The results show that in 8 out of 15 cases (across 3 client count settings with 5 clients each), the  best  performance with reduced number of participating clients aligns with that of when all clients are involved (i.e., 5-client performance).
Such results demonstrate that in some cases, reducing the number of participants without compromising model performance is possible, which underscores the need for selecting participating clients.
Notably, among the 8 cases where  best  performance aligns with the full client participation performance, there are 5 cases in which the  random  performance is lower than and does not align with the full client performance.
Such results highlights the importance of carefully selecting suitable clients for the host. In evaluating the number of cases where client-selected performance aligns with the  best  performance, the results from employing precision, cosine similarity, euclidean distance, and KL-divergence for selection each results in 5, 4, 3 and 2 aligned cases respectively.
This indicates that our precision-based method outperforms other baseline metrics in aligning with the upper-bound performance, highlighting the efficacy of our method in selecting appropriate participating clients for the host.
Furthermore, when selecting participating clients with our precision-based method, 6 out of 15 cases achieve equal or higher performance compared to the full client participation performance.
Such results suggests that our precision-based method can maintain the performance achieved with full client engagement while reducing the number of participating clients.

-----------------------------------------

Table: 2407.21282:S4.T1
Caption: TABLE I: COMPARISON OF FEDERATED LEARNING WITH CENTRALIZED TRAINING ON HHAR 
References: Table I displays that all five federated learning strategies outperformed the centralized training approach on HHAR. Notably, FedAvgM achieved superior performance not only compared to centralized training but also excelled over the other strategies under various hidden unit settings. Moreover, with 128 hidden units, FedProx delivered Precision, Recall, and F1 scores of 80.78%, 81.10%, and 80.81% respectively, showing a significant improvement over the other three federated strategies. With 256 hidden units, FedAvg surpassed the others, with increases of 5.84%, 5.85%, and 5.90% in the three metrics respectively. Therefore, when implementing DeepConvLSTM on a single layer using one of these five federated learning strategies, the overall performance on the HHAR dataset exceeded that of the centralized approach, with specific outcomes depending on the number of hidden units.

-----------------------------------------

Table: 2404.13318:S5.T2
Caption: Table 2: 
Correlation between precision (and recall) with performance delta in two-client federated learning scenarios. Each correlation value is derived from 60 data points, representing 20 host-subject combinations across three seeds.

References: In this section, we present the federated learning results using our EHRFL framework, incorporating five distinct clients with heterogeneous EHRs, specifically MIMIC-III-CV, MIMIC-III-MV, MIMIC-IV, eICU-South, and eICU-West mentioned above.
To the best of our knowledge, our framework is the first to enable federated learning with clients of distinct medical codes and EHR database schemas. tab:full shows the average macro AUROC results across 12 clinical tasks on three distinct scenarios: federated learning on our EHRFL framework, single-source modeling trained on each client’s data, and centralized learning with aggregated data from all clients.
We include single-source and centralized training results for comparison with our EHRFL outcomes.
Single-source training results indicate the performance attainable from solely training on an individual client’s data, considered as a lower-bound.
Conversely, centralized learning shows the results achievable by training on aggregated data across multiple data sources, representing an optimal scenario.
Each client’s dataset is partitioned into train, validation, and test sets in an 8:1:1 ratio. The results show that the federated learning performance for each client is either comparable to or higher than single-source results.
In cases where federated learning performance is comparable but not higher than single-source performance (i.e., MIMIC-III-CV, MIMIC-IV), the results underscore the ongoing challenge of addressing the non-IID problem in existing current federated learning algorithms (e.g., FedProx, FedBN, FedPxN), despite their focus on mitigating the non-IID issue, particularly when compared to the centralized learning performance.
However, notably, the federated learning performance higher than that of single-source modeling for certain clients (i.e., MIMIC-III-MV, eICU-South, and eICU-West) highlights the capability of our framework to facilitate collaborative learning across data sources of heterogeneous EHR systems.
These results provide the first empirical evidence of the feasibility and effectiveness of federated learning across heterogeneous EHR systems. In  \sectionref refined, we further apply our precision-based method outlined in  \sectionref meth:precision_recall for selecting participating clients, aiming for reducing clients for cost reduction of the host.

-----------------------------------------

Table: 2207.08869:S5.T1
Caption: Table 1: FLAIR benchmark results on test set. For setting, C, FL, PFL stands for centralized, federated and private federated learning. C and O denotes whether the metrics are per-class or overall. AP denotes averaged precision; P denotes precision; R denotes recall; and F1 denotes F1 score.
References: Table 1 summarizes the benchmark results on the FLAIR test set.
For the coarse-grained taxonomy, we observe that the performance gap between centralized and federated setting is about 20% on the per class metrics and 6% on the overall metrics if the models are trained from scratch.
These gaps are reduced to 8% and 2% if models are fine-tuned from pretrained ResNet.
When DP is applied, the per class metrics drop about 40% and overall metrics 24% from non-private federated learning if training from scratch.
When fine-tuning with DP, the drop is less significant, about 30% for per class metrics and 10% for overall metrics.

-----------------------------------------

Table: 2303.11745:S2.T6
Caption: TABLE VI: Federated deep learning approach (CNN) evaluation results for multi-classification with IID and Non-IID data.
References: Table VI presents the evaluation results of the federated deep learning approach (CNN) for multi-classification in federated model performance with the IID and Non-IID data and different numbers of honest clients Khsubscript𝐾ℎK_{h} and malicious clients Kmsubscript𝐾𝑚K_{m}. with a poisoning attack (i.e., α𝛼\alpha =60%absentpercent60=60\%) and the number of honest clients is less than the number of malicious clients (i.e., Kh=3subscript𝐾ℎ3K_{h}=3 and Km=7subscript𝐾𝑚7K_{m}=7), we observe that the CNN classifier is affected and give negative results with the three performance metrics, namely, precision, recall, and F1 score.

-----------------------------------------


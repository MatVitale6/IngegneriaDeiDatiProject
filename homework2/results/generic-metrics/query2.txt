Query: precision and recall for deep learning on ImageNet dataset

Table: 2307.07333:S5.T3
Caption: Table 3: A breakdown of the precision, recall, and F-measure of the amodal, invisible, and visible mask predictions by UOAIS-Net on the OSD-Amodal dataset after training on the UOAIS-Sim and SynTable-Sim dataset. P: Precision, R: Recall, F: F-measure
References: Table 2 compares the performance of UOAIS-Net on the OSD-Amodal dataset after training on the UOAIS-Sim tabletop dataset and our SynTable-Sim sample dataset, we can see that the UOAIS-Net trained on the SynTable-Sim dataset outperforms the UOAIS-Net trained on the UOAIS-Sim tabletop dataset in all metrics except Fùí™subscriptFùí™\textit{F}_{\mathcal{O}}. Most importantly, the OOAM accuracy of UOAIS-Net improves by 6.6% after being trained on the SynTable-Sim. This remarkable improvement shows that UOAIS-Net trained using a SynTable-generated synthetic dataset can determine the object occlusion order in a cluttered tabletop scene much more accurately than when it is trained on the UOAIS-Sim tabletop dataset. We provide images of the inference results on the OSD-Amodal dataset in the supplementary materials. A detailed breakdown of the precision P, recall R, and F-measure F, and F@.75 scores for the amodal, invisible, and visible masks are shown in Table 3. We observe that except for the Boundary precision scores of the amodal and invisible masks, UOAIS-Net achieves substantial improvements in the precision, recall, and F-measure scores for the amodal, invisible, and visible masks. Similarly, from Table 4, the UOAIS-Net model trained on the SynTable-Sim dataset also outperforms the one trained on UOAIS-Sim tabletop dataset in all metrics when both models are benchmarked on SynTable-Sim validation dataset. Our experiments demonstrate the effectiveness of our proposed dataset generation pipeline, SynTable, in improving the Sim-to-Real transfer performance of SOTA deep learning computer vision models for UOAIS. These results also highlight the potential of SynTable for addressing the challenge of annotating amodal instance segmentation masks.

-----------------------------------------

Table: 2410.07612:S4.T10
Caption: TABLE X: Deep reinforcement learning model performances on NSL-KDD datasets in last few years.
References: DRL-based intrusion models have reached many interesting results, some researchers claim they as a state-of-the-art methods among all intrusion detection models. In this section, we are going to introduce the model performances of DRL-based intrusion detection models in different datasets. Table¬†X shows the performance of DRL models on the NSL-KDD dataset in recent years. Models such as Big-IDS, MAFSIDS, and A-DQN demonstrate excellent performance in terms of accuracy and F1 score, with MAFSIDS particularly standing out, achieving an accuracy of 99.10% and an F1 score of 99.10%. Different models also excel in precision and recall, with the DRL+RBFNN model showing a balanced performance across these metrics. Table¬†XI lists the performances of DRL models on the UNSW-NB15 and CMU-CERT datasets. Overall, these models perform well on the UNSW-NB15 dataset, with the DQN model achieving an accuracy of 91.80% and an F1 score of 92.44%. In comparison, the AE-DQN model also performs notably well on the CMU-CERT dataset, with an accuracy of 88.80% and an F1 score of 89.90%. Table¬†XI shows the performance on the CIC-IDS2017 dataset, models like DRL+RBFNN and A-DQN perform excellently across all metrics, with the DRL+RBFNN model achieving an accuracy of 99.70%, and precision and F1 scores of 99.60% and 99.60%, respectively. Table¬†XI also presents the performance on the CIC-IDS2018 dataset. The ID-RDRL model stands out, particularly in recall and F1 score, achieving 100.00% and 96.30%, respectively. The performance on the CIC-IDS2019 dataset is also shown in Table¬†XI. The ADQN model excels in all metrics, especially in accuracy (99.60%) and F1 score (99.40%). The DQN+CNN model also performs well in terms of precision and recall. For the performance on the AWID dataset, models like AE-SAC and SSDQDN show outstanding performance across all metrics, particularly the AE-SAC model, with an accuracy of 98.98%, and precision and F1 scores of 98.96% and 98.92%, respectively. The performance of various DRL models varies across different datasets. Table¬†X and Table¬†XI illustrate that different models may exhibit significant performance differences depending on the dataset, suggesting that each model may have unique adaptability to specific types of data. When selecting a deep reinforcement learning model for network intrusion detection, it is essential to consider the model‚Äôs accuracy, precision, recall, and F1 score comprehensively to choose the most suitable model for the specific application scenario.

-----------------------------------------

Table: 1802.03596:S4.T2
Caption: Table 2: Comparison between deep meta-learning and vanilla meta-learning (deep version).
References: DEML version vs.¬†vanilla deep version. To validate that the improvements of DEML are not merely because of the deeper neural network and rescaled images, we also evaluate the deep versions of the previous approaches on MiniImagenet as mentioned in Section¬†4.2. We enlarge the meta-training dataset by merging together the original 64 classes of MiniImagenet and the 200 classes of ImageNet-200. The results are summarized in Table¬†2. It can be seen that simply enlarging the network and training dataset can not lead to a higher accuracy. DEML leverages the power of deep learning in a more principled way and achieves superior performance.

-----------------------------------------

Table: 2410.07612:S4.T11
Caption: TABLE XI: Deep reinforcement learning model performances on other public datasets
References: DRL-based intrusion models have reached many interesting results, some researchers claim they as a state-of-the-art methods among all intrusion detection models. In this section, we are going to introduce the model performances of DRL-based intrusion detection models in different datasets. Table¬†X shows the performance of DRL models on the NSL-KDD dataset in recent years. Models such as Big-IDS, MAFSIDS, and A-DQN demonstrate excellent performance in terms of accuracy and F1 score, with MAFSIDS particularly standing out, achieving an accuracy of 99.10% and an F1 score of 99.10%. Different models also excel in precision and recall, with the DRL+RBFNN model showing a balanced performance across these metrics. Table¬†XI lists the performances of DRL models on the UNSW-NB15 and CMU-CERT datasets. Overall, these models perform well on the UNSW-NB15 dataset, with the DQN model achieving an accuracy of 91.80% and an F1 score of 92.44%. In comparison, the AE-DQN model also performs notably well on the CMU-CERT dataset, with an accuracy of 88.80% and an F1 score of 89.90%. Table¬†XI shows the performance on the CIC-IDS2017 dataset, models like DRL+RBFNN and A-DQN perform excellently across all metrics, with the DRL+RBFNN model achieving an accuracy of 99.70%, and precision and F1 scores of 99.60% and 99.60%, respectively. Table¬†XI also presents the performance on the CIC-IDS2018 dataset. The ID-RDRL model stands out, particularly in recall and F1 score, achieving 100.00% and 96.30%, respectively. The performance on the CIC-IDS2019 dataset is also shown in Table¬†XI. The ADQN model excels in all metrics, especially in accuracy (99.60%) and F1 score (99.40%). The DQN+CNN model also performs well in terms of precision and recall. For the performance on the AWID dataset, models like AE-SAC and SSDQDN show outstanding performance across all metrics, particularly the AE-SAC model, with an accuracy of 98.98%, and precision and F1 scores of 98.96% and 98.92%, respectively. DRL-based intrusion models have reached many interesting results, some researchers claim they as a state-of-the-art methods among all intrusion detection models. In this section, we are going to introduce the model performances of DRL-based intrusion detection models in different datasets. Table¬†X shows the performance of DRL models on the NSL-KDD dataset in recent years. Models such as Big-IDS, MAFSIDS, and A-DQN demonstrate excellent performance in terms of accuracy and F1 score, with MAFSIDS particularly standing out, achieving an accuracy of 99.10% and an F1 score of 99.10%. Different models also excel in precision and recall, with the DRL+RBFNN model showing a balanced performance across these metrics. Table¬†XI lists the performances of DRL models on the UNSW-NB15 and CMU-CERT datasets. Overall, these models perform well on the UNSW-NB15 dataset, with the DQN model achieving an accuracy of 91.80% and an F1 score of 92.44%. In comparison, the AE-DQN model also performs notably well on the CMU-CERT dataset, with an accuracy of 88.80% and an F1 score of 89.90%. Table¬†XI shows the performance on the CIC-IDS2017 dataset, models like DRL+RBFNN and A-DQN perform excellently across all metrics, with the DRL+RBFNN model achieving an accuracy of 99.70%, and precision and F1 scores of 99.60% and 99.60%, respectively. Table¬†XI also presents the performance on the CIC-IDS2018 dataset. The ID-RDRL model stands out, particularly in recall and F1 score, achieving 100.00% and 96.30%, respectively. The performance on the CIC-IDS2019 dataset is also shown in Table¬†XI. The ADQN model excels in all metrics, especially in accuracy (99.60%) and F1 score (99.40%). The DQN+CNN model also performs well in terms of precision and recall. For the performance on the AWID dataset, models like AE-SAC and SSDQDN show outstanding performance across all metrics, particularly the AE-SAC model, with an accuracy of 98.98%, and precision and F1 scores of 98.96% and 98.92%, respectively. DRL-based intrusion models have reached many interesting results, some researchers claim they as a state-of-the-art methods among all intrusion detection models. In this section, we are going to introduce the model performances of DRL-based intrusion detection models in different datasets. Table¬†X shows the performance of DRL models on the NSL-KDD dataset in recent years. Models such as Big-IDS, MAFSIDS, and A-DQN demonstrate excellent performance in terms of accuracy and F1 score, with MAFSIDS particularly standing out, achieving an accuracy of 99.10% and an F1 score of 99.10%. Different models also excel in precision and recall, with the DRL+RBFNN model showing a balanced performance across these metrics. Table¬†XI lists the performances of DRL models on the UNSW-NB15 and CMU-CERT datasets. Overall, these models perform well on the UNSW-NB15 dataset, with the DQN model achieving an accuracy of 91.80% and an F1 score of 92.44%. In comparison, the AE-DQN model also performs notably well on the CMU-CERT dataset, with an accuracy of 88.80% and an F1 score of 89.90%. Table¬†XI shows the performance on the CIC-IDS2017 dataset, models like DRL+RBFNN and A-DQN perform excellently across all metrics, with the DRL+RBFNN model achieving an accuracy of 99.70%, and precision and F1 scores of 99.60% and 99.60%, respectively. Table¬†XI also presents the performance on the CIC-IDS2018 dataset. The ID-RDRL model stands out, particularly in recall and F1 score, achieving 100.00% and 96.30%, respectively. The performance on the CIC-IDS2019 dataset is also shown in Table¬†XI. The ADQN model excels in all metrics, especially in accuracy (99.60%) and F1 score (99.40%). The DQN+CNN model also performs well in terms of precision and recall. For the performance on the AWID dataset, models like AE-SAC and SSDQDN show outstanding performance across all metrics, particularly the AE-SAC model, with an accuracy of 98.98%, and precision and F1 scores of 98.96% and 98.92%, respectively. DRL-based intrusion models have reached many interesting results, some researchers claim they as a state-of-the-art methods among all intrusion detection models. In this section, we are going to introduce the model performances of DRL-based intrusion detection models in different datasets. Table¬†X shows the performance of DRL models on the NSL-KDD dataset in recent years. Models such as Big-IDS, MAFSIDS, and A-DQN demonstrate excellent performance in terms of accuracy and F1 score, with MAFSIDS particularly standing out, achieving an accuracy of 99.10% and an F1 score of 99.10%. Different models also excel in precision and recall, with the DRL+RBFNN model showing a balanced performance across these metrics. Table¬†XI lists the performances of DRL models on the UNSW-NB15 and CMU-CERT datasets. Overall, these models perform well on the UNSW-NB15 dataset, with the DQN model achieving an accuracy of 91.80% and an F1 score of 92.44%. In comparison, the AE-DQN model also performs notably well on the CMU-CERT dataset, with an accuracy of 88.80% and an F1 score of 89.90%. Table¬†XI shows the performance on the CIC-IDS2017 dataset, models like DRL+RBFNN and A-DQN perform excellently across all metrics, with the DRL+RBFNN model achieving an accuracy of 99.70%, and precision and F1 scores of 99.60% and 99.60%, respectively. Table¬†XI also presents the performance on the CIC-IDS2018 dataset. The ID-RDRL model stands out, particularly in recall and F1 score, achieving 100.00% and 96.30%, respectively. The performance on the CIC-IDS2019 dataset is also shown in Table¬†XI. The ADQN model excels in all metrics, especially in accuracy (99.60%) and F1 score (99.40%). The DQN+CNN model also performs well in terms of precision and recall. For the performance on the AWID dataset, models like AE-SAC and SSDQDN show outstanding performance across all metrics, particularly the AE-SAC model, with an accuracy of 98.98%, and precision and F1 scores of 98.96% and 98.92%, respectively. The performance of various DRL models varies across different datasets. Table¬†X and Table¬†XI illustrate that different models may exhibit significant performance differences depending on the dataset, suggesting that each model may have unique adaptability to specific types of data. When selecting a deep reinforcement learning model for network intrusion detection, it is essential to consider the model‚Äôs accuracy, precision, recall, and F1 score comprehensively to choose the most suitable model for the specific application scenario.

-----------------------------------------

Table: 2406.03577:S5.T8
Caption: Table 8: The cross project validation from 15 Android projects, with 5 projects having both precision and recall higher than 80% (ConnectBot, Email, Coolreader, Crosswords, AnkiDroid) 
References: Train-Multiple-Predict-One To further improve the learning performance, we conduct 15-fold cross-validation by choosing 14 projects from the same domain of the Android project for training. The remaining project is reserved for testing. Table 8 contains the cross-validation results, ordered by precision and recall values. 5 out of the 15 experiments have both precision and recall values equal to or greater than 80%; 10 out of 15 experiments have both precision and recall equal to or greater than 70%. Referring to Table¬†3, the 5 experiments with precision and recall below 70% have the ratio of vulnerable files below 40%. Referring to Table¬†8, cross-project validation improves the learning performance under the threshold of 80% to 5 projects out of 15 projects. This approach of transfer learning, by combining the features from the Android project repository to tune the random forest model, achieves comparable learning performance to deep learning models ResNet and LSTM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03577v1#bib.bib2" title="">2</a>]</cite> (4.2 projects out of 15 projects).
[2] (4.2 projects out of 15 projects).

-----------------------------------------

Table: 2303.11745:S2.T6
Caption: TABLE VI: Federated deep learning approach (CNN) evaluation results for multi-classification with IID and Non-IID data.
References: Table VI presents the evaluation results of the federated deep learning approach (CNN) for multi-classification in federated model performance with the IID and Non-IID data and different numbers of honest clients Khsubscriptùêæ‚ÑéK_{h} and malicious clients KmsubscriptùêæùëöK_{m}. with a poisoning attack (i.e., Œ±ùõº\alpha =60%absentpercent60=60\%) and the number of honest clients is less than the number of malicious clients (i.e., Kh=3subscriptùêæ‚Ñé3K_{h}=3 and Km=7subscriptùêæùëö7K_{m}=7), we observe that the CNN classifier is affected and give negative results with the three performance metrics, namely, precision, recall, and F1 score.

-----------------------------------------

Table: 2109.12546:S6.T6
Caption: TABLE VI: Pima Diabetes Dataset results: recall, precision and F1 measure
References: Table V, Table V and Table VI show the results observed for the three imbalanced public datasets of Credit Card Fraud, Breast Cancer and Pima Diabetes. We compare the five oversampling techniques in combination with four classification algorithms. The performance of each classification method is measured in terms of recall, precision and F1 score. For the Credit Card Fraud Dataset in Table V, the highest F1 score was achieved when SDG-GAN was combined with RF for a score of 91.31%. In Table V for the Breast Cancer Dataset, cGAN combined with XGBoost outperformed the rest of the methods with F1 score of 91.95%. Similarly with the Credit Card, in the Pima Diabetes Dataset, SDG-GAN in combination with RF produced the best results with an F1 score of 70.80% as Table VI indicates. This was a significant improvement of ‚âà\approx 5% compared to when no oversampling was used and an improvement of ‚âà\approx 2% than the second-best combination between MLP and ADASYN.
Another observation that could be drawn from the results was that when the standard oversampling techniques were used i.e. SMOTE, ADASYN, there was a drastic improvement in the classification of the minority class with better overall recall compared to precision (in the majority of cases). However, simultaneously, there was a huge drop in the classification accuracy of the majority class. This was supported by the increase of the recall score in the Credit Card Fraud Dataset prior to the use of any oversampling method; on average, the recall was 85% and the precision 94%. When SMOTE was used, the recall score increased significantly, while the precision decreased. However, this was not the case when SDG-GAN was used for oversampling, whereby we saw a more robust improvement in the classification metrics, as Table V and Table VI show.

-----------------------------------------

Table: 2109.12546:S6.T5
Caption: TABLE IV:  Credit Card detection results: recall, precision and F1 measureTABLE V: Breast Cancer detection results: recall, precision and F1 measure
References: Table V, Table V and Table VI show the results observed for the three imbalanced public datasets of Credit Card Fraud, Breast Cancer and Pima Diabetes. We compare the five oversampling techniques in combination with four classification algorithms. The performance of each classification method is measured in terms of recall, precision and F1 score. For the Credit Card Fraud Dataset in Table V, the highest F1 score was achieved when SDG-GAN was combined with RF for a score of 91.31%. In Table V for the Breast Cancer Dataset, cGAN combined with XGBoost outperformed the rest of the methods with F1 score of 91.95%. Similarly with the Credit Card, in the Pima Diabetes Dataset, SDG-GAN in combination with RF produced the best results with an F1 score of 70.80% as Table VI indicates. This was a significant improvement of ‚âà\approx 5% compared to when no oversampling was used and an improvement of ‚âà\approx 2% than the second-best combination between MLP and ADASYN.
Another observation that could be drawn from the results was that when the standard oversampling techniques were used i.e. SMOTE, ADASYN, there was a drastic improvement in the classification of the minority class with better overall recall compared to precision (in the majority of cases). However, simultaneously, there was a huge drop in the classification accuracy of the majority class. This was supported by the increase of the recall score in the Credit Card Fraud Dataset prior to the use of any oversampling method; on average, the recall was 85% and the precision 94%. When SMOTE was used, the recall score increased significantly, while the precision decreased. However, this was not the case when SDG-GAN was used for oversampling, whereby we saw a more robust improvement in the classification metrics, as Table V and Table VI show.

-----------------------------------------

Table: 2407.17755:S4.T2
Caption: Table 2: Comparative analysis among the proposed model and existing models across evaluation metrics including Precision, Recall, Accuracy, Cohen‚Äôs Kappa Score, and F1 Score.
References: Tabel 2 and Fig. 6 illustrate the superior performance of the proposed hybrid model against other well-known deep learning architectures in detecting diabetic retinopathy. Our model demonstrates higher accuracy and recall, showcasing its effectiveness in this task. Specifically, our ensemble model, leveraging InceptionV3 and Densenet121, achieves outstanding results across all metrics. Notably, it achieves a specificity and precision of 0.99, an accuracy rate of 0.99, and a Cohen‚Äôs Kappa Score of 0.98. These outcomes underscore the capability of our model to capture both local and global features in retinal images, thereby significantly enhancing diabetic retinopathy detection. Consequently, our ensemble model emerges as a promising tool to support clinicians in diagnosis.

-----------------------------------------

Table: 2209.01338:S6.T4
Caption: Table 4. Performance comparison of the proposed approach with the existing ones using accuracy (in %). [P: Precision, R: Recall, F: F1subscriptùêπ1F_{1} score, A: Accuracy]
References: We compare FedAR+ approach with three state-of-the-art solutions including two best performing plug-load identification models from¬†(Schwermer et¬†al., 2022) and Household Appliance Recognition through Bayes classification (HARB)¬†(Yan
et¬†al., 2019). The work¬†(Schwermer et¬†al., 2022) leverages FL to train four different deep learning models; we pick two best performers, long-short term memory (LSTM) and convolutional neural network (CNN), named as LSTM-AR and CNN-AR for the convenience. Similar to the proposed approach, LSTM-AR and CNN-AR models are also trained across 101010 clients (possessing non-iid data) over 303030 FL rounds with aggregation at every 505050 local epochs. On the other hand, HARB¬†(Yan
et¬†al., 2019) follows a central learning paradigm. In HARB, the time series data are transformed into a set of statistical features (e.g., working time, power range, frequency of the use, etc.) on which Bayesian learning is applied to obtain posterior class probabilities that are used for appliance prediction. As the existing solutions do not incorporate any noise handling method, we consider two variants of the proposed approach: 1) FedAR: without noise handling and 2) FedAR+: with noise handling, in the interest of fair comparison. Table¬†4 shows the comparison results using precision, recall, F1subscriptùêπ1F_{1} score, and accuracy. We make following important observations:

-----------------------------------------

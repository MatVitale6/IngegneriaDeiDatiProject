Query: evaluation of BERT embeddings for similarity tasks

Table: 2407.15124:S8.T3
Caption: Table 3: BiLSTM CRF models with Base BERT vs ChemBERT embeddings
References: Table 3 summarizes the performances obtained by using the BERT pretrained on the chemical domain dataset for generating the token embeddings against the Base BERT performance. We note that the ChemBert used here is the recobo/chemical-bert-uncased-pharmaceutical-chemical-classifier pretrained BERT since this gave superior results as compared to the other pretrained BERT model. Hence, we only report results and perform comparative analysis for this version of chemical pretrained BERT.
Here again we see that finetuning benefits the performance considerably. We observe minor gains in the performance of the model based on ChemBert embeddings. This improvement is not significant. Although we would have expected a good improvement upon using the ChemBERT embeddings, model finetuning closes this gap.

-----------------------------------------

Table: 1709.06671:S4.T1
Caption: Table 1: Results on word similarity, analogy, relation and short-text classification tasks. For each task, the best performing method
is shown in bold. Statistically significant improvements over the best individual source embedding are indicated by an asterisk.
References: We summarise the experimental results for different methods on different tasks/datasets in Table 1.
In Table 1, rows 1-5 show the performance of the individual source embeddings.
Next, we perform ablation tests (rows 6-20) where we hold-out one source embedding and use the other four with each meta-embedding method.
We evaluate statistical significance against best performing individual source embedding on each dataset.
For the semantic similarity benchmarks we use Fisher transformation to compute p<0.05𝑝0.05p<0.05 confidence intervals for Spearman correlation coefficients.
In all other (classification) datasets, we used Clopper-Pearson binomial exact confidence intervals at p<0.05𝑝0.05p<0.05. Overall, from Table 1, we see that the Proposed method (row 25) obtains the best performance in all tasks/datasets.
In 6 out of 12 benchmarks, this improvement is statistically significant over the best single source embedding.
Moreover, in the MEN dataset (the largest among the semantic similarity benchmarks compared in Table 1 with 3000
word-pairs), and the Google dataset, the improvements of the Proposed method over the previously proposed 1TON and 1TON+
are statistically significant.

-----------------------------------------

Table: 2410.00004:S3.T1.7
Caption: Table 1: Perplexity results of experiments for Retro-li-on/-off with Bert and SBert embeddings averaged over three random seeds.
References: For the first batch of experiments, we gradually increase the number of neighbors from 2 up to 10 and run the experiments for three random seeds. In Table 1 we report their average. Bert embeddings with 5 neighbors already outperforms Retro-li-off. Moreover, changing the embedding model to SBert improves our performance significantly. This shows that not only does SBert find more semantically similar neighbors, but our model also correctly attends to them. We take the best Retro-li-off checkpoint and keep training it for six random seeds, with and without retrieval, and with and without regularization. WikiText-103 is taken as the base dataset (dataset A, see above) for the taining. We train both Retro-li-on and Retro-li-off, in order to confirm that the improved performance is not due to the increased number of training tokens. As is evident in the rows for Retro-li-on with ideal retrieval in Table 3, when compared to Table 1 with training from scratch, this setup decreases WikiText-103-Validation set perplexity for both, Retro-li-on and Retro-li-off. The seemingly significant perplexity improvement of Retro-li-off is due to there no longer being one particularly bad random seed, as this setup reduces the variance across seeds.

-----------------------------------------

Table: 2405.08997:S4.T1.1
Caption: Quality of different embeddings models in measuring semantic similarity between sentences. A lower average displacement and higher RBO indicate a better embeddings model for this purpose.
References: We evaluated seven different embeddings models for this purpose and measured the semantic similarity between twelve target sentences and a ranked list of 10 sentences for each ranging from most to least semantically similar (sentences can be found in Appendix B).
For each target sentence, we compare the ground-truth ranking of the 10 sentences to the ranking determined by the semantic similarity scores yielded by a particular embeddings model.
We measure the similarity between the two rankings using two metrics: average displacement (average distance between a sentence’s position in the computed ranking and its position in the target ranking) and RBO (Rank-biased Overlap Webber et al. (2010)).
Table 1 tabulates the results of this evaluation.


-----------------------------------------

Table: 2407.11654:S4.T2
Caption: TABLE II: Fine-Tuning results for BERT and RoBERTa on SST2, QNLI, MNLI, CONLL2003, and WNUT17 datasets across different SFL training scenarios, evaluated via Accuracy (for SC) and F1 Scores (for NER).
References: Table I summarizes the experiment configuration and Table II shows the fine-tuning results for each scenario. We further include detailed performance plots for the aggregated global SFL model after each global round, i.e. after each Nepochssubscript𝑁epochsN_{\text{epochs}}, for every dataset, base model, and scenario in Figure 3. As shown in Table II, for all three SC datasets, R-SFLLM is able to consistently safeguard the distributed training in general, leading to robust global models with classification accuracies near-identical or very close to the baselines. For instance, BERT achieves near-identical performance to the SST2 baseline with accuracies above 91%percent9191\% for the scenarios when worst-case jamming is absent (Gaussian) and when R-SFLLM protection against it is employed (Protection). This indicates that AWGN alone does not significantly impact the word embeddings and is negligible within a statistical variance, thereby acting as a light regularizer. As for the scenario where no protection is provided (No Protection), the worst-case jammer is successful in maximally disrupting the distributed training, resulting in a global model accuracy of around 50%percent5050\%. As such, the global SFL model is no better than simple guessing, in other words, it has not been able to learn anything and subsequently has worst-case binary classification performance. This answers the question of whether worst-case jamming translates into worst-case performance positively. Moreover, each client observes near optimal performance from early epochs on, such that the global model already converges after the first global round as shown in Figure 3a. The same can be observed for MNLI and QNLI in Figures 3c and 3d. Hence, we can state that BERT is already well pre-trained on sentiment analysis, such that SC might be an easy fine-tuning task. Figure 4 further shows the QNLI performance for RoBERTa clients 1 to 3, respectively, where clients 1 and 2 approach the baseline whereas client 3 aligns more to the observed global SFL model. When further comparing the MSEs for each user in Table II, client 1 experiences the lowest MSE, while client 3 experiences the highest MSE, which is about 2.8 times higher than the one from client 1. This indicates that the developed resource allocation of the proposed protection scheme is not fair. However, this difference, while significant, does not seem to particularly affect the BERT model. This corroborates that RoBERTa is more sensitive to noisy word embeddings in general. However, since this is not necessarily observed for RoBERTa on the SST2 dataset, there likely is an additional dependence on the data distribution, such that a potentially non-independently-and-identically distributed (non-IID) data split may further decrease the performance in case of corrupted word embeddings. Thus, fine-tuning RoBERTa may result in an initially worse model if some clients underperform. Table II shows that BERT similarly achieves near-identical performance to the SFL baseline for CONLL2003 with F1 scores above 92%percent9292\% for the scenarios Gaussian and Protection. Again, No Protection results in maximal disruption, with consistent F1 scores around 10%percent1010\%. This suggests that the model is either missing almost all of the entities (low recall) or predicting almost all entities incorrectly (low precision), which ultimately renders the obtained global model unusable for NER. Thus, worst-case jamming results in worst-case model performance. In addition, Figure 3g again shows that each client observes near optimal performance from early epochs on. This further corroborates the assumption that the BERT architecture is robust against noisy embeddings due to potentially unfair resource allocation in wireless SFL.

-----------------------------------------

Table: 2406.18847:A1.T9.4
Caption: Table 9: Additional evaluation metrics, METEOR and BERT scores.
References: The evaluation metrics for LAPDOG have been broadened to incorporate METEOR and BERT scores. This enhancement supplements the foundational assessment based on F1, BLEU, and ROUGE-L metrics, presenting a more diverse evaluation landscape. The additional evaluation outcomes are tabulated in Table 9. Based on the results in Table 9, LAPDOG excels in the METEOR score relative to baseline models, showcasing its capability in nuanced linguistic comprehension. However, the variance in BERT scores is minimal, likely due to LAPDOG’s optimization for traditional metrics. Enhancing performance by tailoring optimization for BERT scores represents a promising area for future inquiry.

-----------------------------------------

Table: 2406.18847:A1.T9.4
Caption: Table 9: Additional evaluation metrics, METEOR and BERT scores.
References: The evaluation metrics for LAPDOG have been broadened to incorporate METEOR and BERT scores. This enhancement supplements the foundational assessment based on F1, BLEU, and ROUGE-L metrics, presenting a more diverse evaluation landscape. The additional evaluation outcomes are tabulated in Table 9. Based on the results in Table 9, LAPDOG excels in the METEOR score relative to baseline models, showcasing its capability in nuanced linguistic comprehension. However, the variance in BERT scores is minimal, likely due to LAPDOG’s optimization for traditional metrics. Enhancing performance by tailoring optimization for BERT scores represents a promising area for future inquiry.

-----------------------------------------

Table: 2407.15124:S8.T2
Caption: Table 2: Trigram vs BiLSTM CRF decoder for base BERT models
References: Table 2 depicts the comparison between the Trigram-based decoder vs the BiLSTM CRF-based decoder for the bert-base-uncased BERT embeddings. We compare the performances of the models with and without BERT finetuning. The fuzzy f1 score for the CRF-based models are higher in both the scenarios. We use 16 paragraph sequences for the BiLSTM encoding-decoding and CRF decoding. Naturally, the CRF model is better able to capture the long range dependencies as compared to the trigram decoder model that just considers 1 paragraph before and after the current paragraph. Hence, for the future studies, we only train and evaluate the BiLSTM-CRF based architecture.

-----------------------------------------

Table: 2406.17745:S4.T5
Caption: Table 5. Average similarity of EGIN and DNN embedding
References: Besides, we randomly choose 1 million intra-category and inter-category item pairs from our industrial dataset and test their average cosine similarities of embeddings produced by different methods. Table 5 shows us that the intra-category similarity of EGIN is much higher than the inter-category, whereas the difference from DNN embedding is relatively indistinctive. At the same time, our graph embedding appears to be more capable of capturing relationships between similar items than EGES and DNN.

-----------------------------------------

Table: 2312.17493:S5.T4
Caption: Table 4. The performance of the global model under different ϵitalic-ϵ\epsilon for Bert in general and Medical tasks.
References: In our experiments, we evaluated the performance of several models in the context of differential privacy for medical tasks. We investigated the impact of varying the parameters  ϵ italic-ϵ \epsilon  and  δ 𝛿 \delta  for the language model fine-tuning on the medical datasets. In all experiments, the compression rank was fixed at  512 512 512  and the  δ 𝛿 \delta  is set to  1 ​ e − 5 1 𝑒 5 1e-5  when we varying the  ϵ italic-ϵ \epsilon  and the  ϵ italic-ϵ \epsilon  is settd to  2 2 2 . One notable trend is the general decrease in performance across all models with stricter privacy settings (lower  ϵ italic-ϵ \epsilon  and higher  δ 𝛿 \delta  values). For instance, GPT-2’s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when  ϵ italic-ϵ \epsilon  is reduced to 2 and  δ 𝛿 \delta  to  1 ​ e − 05 1 𝑒 05 1e-05 , respectively (Tables  3  and  7 ). This trend indicates a trade-off between privacy and utilities. In contrast, some models like Llama-7B show a more resilient performance under varying  ϵ italic-ϵ \epsilon  values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when  ϵ italic-ϵ \epsilon  is increased from the original to 10 (Table  6 ). This suggests that certain models might be better suited for privacy-sensitive applications. Additionally, the impact of changing  δ 𝛿 \delta  values appears to be more model-specific. Bert’s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when  δ 𝛿 \delta  is reduced to  1 ​ e − 06 1 𝑒 06 1e-06  (Table  8 ), highlighting a potentially higher sensitivity to  δ 𝛿 \delta  adjustments. We can observe a significant trade-off between privacy and utility in medical tasks using models like GPT-2, Bert, ChatGLM-6B, and Llama-7B. Our data clearly shows that stricter privacy settings (lower  ϵ italic-ϵ \epsilon  and  δ 𝛿 \delta  values) generally correlate with reduced performance across tasks.

-----------------------------------------

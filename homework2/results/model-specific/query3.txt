Query: training convergence for federated learning models

Table: 2206.10048:S4.T3
Caption: Table 3: Accuracy convergence among distributed node models.Each local model is evaluated on all test sets of the federation in order to measure convergence and generalization (lower standard deviation corresponds to higher convergence).
References: We also compare FedER against standard training of the local node models, referred to as “Standalone” . Classification accuracy is computed using local node models on their own data. The results, reported in Table 2, show that standalone training appears to be the most favourable scenario. Centralized strategies perform generally worse than standalone training, because of the non-i.i.d. nature of the data. However, when the centralized approach is trained with original data augmented with synthetic samples, its classification accuracy is on par with the standalone training, possibly due to the learned generative latent spaces that likely tend to smooth different modes of non-i.i.d. data. FedER, instead, outperforms its centralized counterpart and yields slightly worse performance (1.5 percent points less) than standalone training. Although this may appear, at a first glance, as a shortcoming of FedER, we recall that in a federated learning scenario, we aim at building a model that, leveraging multiple data distributions present in the federation, may generalize better, thus addressing possible future data drifts. In order to assess the capabilities of the trained models to achieve such a generalization, we measure the decision convergence by evaluating how a local node model performs on other node datasets. Results are in Table 3 and show a good average accuracy, with a low standard deviation, by FedER, indicating that each node model performs equally well on its own dataset and on the others (i.e., all node models converge to similar decisions). Conversely, standalone training yields significantly lower accuracy and higher standard deviation than ours, demonstrating to be an unsuitable strategy for the sought generalization properties.

-----------------------------------------

Table: 2209.03137:Ch3.T2
Caption: Table 3.2: Applications with pre-trained models in Federated Transfer Learning.
References: Federated Transfer Learning is a variant of Federated Learning  [ 77 ] . Federated Transfer Learning utilizes different datasets which are neither identical in sample space nor in feature space. The initial goal of Federated Learning is to carry out useful machine learning methods in multiple devices, and build an aggregated model based on dataset across multiple devices, while ensuring users’ privacy and security  [ 37 ] . Thus, we should obey the original goal of federated (transfer) learning. Although they are not chosen by us, we briefly review some other alternative for security and privacy in machine learning (see Table 3.1) for completeness. Security Solution  \@slowromancap i@.  The paper  [ 41 ]  is a start in federated machine learning, and this paper is focused on data security in a multi-party privacy-preserving setting. The similar focus is also in the paper  [ 70 ] , which emphasizes the use of multi-party computation (MPC) can improve efficiency by an order of magnitude in semi-honest security settings. In the paper  [ 70 ] , the authors use the SPDZ  [ 17 ]  protocol, which is an implementation of MPC, and experimental results proved that the protocol outperforms homomorphic encryption (HE) in terms of communication and time under the same Federated Transfer Learning framework. Security Solution  \@slowromancap ii@.  Further, the authors in  [ 52 ]  proposed a new method of authentication and key exchange protocol to provide an efficient authentication mechanism for Federated Transfer Learning blockchain (FTL-Block), which uses the Novel Supportive Twin Delayed DDPG (S-TD3) algorithm  [ 52 ] . The participants implement the authentication in each part, which completely relies on the credit of the participants. This authentication mechanism can integrate the users’ credit with both local credit and cross-region credit. With the help of S-TD3 algorithm, the training of the local authentication model achieves the highest accuracy  [ 52 ] . Transfer Learning method is applied to reduce the extra time cost in the authentication model, and domain the authentication model can be accurately and successfully migrated from local to foreign users  [ 52 ] . Moreover, Majeed  et al .  [ 50 ]  proposed the cross-silo secure aggregation technique based on MPC for secure Federated Transfer Learning. Security Solution  \@slowromancap iii@.  In addition, the existing approaches mainly focus on homogeneous feature spaces, which will leak privacy when dealing with the problems of covariate shift and feature heterogeneity. Thus, Gao  et al .  [ 27 ]  provide a privacy preserving Federated Transfer Learning framework for homogeneous feature spaces called heterogeneous Federated Transfer Learning (HFTL). Specifically, Gao  et al . designed privacy-preserving Transfer Learning method to remove covariate shifts in homogeneous feature spaces, and connect heterogeneous feature spaces from different participants  [ 27 ] . Besides, two variants based on HE and secret sharing techniques are applied in the HFTL. Experimental results have demonstrated that the framework performs general feature-based Federated Learning methods and self-learning methods under the same challenge constraints, and HFTL also is shown to have practical efficiency and scalability  [ 27 ] . Our Framework without Additional Security Solutions.  To conclusion, all the these security solutions  [ 70 ]   [ 17 ]   [ 52 ]   [ 27 ]  added to add additional security protection mechanisms. Since our primary goal is to investigate Federated Learning itself, we do not incorporate any of the mechanisms above, although we believe our solution and framework can benefit from them in security, too. There are some successful applications based on different Transfer Learning methods. For edge devices, tiny devices like mobile phones, medical instruments, smart manufacturing, etc.  [ 15 ]   [ 76 ]   [ 32 ] . There are two strategies are used in Federated Transfer Learning:  Pre-trained models  can be reused in related tasks, while  domain adaptation  can be used from a source domain to a related target domain. A  pre-trained  model can be used directly in some Federated Transfer Learning frameworks. Domain adaptation is also known as the knowledge transferring from the source domain to the target domain  [ 10 ] . Application  \@slowromancap i@ with Pre-trained Models.  After FedHealth  [ 15 ] , Chen  et al .  [ 14 ]  proposed FedHealth2, a weighted Federated Transfer Learning framework through batch normalization . In FedHealth2, all participants aggregate the features without compromising privacy security, and obtain local models for participants via weighting and protecting local batch normalization  [ 14 ] . More specifically, FedHealth2 achieves the similarities in participants with the support of a pre-trained model. The similarities are confirmed by the metrics of the data distributions, and the metrics can be determined by outputs values of the pre-trained model. Then, with the achieved similarities, the server can do the averaging of the weighted models’ parameters in a localized approach and produce a unique model for each participant  [ 14 ] . Experimental results have shown that FedHealth2 enables the local participants’ models to do the recognition with higher accuracy. Besides, FedHealth2 can achieve similarity in several epochs even if no pre-trained model exists  [ 14 ] . Application  \@slowromancap ii@ with Pre-trained Models.  Moreover, FedURR  [ 26 ]  also benefits from the pre-trained model. In Urban risk recognition (URR) task, the urban management usually has multiple departments, each of which stores a large amount of data locally. When data is uploaded to a central database, it means huge cost and a lot of time consumption, and there exists a risk of data leakage  [ 26 ] . Thus, the proposed framework FedURR integrates two types of Transfer Learning into the Federated Learning framework, i.e. fine-tuning based and parameter sharing based Transfer Learning methods. With the help of fine-tuning and parameter sharing, they are connected into different stages of Federated Learning with an precisely design. The experimental results are shown that FedURR can improve multi-department collaborative URR accuracy  [ 26 ] . Application  \@slowromancap iii@ with Pre-trained Models.  Furthermore, Zhang  et al .  [ 79 ]  proposed the first Federated Transfer Learning framework, to solve problems in Disaster Classification in Social Computing Networks (FedDCSCN). The authors want to eliminate shortcomings of the local models of the participants, which are deep learning models. The local models need a large number of high-quality samples, and fast computation speed is required to accelerate the training process  [ 79 ] . In addition, the data labeling process is time consuming in the field of social computing, which hinders the use of deep learning networks  [ 79 ] . Thus, Federated Learning and Transfer Learning are combined to address the problems. Pre-trained model based Transfer Learning is used as to reduce communication and computation costs  [ 79 ] . Besides, homomorphic encryption approach is applied as a additional to preserve the local data privacy of social computing participants  [ 79 ] . Experimental results are shown that a feasible but not ideal performance is obtained by the framework in the social computing field  [ 79 ] . Application  \@slowromancap i@ with Domain Adaptation.  FedSteg  [ 76 ]  provides an example for using domain adaptation.
Image steganography is the method of concealing secret information within images. Conversely, image steganalysis is a counter method to image steganography. This method intends to detect the secret information within images. Through this detection technique, the steganographic features which are generated by image steganographic methods can be extracted. However, there are still problems that exist in image steganalysis. Image steganalysis algorithms train on machine learning models which rely on a large amount of data. However, it is hard to aggregate all the steganographic images to a global cloud server.
Moreover, the users do not want unrelated people to snoop on confidential information. To solve the problems, Yang  et al . propose the framework called FedSteg. FedSteg trains a machine learning model with a privacy-protecting technique through domain adaptation. Domain adaptation is used to train the local model by decreasing the domain discrepancy between the global server and local data. Compared with traditional non-federated steganalysis techniques, the experiment results show that FedSteg achieves certain improvements  [ 76 ] . Application  \@slowromancap ii@ with Domain Adaptation.  Fedhealth  [ 15 ]  benefits from domain adaptation. Wearable devices allow people to get access to and record healthcare information. Additionally, smart wearable devices use a large amount of personal data to train machine learning models. Different wearable devices have diverse characteristics and domains. However, the healthcare data from different people with diverse monitoring patterns are difficult to aggregate together to generate robust results. Each personal data is an island. Besides, the machine models using personal data are hard to train on cloud servers. To solve data isolation and locally training problem, Chen  et al . proposed a Federated Learning framework called FedHealth  [ 15 ] . In this paper, the authors used a neural network (NN), which has two convolutional layers, two pooling layers, and three fully-connected layers  [ 15 ] . NN aims at extracting low-level features. Domain adaptation is applied to transfer the extracted features from server to clients by minimizing the feature distance between server and clients. Compared to the approaches without Federated Learning and traditional methods (KNN, SVM, and RF), FedHealth achieves better performance  [ 15 ] . Application  \@slowromancap iii@ with Domain Adaptation.  One more example that uses domain adaptation is the electroencephalographic (EEG) signal classification  [ 32 ] . Brain-Computer Interface (BCI) systems are mainly to identify the users’ consciousness from the brain states. Deep learning methods achieve success in the BCI field for classification of EEG signals. However, the success is restricted to the lack of a large amount of data. Besides, according to the privacy of personal EEG data, it is constrained to build a collection of big BCI dataset. In order to solve the lack of data and the private privacy problems, Ju  et al . proposed a Federated Transfer Learning method for EEG Signal classification. They propose an method which use Transfer Learning technique with domain adaptation to extract the common discriminative information, and map the common discriminative information into a spatial covariance matrix, then subsequently fed the spatial covariance matrix to a deep learning based Federated Transfer Learning architecture  [ 32 ] . The proposed architecture based on deep learning has 4 layers, namely Manifold reduction layer (M), Common embedded space (C), Tangent projection layer (T) and Federated layer (F), the middle two layers (M and T) provide the functionality of Transfer Learning  [ 32 ] . The experimental result shows that this method using domain adaption in Federated Learning architecture has robust generation ability. There are two special cases of the problems to be solved in the heterogeneous Federated Transfer Learning setting, and one case for quantifying the performance of Federated Transfer Learning. Model Distillation.  FedMD  [ 45 ]  provides a way to solve statistical heterogeneity (the non-IID problem) in Federated Transfer Learning. Concretely, the authors in FedMD focus on the differences of local models  [ 45 ] . The authors in FedMD identify that communication is the key to fix model heterogeneity. Devices should have the ability to learn the communication protocol to leverage Transfer Learning and model distillation. The communication protocol aims to reuse the models, which are trained from a public dataset. Each client achieves a well-trained model, and applies the well-trained model on local data which is considered as Transfer Learning with model distillation. Thus, the proposed FedMD, which combines Federated Learning and Transfer Learning with knowledge distillation, allows participants to create their models locally, and a communication protocol that utilizes the power of Transfer Learning with model distillation  [ 45 ] . FedMD is demonstrated its efficiency to work on different tasks and datasets  [ 45 ] . Knowledge Distillation.  Wang  et al .  [ 75 ]  propose Federated Transfer Learning via Knowledge Distillation (FTLKD), which is a robust centralized prediction framework, and is used to solve data islands and data privacy. This framework helps participants to do heterogeneous defect prediction (HDP), predict the defect tendency regarding private models. Concretely, a pre-trained model of public datasets is transferred to the private model, and the model on the private data to converge by fine-tuning, and then the final output in each participant’s private model is conveyed through knowledge distillation  [ 75 ] . Besides, HE is used to encrypt data without disturbing the processing results. Experimental results on 9 projects in 3 public databases (NASA, AEEEM and SOFTLAB) show that FTLKD outperforms the related competing methods  [ 75 ] . Quantifying Performance.  In addition, the authors in  [ 35 ]  analyze  three  major bottlenecks in Federated Transfer Learning and their potential solutions. The main bottleneck is inter-process communication. Data exchange and memory copy in a device can cause extremely high latency. JVM native memory heap and UNIX domain sockets give us the opportunity to alleviate the type of bottlenecks  [ 35 ] . The second bottleneck is in the additional encryption tool that increases computational cost. The last is the traditional congestion control problem. Intensive data exchange causes heavy network traffic  [ 35 ] . Transfer Learning Strategies for Our Framework.  The secure methods from papers  [ 70 ]   [ 17 ]   [ 52 ]   [ 27 ] , show that these are additionally add to Federated Transfer Learning meanwhile keep its original structure. The paper  [ 35 ]  shows that additional secure methods bring a bottleneck to Federated Transfer Learning. Thus, we need no additional security methods but keep the original structure of Federated Transfer Learning. The methods with Pre-trained models  [ 15 ]   [ 26 ]   [ 79 ]  can be considered in our framework. The applications  [ 76 ]   [ 15 ] [ 32 ]  show that domain adaptation can be used to transfer features from server to participants. Methods  [ 45 ] [ 35 ]  can be considered to solve problems where the data distributions of participants are different. However, these two methods require an additional public dataset. In conclusion, the methods of transferring from pre-trained models and strategies with domain adaptation can be considered in our framework.

-----------------------------------------

Table: 2102.02849:S4.T1
Caption: Table 1. Federated Learning Training Policies: Characteristics
References: In this section we review the main characteristics of synchronous and asynchronous federated learning policies, and introduce our novel semi-synchronous policy. Figure 1 sketches their training execution flow.
We compare these approaches under three evaluation criteria: convergence time, communication cost and energy cost. Rate of convergence is expressed in terms of parallel processing time, that is, the time it takes the federation to compute a community model of a given accuracy with all the learners running in parallel. Communication cost is measured in terms of update requests, that is, the number of local models sent from any learner to the controller during training. Each learner also receives a community model after each request, so the total number of models exchanged is twice the update requests. Energy cost is based on the cumulative processing time (total wall-clock time across learners required to compute the community model) and on the energy efficiency of each learner (e.g., GPU, CPU).
Table 1 summarizes our findings (cf. Section 6).

-----------------------------------------

Table: 2109.12062:S4.T3
Caption: TABLE III: Experiments evaluated on test sets. The Local columns refer to the average performance of local models trained on local data. The Federated columns refer to the performance of a single global model, trained with FedAvg [32]. The Synthetic columns refer to the average performance of local models trained with synthetic data coming from the SGDE protocol. All the models are evaluated on a hold-out set. We highlight the average improvement of federated learning (Federated) and SGDE (Synthetic) on the evaluation metrics with respect to local (Local) training.
References: The first set of baseline experiments, identified with the Local keyword, measures the classification accuracy, F1 score, and AUC of a machine learning model over different classification tasks using 10-fold cross-validation.
Results are available in Table I.
Instead, the second set of baseline experiments measures the performance of locally-trained models on external data.
For each client, a machine learning model is trained on the entire local dataset, evaluating its performance on the test set.
The best number of training epochs is found via cross-validation.
The results are collected in Table III. In the set of experiments identified with the Federated keyword, a single machine learning model is trained on a collection of private client datasets using the FedAvg algorithm [32].
Table I collects the average model metrics evaluated on private validation splits from client datasets.
Instead, Table III reports the metrics of the same model evaluated on the test set.
Federated averaging was run until convergence, imposing the average between accuracy, F1, and AUC as validation score for the early stopping criterion. In the experiments marked as Synthetic, each client trains a machine learning model locally using the optimal number of generated samples produced by the generators exchanged with SGDE. The synthetic dataset is constructed from samples uniformly picked from each available generator.
In Table I, the average metrics of local models trained on synthetic samples and evaluated on private client datasets are collected under the Synthetic column. In Table III, instead, are reported the average metrics of the same models evaluated on the test set. The results are confirmed in Table III too, where models are evaluated on the test set.
By combining the Local and Synthetic experiments, there is an average improvement of accuracy and AUC of 1.85% and 0.36%, respectively.
The F1 score increases by 8.06% on average, confirming the strong fairness advantage granted by taking part in the SGDE generator exchange. So far, the discussion focused on the performance difference of training on synthetic samples with respect to training on real local data.
Then, the next natural question is how training on synthetic samples compares to federated learning, the most prominent privacy-preserving training technique for a machine learning model on distributed data.
The interesting result is that in most cases SGDE has still an advantage, not only by design in transparency communication costs, but in the final classification performance too.
The experiments show that SGDE performs similarly, or even outperforms the standard FedAvg algorithm [32] in settings with a small number of clients and unbalanced data distributions.
The result is more evident in the first five rows of Table I and Table III, where the experiments involve small tabular datasets and logistic regression as global machine learning model.

-----------------------------------------

Table: 2109.12062:S4.T1
Caption: TABLE I: Experiments evaluated on local data splits. The Local columns refer to the average performance of local models trained on local data and evaluated with 10-fold cross-validation. The Federated columns refer to the performance of a single global model, trained with FedAvg [32] and evaluated on private validation splits. The Synthetic columns refer to the average performance of local models trained with synthetic data coming from the SGDE protocol. Models in the Synthetic columns are evaluated on the entire local datasets. We highlight the average improvement of federated learning (Federated) and SGDE (Synthetic) on the evaluation metrics with respect to local (Local) training.
References: The first set of baseline experiments, identified with the Local keyword, measures the classification accuracy, F1 score, and AUC of a machine learning model over different classification tasks using 10-fold cross-validation.
Results are available in Table I.
Instead, the second set of baseline experiments measures the performance of locally-trained models on external data.
For each client, a machine learning model is trained on the entire local dataset, evaluating its performance on the test set.
The best number of training epochs is found via cross-validation.
The results are collected in Table III. In the set of experiments identified with the Federated keyword, a single machine learning model is trained on a collection of private client datasets using the FedAvg algorithm [32].
Table I collects the average model metrics evaluated on private validation splits from client datasets.
Instead, Table III reports the metrics of the same model evaluated on the test set.
Federated averaging was run until convergence, imposing the average between accuracy, F1, and AUC as validation score for the early stopping criterion. In the experiments marked as Synthetic, each client trains a machine learning model locally using the optimal number of generated samples produced by the generators exchanged with SGDE. The synthetic dataset is constructed from samples uniformly picked from each available generator.
In Table I, the average metrics of local models trained on synthetic samples and evaluated on private client datasets are collected under the Synthetic column. In Table III, instead, are reported the average metrics of the same models evaluated on the test set. From an individual client perspective, the most compelling question is whether joining the SGDE protocol is beneficial from a utility standpoint.
In Table I, each member of the federated network improved the classification average accuracy and AUC by 2.66% and 0.68%, respectively over all the datasets.
This result allows us to assert that synthetic data coming from generators exchanged with SGDE is more effective than single-client local data to learn a classification task.
Thus, synthetic data can effectively substitute privacy-protected local data in a machine learning procedure.
In fact, in our experiments, SGDE is crucial to increase the amount of available information of a single client to train a machine learning model, without compromising the privacy of the other federation individuals. So far, the discussion focused on the performance difference of training on synthetic samples with respect to training on real local data.
Then, the next natural question is how training on synthetic samples compares to federated learning, the most prominent privacy-preserving training technique for a machine learning model on distributed data.
The interesting result is that in most cases SGDE has still an advantage, not only by design in transparency communication costs, but in the final classification performance too.
The experiments show that SGDE performs similarly, or even outperforms the standard FedAvg algorithm [32] in settings with a small number of clients and unbalanced data distributions.
The result is more evident in the first five rows of Table I and Table III, where the experiments involve small tabular datasets and logistic regression as global machine learning model.

-----------------------------------------

Table: 2406.09831:S3.T3
Caption: Table 3: Pre-Training of LLM in Federated Learning
References: Tian et al.  [ 32 ]  explore the feasibility of applying federated learning to the pre-training phase of BERT (Bidirectional Encoder Representations from Transformers), a popular language model. Named FedBERT, this approach adapts the pre-training process to work across distributed data sets without compromising data privacy. The study demonstrates how federated learning can be effectively utilized to pre-train BERT with data from multiple decentralized sources, ensuring that the private data remains local while still contributing to the training of a robust model. The paper highlights modifications to the BERT training algorithm to accommodate the federated setting, including adjustments to handle the uneven distribution of data across nodes. Focusing on the practical aspects, Agarwal et al.  [ 33 ]  provide a detailed examination of deploying pretrained language models in a federated learning framework. It discusses the challenges and solutions related to synchronization, data heterogeneity, and maintaining model performance when the pretrained model is adapted to new datasets across distributed environments. The paper proposes several optimization techniques to enhance the efficiency and effectiveness of federated learning for pretrained models, such as dynamic update rates and selective parameter updating to cope with the diverse data distributions typically encountered in federated settings. Addressing the challenge of multilingual contexts, Weller et al.  [ 34 ]  introduce techniques for leveraging pretrained language models to enhance language understanding across different languages in a federated learning scenario. It explores the application of multilingual BERT models, adapting them to federated settings to improve model training across linguistically diverse data. This approach not only broadens the applicability of federated learning but also enhances the inclusivity of language technologies, allowing for effective model training without centralizing data from various language communities. Malaviya et al.  [ 35 ]  address one of the significant challenges in federated learning: the high communication overhead. It presents strategies for reducing bandwidth consumption during the pre-training of language models in federated setups. By employing parameter-efficient techniques such as layer-wise relevance propagation and other fine-tuning methods that minimize the number of parameters that need to be updated and transmitted, the paper successfully decreases the network load, which is crucial for practical implementations of federated learning where bandwidth may be limited. These papers collectively cover a spectrum of methodologies for integrating pre-training phases of large language models with federated learning principles. Starting from adapting existing models like BERT to federated settings, moving through practical deployment considerations, addressing multilingual training needs, reducing communication overhead, and finally discussing comprehensive lifecycle approaches, these studies showcase a progressive enhancement in the techniques and applications of federated learning. Each paper builds on the notion that federated learning can be effectively scaled and adapted to pre-train language models in a way that respects privacy, manages resources efficiently, and embraces linguistic diversity, thus advancing the field towards more inclusive and technologically feasible solutions. The four papers significantly advance swarm intelligence principles in federated learning by demonstrating how large language models can be collaboratively pre-trained across distributed nodes. Each paper introduces a different facet of decentralized intelligence:  [ 32 ]  adapts BERT for privacy-preserving collaborative training,  [ 33 ]  tackles practical deployment challenges such as dynamic updating to handle data heterogeneity,  [ 34 ]  extends federated learning to multilingual contexts enhancing linguistic inclusivity, and  [ 35 ]  reduces communication overhead with parameter-efficient techniques. Together, these studies exemplify how federated learning can emulate swarm behavior, optimizing collaborative tasks through decentralized interactions and contributing to the development of robust, scalable models without centralized control.

-----------------------------------------

Table: 2206.10048:S4.T5
Caption: Table 5: FedER classification accuracy w.r.t. buffer size. Each local model is evaluated on all test sets of the federation in order to measure convergence and generalization (lower standard deviation corresponds to higher convergence).
References: We then compare our approach to state-of-the-art federated learning approaches, namely: a) centralized federated methods, FedAvg [23] and FedProx [24], which have shown to perform generally better than decentralized methods [39, 37], and b) a personalized method, FedBN [25].
As already mentioned, to avoid biased assessment, we use the official code repository222https://github.com/med-air/FedBN of FedBN [25] and hyper-parameter selection on the tested datasets was carried out through grid search on training rounds/epochs, learning rate and μ𝜇\mu for FedProx [24] using 5-fold cross validation as for our approach.
Results, for the tuberculosis and the melanoma tasks, are reported in Table 4 and show that FedER outperforms all methods under comparison. Interestingly, FedER learning strategy does better than: a) centralized methods, FedAvg [23] and FedProx [24], suggesting that experience replay is a more effective feature aggregation approach than naive parameter averaging; b) personalized methods, such as FedBN [25], which affects a limited aspect of feature representation (i.e., input layer distributions), while our approach adapts the entire model to local and remote tasks.
These above results suggest that experience replay plays a key role in federated models as a principled way to integrate features coming from different data distributions. To further assess its contribution, we evaluate FedER performance when using buffer at different sizes. Results on the tuberculosis task, measured as mean and standard deviation of the local node models over a given dataset, are shown in Table 5 and indicate a clear contribution of the buffer in terms of overall performance and models’ agreement. Indeed, with no buffer we obtain the lowest average performance and the highest standard deviation. As the buffer is enabled, we can observe a performance gain (mainly for the Shenzhen dataset) and a significant drop in standard deviation. Performance improves as buffer size increases, although gain becomes negligible above 512. Since higher buffer sizes result in more data to be shared among nodes, we use a buffer size of 512, as the best trade-off between accuracy and communication costs.

-----------------------------------------

Table: 2206.10048:S4.T6
Caption: Table 6: Classification accuracy of the proposed privacy-enhanced strategies in the non-i.i.d. setting. FedER-A: only buffers are shared (Fig. 5-a). FedER-B: models are trained on synthetic data only (Fig. 5-b). Node performance measures how each node model performs on its own private dataset, while node convergence assesses how a node model performs on other federation nodes.
References: Given the high realism of generated samples, we run additional tests by proposing two FedER variants aiming to increase the level of privacy preservation: a) FedER-A: models are not shared among nodes — only synthetic buffers are sent and received; b) FedER-B: models are trained only using synthetic data, even on local nodes. Fig. 5 shows the internal architecture of each node in the two variants. Results obtained with these alternative privacy-enhanced configurations are provided in Table 6. It can be noted that FedER-A (i.e., “buffer-only sharing”) configuration achieves comparable performance to our standard FedER (82.76 vs 83.41), but, remarkably, it outperforms all existing federated learning methods on the same datasets (compare Table 4 with the node performance block in Table 6). The FedER-B (i.e., “synthetic-only training”) configuration, instead, performs slightly worse than the other two configurations, but is still on par with existing federated methods.

-----------------------------------------

Table: 1911.05861:A0.T1
Caption: Table S1: 
Model performance and privacy ϵitalic-ϵ\epsilon attained at each hospital following federated differentially private learning. Results shown are the best AUC-ROC on the validation set achieved over with early-stopping over ten rounds of federated learning. Prior to federated learning, the DP-SGD hyperparameters are selected separately at each hospital to maximize the local AUC-ROC after ten epochs of local differentially private training.

References: Prior to experimentation with differentially private training, we aimed to establish the efficacy of federated learning over centralized and local learning. We find that while there is often a benefit to federated learning over local learning, often attaining an AUC-ROC comparable with that of centralized learning, the improvements are often not large enough to be rendered statistically significant on the basis of the 95% confidence interval for the difference in AUC-ROC between either the central or federated model with the corresponding local model (Table 1).
In particular, centralized and federated learning for prediction of prolonged length of stay improve on local learning for thirteen and twelve hospitals, respectively, whereas centralized and federated learning only benefit mortality prediction in seven and five cases, respectively. When the records from all hospitals are aggregated for differentially private centralized training, it is feasible to attain relatively strong privacy guarantees (ϵ≈1italic-ϵ1\epsilon\approx 1) if σ/S=1.0𝜎𝑆1.0\sigma/S=1.0 and S=10.0𝑆10.0S=10.0 (Figure 1) with a relatively minor reduction in terms of the validation AUC-ROC at the end of training (prolonged length of stay 0.763 vs. 0.73; mortality 0.876 vs. 0.832). When attempting to perform federated learning in a differentially private manner, we find that even with DP-SGD hyperparameters selected on the basis of local training, the models derived from differentially private federated learning often perform poorly in terms of both AUC-ROC and ϵitalic-ϵ\epsilon, and that this effect is exacerbated for mortality prediction (Table S1). It is likely that a practical tuning strategy for differentially private federated averaging could be identified with further experimentation, but it is unclear if such a strategy would generalize to similar data sets and prediction tasks. This is problematic, for both this and related work, as neglecting to account for the privacy cost of model selection produces optimistic underestimates of the privacy costs [25, 26]. In future work, it is of interest to conduct controlled experiments to directly compare our approach to cyclical weight transfer [9] and split learning [18, 19, 20] to gain insight into the relative efficacy of differentially private federated averaging over alternatives.

-----------------------------------------

Table: 2409.01235:S2.T2
Caption: Table 2:  MAE [95% confidence interval] of BrainAge models trained locally and in a federated way. Models were trained and tested in The Maastricht Study (TMS) and the Rotterdam Study (RS), using the Leiden Longevity Study (LLS) as the external test cohort.
References: As shown in Table 2, the federated BrainAge model demonstrated its capability to predict the chronological age across different cohorts with mean absolute errors (MAE) of 5.59 years in the RS test set, 4.36 years in the TMS test set, and 4.60 years in the external test set (LLS). This federated model showed a better performance than local models that were tested on data from a different cohort. To highlight, the local model trained on RS data (training MAE = 2.48, test MAE = 4.21) achieved lower performance on TMS (test MAE = 6.45) and LLS (test MAE = 5.66) than the federated model. In addition, the local model trained on TMS data (training MAE = 3.10, test MAE = 4.72) also achieved lower performance on RS (test MAE= 7.29) and LLS (test MAE = 5.63) than the federated model, suggesting the two local models were unable to maintain performance when tested with data from a different cohort. Furthermore, the federated model showed more similar results of each subset compared to the local models, indicating less overfitting. These observations are supported by the results obtained with the 3-fold cross-validation approach (Supplementary Table 3), which indicates similar MAE estimates for the three models. Model optimization for BrainAge is detailed in Supplementary Table 2. In the federated architecture, selecting the model with the lowest MAE from each cohort showed optimal convergence. Performance was most similar between cohorts when models were aggregated between cohorts with equal importance weighting. Finally, increasing the number of epochs per round of training did not improve the performance.


Optimized model hyperparameters were an initial learning rate of 1×10−31superscript1031\times 10^{-3}, a learning rate decay of 1×10−21superscript1021\times 10^{-2}, and a dropout rate in the last layer of 5×10−15superscript1015\times 10^{-1}. We employed the Adam algorithm to train the network for 20 rounds, three epochs each round and used a batch size of eight, the maximum possible due to computing memory limits. Computation time for federated training of the BrainAge model was 64% higher on average than that of a central training (Table 3). The application of the federated BrainAge model to the NCDC cohorts revealed the need to further improve the hyperparameters and aggregation methods compared to the initial experiment with ADNI data. The heterogeneity of the dataset affected the model’s convergence and exacerbated the problem of overfitting one of the cohorts.


To assess the impact of the model training options (number of epochs by round, model selection, and weighted averaging based on sample size), we repeated a 3-fold cross-validation for each. The results in Table 2 suggest that a smaller number of epochs, no weighted averaging, and selecting the local model with higher MAE improves the performance. Regarding the hyperparameters, we observed that a higher learning rate decay (1×10−21superscript1021\times 10^{-2} vs 1×10−41superscript1041\times 10^{-4}) and dropout rate (0.5 vs 0.25) benefited convergence.

-----------------------------------------

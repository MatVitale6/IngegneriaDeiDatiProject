Query: metrics comparison for GPT models on summarization

Table: 2306.06687:id_table_4
Caption: Comparison of results of Binary Locating Metric and GPT Metric of existing MLLMs. The Binary-Locating Metric is the accuracy of the predicted position, and the GPT Metric is the score from GPT response.
References: Binary Locating Metric.          Table                  4                shows the zero-shot results of the MLLMs on the proposed Binary Locating Metric and GPT Metric.The Binary Locating Metric covers the data from VOC2012, FSC147, and LSP.Since the our baseline model has been trained on a small amount of data with detection instructions, it significantly improves in localizing accuracy. GPT Metric.          We calculated GPT scoresusing a variety of tasks, including VQA, classification, captioning, as well as a small number of detection and counting tasks.As shown in Table                  4                , LLaVA surpasses other models in performance, while LAMM, although slightly lower than LLaVA, still outperforms Minigpt4 and mPLUG-owl by a wide margin.

-----------------------------------------

Table: 2311.15562:S3.T9
Caption: Table 9: Results for the state-of-the-art vision and language models on our dataset, evaluated with reference-based metrics ( ROUGE-L, METEOR, BERTscore, RefCLIP-S, and GPT-4) and reference-less metric (CLIP-S). Models or scores labeled with "~" are evaluated on 1,903 random samples. We only reported GPT-4V model and GPT-4 metric on samples due to the high cost of calling GPT-4V and GPT-4’s API.
References: Results are shown in Table 9.
We observe that all models perform poorly, especially when considering the results from the METEOR and GPT-4 metrics. In other words, considerable progress still remains to achieve the status of some modern models [54, 55, 44] which are purported to match or even surpass human performance on some VQA datasets [17, 37]. Supplementing the main paper, where we reported the performance of the six models with respect to the three most human-aligned metrics (i.e., METEOR, GPT-4 metric, and BERTscore), we report here performance with respect to all six metrics. Overall results are shown in Table 9. We observed that all models perform poorly on our dataset, especially when considering ROUGE-L, METEOR, and GPT-4 metrics.
When comparing across metrics, we find that all metrics except CLIP-S indicate GPT-4 and mPLUG-Owl are the best two models.

-----------------------------------------

Table: 2404.14680:S3.T2.1.1
Caption: Best GPT translation metrics for each language, computed by the best mean translation quality over all tested sentence
References: Table 2 summarizes the best performing GPT models for translating 50505050 foreign languages, using the four different translation metrics. Table 2 reports the best mean translation quality per sentence which are given by the rounded value to 3333 decimal places. The final aggregate metric of the best performing GPT model across all languages, for each of the language quality metrics, is computed as the mean of the vector of all 50505050 language scores (this aggregate metric is not weighted by the different amounts of sentences that were translated in the language dataset).
 Notably, of the 16161616 GPT models, only a small subset of these was the best performing for any tuple of language and translation quality metric. Specifically, the models that had the best mean scores (for any combination of language and translation quality measure) were; ReMM-v2-L2-13B, Turdus, Llama2-chat-AYT-13B, wizardLM-13B-1.0-fp16, and zephyr-7b-alpha. ReMM-v2-L2-13B was the best performing model overall. Importantly, for each language, the translation scores shown in Table 2 were computed on the exact same translated sentences, but the best performing GPT model was not always the same across the 4444 translation quality metrics.
 Table 3 shows the mean translation quality metrics, for the four language metrics, across all 50505050 languages being translated into English, using Google translate. The same test sentences translated by the GPT models, for each language, were also translated using Google translate - therefore the entries in Table 3 should be compared against the best performing GPT models in Table 2. These results show the performance of Google translate, using it as a reasonable performance benchmark for automated machine translation of languages. Interestingly, there were exactly two languages where, for at least one of the language metrics (although, in these cases it was for all four language quality metrics), the best performing GPT model had better mean sentence translation quality than google translate. These two languages were French and Chinese. For all other languages, either the best performing GPT model was definitively worse at translating, or was comparable to within a small margin. The languages for which the best performing GPT model and google translate performed marginally the same were German, Spanish, Italian, Russian, Korean, Serbian, Japanese, Ukrainian, Vietnamese, and Bosnian.


-----------------------------------------

Table: 2404.15821:S2.T1
Caption: Table 1: Different features of related works. The table presents an overview of some readily available evaluation tools for evaluating tabular synthetic data. “Ease of Use” denotes how easily the tool can be integrated into an experimental setting. Configurability denotes if the tool allows bundling and customising metrics. Extendibility denotes if new custom metrics can easily be added, and Ranking denotes if the tool can help if multiple synthetic datasets are to be decided between. “Mixed Type Strategy” denotes the primary strategies the tool uses for taking care of numerical and categorical data. The number of metrics in SynthCity is approximate because a few of them are distinct implementations of the same metric (e.g. using different classifiers). For SynthEval, the “18+12” metrics signify that we have 18 different metric modules, but some of them produce supplementary results.
References: A few ecosystems for evaluating synthetic data already exist. Some that have usable Python implementations are listed in Table 1 above. Synthcity by [16] hosts a variety of metrics in addition to an intuitive interface for accessing a selection of generative models. However, categoricals are treated as ordinal integers, making the evaluation slightly biased towards numerical values (see Figure 1). Output values are mapped to the zero-one interval, which makes them clear to read but hides away some details. There is potential for confusion when metrics with the same units are mixed and some are to be “maximised” and others “minimised”. SDmetrics222Also available as a module in their SDGym framework: https://docs.sdv.dev/sdgym by [17] allows users to generate quality and diagnostic reports, as well as column-level figures for visual comparison. The selection of metrics is mostly limited to high-level dataset similarity and integrity with only a few recognised metrics appearing. The framework requires a metadata file, which is non-trivial to create and compile, and was thus not included in the figure. Table Evaluator by [18] is a library to evaluate how similar synthesised datasets are to real data through quantitative and visual evaluation. The evaluation is at large based on the performance of classifier models, together with a few other statistical methods. Metrics such as the correlation matrix are adapted to work for categorical data and one-hot encoding is used for nearest neighbour calculations. The tool produces a summary score that can be used for a quick ranking of datasets. The performance varies quite a bit for heterogeneous data. DataSynthesizer was developed by [4] and includes two metrics that check the quality of synthesised data visually using pairwise mutual information and marginal distributions. Numerical data are made discrete for this purpose by binning.
Footnotes: Granulation: Ease of Use; ⋆⁣⋆⁣⋆⋆⋆⋆\star\star\star few lines of code to get all necessary results, ⋆⁣⋆⋆⋆\star\star minor work required e.g. dataset encoding or looping over metrics, ⋆⋆\star major preprocessing, coding, or additional files are needed. Config.; ⋆⁣⋆⁣⋆⋆⋆⋆\star\star\star metrics can be selected as desired AND metrics have accessible options, ⋆⁣⋆⋆⋆\star\star either one of previous, ⋆⋆\star none of previous.

-----------------------------------------

Table: 2410.09776:A9.T17
Caption: Table 17: Comparison of our best model with Qwen-VL and GPT-4o
References: Comparing Block A with C and D, we observe that our proposed method with T5 and BART outperforms existing baseline methods by massive margins. Note that though these baselines are old, unfortunately, there does not exist better (or recent) baselines for the video question generation problem. Further, comparing Block B with C and D, we observe that fine-tuning Transformer-based encoder-decoder models are better than prompt engineering with Alpaca and GPT-3.5-Turbo. Note that we experiment with 19 different prompts for Alpaca and GPT-3.5-Turbo and report the best results (using prompt p2) in the table. Detailed prompt engineering results are in Appendix D. We also experiment with Qwen-VL <cite class="ltx_cite ltx_citemacro_cite">Bai et&#160;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09776v1#bib.bib5" title="">2023</a>)</cite> and GPT-4o&#160;
Bai et al. (2023) and GPT-4o <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2410.09776v1#bib.bib49" title="">2024</a>)</cite> models. It is evident that our best model outperforms these zero-shot state-of-the-art models. To illustrate this, we highlight a few samples in Table&#160;
OpenAI (2024) models. It is evident that our best model outperforms these zero-shot state-of-the-art models. To illustrate this, we highlight a few samples in Table 17 in the Appendix for a clear comparison.

-----------------------------------------

Table: 2408.05242:S4.T3
Caption: Table 3: The detailed statistics encompassing various trainable model parameters and sizes are essential components of our federated learning implementation aimed at constructing a context-based GPT.
References: Context GPT Evaluation: For the evaluation of the models generated by the client machines, we employed metric scales such as Rouge-1, Rouge-2, Rouge-L, and BLEU-4. These metrics assess the quality of the generated output by evaluating the uni-grams, bi-grams, and N-grams of word sequences in comparison to the original sentences. Table 1 provides a detailed breakdown of these evaluation metrics for both the global server and the four client machines, highlighting the overall performance across different measurement scales. Additionally, Table 2 showcases the performance levels achieved by implementing the P-Tuning PEFT strategy, incorporating the latest version for enhanced accuracy. Furthermore, Table 3 offers insights into the training parameters and sizes of the GPT-generated models across the client machines, illustrating the various trainable model sizes and parameters.

-----------------------------------------

Table: 2408.05242:S4.T3
Caption: Table 3: The detailed statistics encompassing various trainable model parameters and sizes are essential components of our federated learning implementation aimed at constructing a context-based GPT.
References: Context GPT Evaluation: For the evaluation of the models generated by the client machines, we employed metric scales such as Rouge-1, Rouge-2, Rouge-L, and BLEU-4. These metrics assess the quality of the generated output by evaluating the uni-grams, bi-grams, and N-grams of word sequences in comparison to the original sentences. Table 1 provides a detailed breakdown of these evaluation metrics for both the global server and the four client machines, highlighting the overall performance across different measurement scales. Additionally, Table 2 showcases the performance levels achieved by implementing the P-Tuning PEFT strategy, incorporating the latest version for enhanced accuracy. Furthermore, Table 3 offers insights into the training parameters and sizes of the GPT-generated models across the client machines, illustrating the various trainable model sizes and parameters.

-----------------------------------------

Table: 2407.14926:S4.T2
Caption: TABLE II: Performance comparison of GPT, Gemini, and Claude
References: Table II compares the performance of three models - GPT, Gemini, and Claude - across four key metrics. GPT achieves the highest scores in three out of the four metrics, with a connectivity score of 0.78, an avoidance score of 0.78, and the lowest approximate time of 0.51. Gemini, on the other hand, has the lowest number of transfers at 3.00, but fails for the other three metrics. Claude falls in between GPT and Gemini. These results suggest that GPT offers the best overall performance, achieving a good balance between high connectivity and avoidance scores, low approximate time, and a reasonable number of transfers.

-----------------------------------------

Table: 2402.13064:A1.T7
Caption: Table 7: Pairwise comparison on various skills between GLAN and other models on Evol-Instruct testset. The scores are the average gap of scores assigned by GPT-4, calculated as avg_score(GLAN)−avg_score​(x)avg_score(GLAN)avg_score𝑥\text{\tt avg\_score({GLAN}{})}-\text{\tt avg\_score}(x).
References: Evol-Instruct testset [39] contains real-world human instructions from diverse sources and it consists of 218 instances with 29 distinct skills. Each instruction is associated with a difficulty level from 1 to 10. The responses are often open ended descriptions and we believe this benchmark is a necessary supplement to IFEval (answers to their instructions are “verifiable”).
Following [39] and [7], we adopt a GPT-4-based automatic evaluation method to conduct a pairwise comparison between GLAN and other models.
Specifically, GPT-4 is instructed to assign a score between 1 and 10 overall score w.r.t. the helpfulness, relevance, accuracy, and level of detail of responses generated by two different models for a given input question. A higher score indicates better overall performance. To mitigate potential order bias, we perform bidirectional comparisons for each response pair and determine their average score. The average score difference to GLAN (i.e., avg_score(GLAN)−avg_score​(x)avg_score(GLAN)avg_score𝑥\text{\tt avg\_score({GLAN}{})}-\text{\tt avg\_score}(x)) serves as the final metric.
Table 5 presents the results of pairwise comparisons across various levels of instruction difficulty.
GLAN showcases superior performance compared to LLaMA-2, Orca 2, Mistral Instruct, and even WizardLM-13B (note that GLAN contains only 7B parameters) on most difficulty levels and overall scores.
This suggests that GLAN demonstrates improved ability to process diverse instructions, regardless of their difficulty or complexity.
Also note that GLAN falls behind GPT-3.5-turbo as other models in comparison. Additionally, we group Evol-Instruct test according to the 29 skills and we observe the same trends. Detailed results are in Appendix (Table 7). GLAN demonstrates strong performance on most skills especially on Math, Coding and Reasoning. However, it slightly falls short in common-sense related tasks.

-----------------------------------------

Table: 2402.13064:S3.T5
Caption: Table 5: Pairwise comparison on various difficulty levels between GLAN and other models on Evol-Instruct testset. The scores are the average gap of scores assigned by GPT-4, calculated as avg_score(GLAN)−avg_score​(x)avg_score(GLAN)avg_score𝑥\text{\tt avg\_score({GLAN}{})}-\text{\tt avg\_score}(x).
References: Evol-Instruct testset [39] contains real-world human instructions from diverse sources and it consists of 218 instances with 29 distinct skills. Each instruction is associated with a difficulty level from 1 to 10. The responses are often open ended descriptions and we believe this benchmark is a necessary supplement to IFEval (answers to their instructions are “verifiable”).
Following [39] and [7], we adopt a GPT-4-based automatic evaluation method to conduct a pairwise comparison between GLAN and other models.
Specifically, GPT-4 is instructed to assign a score between 1 and 10 overall score w.r.t. the helpfulness, relevance, accuracy, and level of detail of responses generated by two different models for a given input question. A higher score indicates better overall performance. To mitigate potential order bias, we perform bidirectional comparisons for each response pair and determine their average score. The average score difference to GLAN (i.e., avg_score(GLAN)−avg_score​(x)avg_score(GLAN)avg_score𝑥\text{\tt avg\_score({GLAN}{})}-\text{\tt avg\_score}(x)) serves as the final metric.
Table 5 presents the results of pairwise comparisons across various levels of instruction difficulty.
GLAN showcases superior performance compared to LLaMA-2, Orca 2, Mistral Instruct, and even WizardLM-13B (note that GLAN contains only 7B parameters) on most difficulty levels and overall scores.
This suggests that GLAN demonstrates improved ability to process diverse instructions, regardless of their difficulty or complexity.
Also note that GLAN falls behind GPT-3.5-turbo as other models in comparison. Additionally, we group Evol-Instruct test according to the 29 skills and we observe the same trends. Detailed results are in Appendix (Table 7). GLAN demonstrates strong performance on most skills especially on Math, Coding and Reasoning. However, it slightly falls short in common-sense related tasks.

-----------------------------------------

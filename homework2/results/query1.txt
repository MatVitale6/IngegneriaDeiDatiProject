Query: f1 score on dataset wdc

Table: 2209.00721:S5.T5
Caption: Table 5: Evaluation of baselines and proposed method on the four datasets/silos (sampled version). The average local F1-score represents the average from each dataset. The average cross F1-score the average from all cross-evaluations. Our proposal and the Autoencoder without (w/o) EFC reports the F1-score on the 10th round of the FL setup.
References: The next step was evaluating our proposed method in contrast to the baselines from section 4.6. The baselines are evaluated locally, with the training and testing sets derived from the same silo. Next, the baselines are evaluated on a cross-evaluation setup, training on the original dataset silo but testing on the test set from the other silos. For instance, training on Bot-IoT, testing on TON-IoT, UNSW-NB15, and CSE-CIC-IDS-2018. The cross-evaluation is then summarized as average F1-scores. The local evaluation is summarized on an average F1-score achieved on each of the four silos for each baseline. The results are summarized on the Table 5. We can confirm the best local performance for Energy Flow Classifier (EFC) by analyzing the results from Table 5. Also, the EFC presented a lower standard deviation representing a more stable performance over the four silos in a localized evaluation setup. By contrast, both Isolation Forest (IF) and Local Outlier Factor (LOF) presented high standard deviations. For instance, IF presented a F1-score of 0.950.950.95 on the sampled version of UNSW-NB15 but 0.010.010.01 on the sampled version of the Bot-IoT (0.580.580.58 for the sampled TON-IoT, and 0.920.920.92 for CSE-CIC-IDS-2018). Similarly, the LOF presented a high F1-score for the sampled version of UNSW-NB15 (0.970.970.97) and a low F1-score of 0.010.010.01 for the Bot-IoT (0.720.720.72 for sampled TON-IoT and 0.930.930.93 for sampled CSE-CIC-IDS-2018). As reported in Table 5, our proposal, stacking of the autoencoder and EFC, was tested in two possible usages: using two thresholds for benign and attack samples (best performer, 0.840.840.84) and only one threshold for just benign samples (0.770.770.77). In both cases, the proposed method outperforms all unsupervised methods under consideration.

-----------------------------------------

Table: 2405.17129:id_table_1
Caption: F1-score, precision and recall on the test dataset including the EXALT baseline results.
References: In terms of individual models, the Zero-Shot (gpt4o) with Explanation and Correction model (ZSEC-gpt4o) achieved the best performance, achieving an F1-score of 0.5726. Other models, such as ZSEC-gpt4turbo and Multi-Binary-Classifier Agentic Workflow (MBCAWF), also perform competitively, with F1-scores exceeding 0.55. The overall system performance further improved through the use of Agentic Workflow and ensemble methods. Notably, the Ensemble-19 model achieves the highest F1-score of 0.6046 on the test dataset, outperforming the EXALT baseline by approximately 0.17 F1-score, ranking second. Results for all submitted single models, Agentic Workflow models, and ensembles for the emotion detection task are presented in Table               1             , with baseline results provided by the EXALT organizers.

-----------------------------------------

Table: 2205.10234:S4.T4
Caption: Table 4: Grid search cross-validation F1-scores compared to the F1-score on a treatment’s own testing set.
References: Table 4 displays the relationship between cross-validation F1-scores and the F1-scores of applying the treatments to the held-out test set. The goal of the grid search is to find the combination giving the highest F1-score (CV Max F1), and in this way it aims to find a combination which has a comparably high F1-score on the held out testing set. For the final models of the data-centralised, federated, and institution A treatments, the F1-score on their own test set is higher than the CV Max F1-score. This indicates that the hyper-parameters picked during cross validation provides a decent F1-score on the held out test set. Only for institution B the opposite was true as the F1-score on its own test set is lower. In an ideal situation, a similar F1-score is preferred as the cross-validation would then provide the most realistic carry-over value.

-----------------------------------------

Table: 2309.17113:S10.T6
Caption: Table 6: Execution time in seconds, with the F1 score in parentheses.
References: In Table 6, we provide information on the training time for each model on individual datasets, along with the corresponding F1 scores enclosed in parentheses. Additionally, we present the average execution time and F1 score across the three benchmark datasets (IMBD, DBLP, and ACM), as well as for Freebase and synthetic datasets.
All the models achieve a similar average accuracy on the three benchmark datasets, with RGCN displaying the shortest execution time and GTN the longest. Despite being the second slowest model in terms of execution time, our model achieves the highest average F1 score. It is important to emphasize that our model is designed to learn meaningful meta-paths in networks with a multitude of relation types, whereas the three benchmark datasets only have a limited number of relation types.
In the Freebase network, both FastGTN and GTN boast the shortest execution times, yet their F1 scores are significantly lower, rendering them ineligible for further consideration.
Despite Simple-HGN being the quickest model, its F1 score falls significantly short, averaging 15 points lower than MP-GNN. On the other hand, RGCN is the second fastest model, but once more, its F1 score lags behind our approach by an average of 9 points.
Lastly, concerning the synthetic datasets, our model ranks third in terms of execution time, with Simple-HGN excluded from the analysis due to its poor F1 scores. It’s worth highlighting that the fastest model, FastGTN, and the second fastest, RGCN, both lag behind MP-GNN by 23 and 17 points, respectively.
In general, our approach is consistently neither the slowest nor the fastest; however, it consistently attains the highest average F1 score in all scenarios.

-----------------------------------------

Table: 2403.10737:S4.T2
Caption: TABLE II: Within-domain evaluation for the base model (Swin Transformer [51]) on different datasets.
F1-score (% ↑↑\uparrow) is the evaluation metric. GFT has a lower average F1-score than BP4D and DISFA.
References: Table II shows the within-domain results for the base model on the three real datasets. From the table, we observe that GFT has a lower average F1-score than BP4D and DISFA. We suspect it is because BP4D and DISFA have better video quality than GFT.

-----------------------------------------

Table: 2309.17113:S9.T5
Caption: Table 5: Macro-F1 scores for three distinct Freebase datasets
References: The scoring function we propose incrementally builds relevant meta-paths. In this section, we replace the meta-paths identified by the scoring function with those obtained through conventional mining methods. In particular, we rank meta-paths using the well-known path-ranking-algorithm (PRA)[11]. Subsequently, we choose the top three most relevant meta-paths with lengths of 2, 3, and 4. These selected meta-paths are then used to train the MP-GNN.
In Table 5, you can observe the macro-F1 scores for three distinct Freebase datasets: PNC, EDC, and TS. It’s important to note that we did not conduct this experiment with IMBD, DBLP, and ACM due to their limited number of relations (3 and 4). Table 5 clearly indicates that the meta-paths identified by the scoring function exhibit significantly higher predictive capabilities compared to those obtained through PRA.

-----------------------------------------

Table: 2011.00695:S3.T1
Caption: Table 1: The event-based F1 score
References: For the DCASE 2018 test set and DCASE 2019 synthetic set, we compare both the event-based F1 score and audio tagging F1 score.
As shown in Table1, for DCASE 2018 test set, the baseline system achieves an event-based F1 score event-based F1 score of 0.315 while the IFD system achieves an event-based F1 score of 0.331.
It can be seen that for event-based F1 score, which is the main evaluation index for SED, the system with domain adaptation can achieves better performance than the baseline system on real-world data set. It proves that the IFD domain adaptation can improve the model performance on real data. In addition, for the event-based F1 score on DCASE 2019 synthetic set, the model with domain adaptation can also achieve better performance than the baseline model, which further prove this conclusion.

-----------------------------------------

Table: 2402.14042:S4.T1
Caption: Table I: LSTM model performance including RMSE and F1 scores of different datasets and the dataset combinations. 
References: Predictive modeling:  Once our models have reached an optimal prediction accuracy, the models were compared concerning the F1 score and RMSE to determine the datasets which yield the best model performances for LSTM. In Table I it is visible that the LSTM model performs best on the real dataset. However, the dataset generated by PPGAN combined with the real dataset yields an acceptable performance with a relatively low RMSE value, followed by DG and PPGAN. From these findings, we can infer that the generated data by DG and PPGAN have less destructive effects when combined with the real data.

-----------------------------------------

Table: 2211.12874:S4.T3
Caption: Table 3: Global model’s average accuracy, F1-score, AUC and FPR comparison between FedAvg and DW-FedAvg for 20 rounds
References: In this section, we summarize the experimental results of our proposed DW-FedAvg and make a comparison with traditional FedAvg  [ 8 ]  under different number of clients and different number of rounds. For our experiment we have considered a total of 3 different client scenario (5, 10, and 15). Also, two different rounds are considered such as 10 and 20. The average test average test accuracy, F1-score, AUC and FPR of the global model are presented in TABLE   2  and   3 . The better results are in bold in the table. For the Malgenome dataset our proposed DW-FedAvg approach has shown almost similar score or greater score in all aspect (accuracy, F1 score, auc score and FPR score) compare to the traditional FedAvg approach. In the first comparison  2 , (where no. of rounds is 10) there are 5 clients. Our proposed DW-FedAvg approach provides an increased accuracy and F1 score of 0.02% and an increased auc score of 0.05% and a decreased FPR score of 0.21% than the FedAvg approach for those first 5 clients. For 10 clients, it gives almost similar accuracy, F1 score and auc score as that of FedAvg approach, whereas the FPR score has been increased to 0.93%. Likewise for 15 clients, it gives a slight decrease or almost similar accuuracy, F1 score and auc score as that of FedAvg approach, whereas the FPR score has been increased to 0.48%. Similarly for round 20  3 , it is also tested among 5, 10 and 20 clients. For 5 clients, it gives almost similar accuracy, F1 score and auc score as that of FedAvg approach, whereas the FPR score has been increased to 0.36%. For 10 clients, our proposed approach provides an increased accuracy of 0.17% and F1 score of 0.23% and an increased FPR score of 0.1% than the FedAvg approach. For 15 clients, it gives a slight decrease or almost similar accuracy, F1 score and auc score as that of FedAvg approach, while the FPR score has increased to 0.23%. Similarly for the Drebin dataset, our DW-FedAvg approach also provides almost similar score or greater score in all aspect compare to the traditional FedAvg approach. This dataset has also been tested through two rounds-10 and 20. In round 10   2 , it is tested among 5, 10 and 20 clients. For 5 clients, the model provides a significant increase of accuracy of 0.32%, F1 score of 0.46% and auc score of 0.01% as that of FedAvg approach. For 10 clients, our proposed approach gives almost similar accuracy, F1 score and auc score as that of FedAvg approach, whereas the FPR score has increased to 0.19%. For 15 clients, our DW-FedAvg approach provides an increased accuracy of 0.37%, F1 score of 0.51%, auc score of 0.05% and an FPR score of 0.42% than the FedAvg approach. Similarly for round 20   3 , it is tested among 5, 10 and 20 clients. For 10 clients, a significant increase of accuracy score of 0.48%, F1 score of 0.7% and auc score of 0.04% is observed in our approach than the FedAvg approach. For 5 and 15 clients, it gives a slight decrease or almost same accuracy, F1 score, auc score and FPR score as that of FedAvg approach. For the Kronodroid dataset, our proposed DW-FedAvg approach provides almost similar score or greater score as well in all aspect i.e., accuracy, F1 score, auc score and FPR score, in compare to the traditional FedAvg approach. Likewise, the other datasets, this dataset has also been tested through two rounds-10 and 20. In both the rounds, it is tested among 5, 10 and 20 clients. In round 10   2 , for 5 and 10 clients, our proposed approach gives a slight decrease or almost similar accuracy, F1 score, auc score and FPR score as that of FedAvg approach. For 15 clients, our proposed approach gives almost similar accuracy, F1 score and the auc score, whereas there is a significant increase of the FPR score of 0.31% than the FedAvg approach. In round 20   3 , for 20 clients, our proposed approach gives almost similar accuracy, F1 score, auc score and FPR score as that of FedAvg approach. For 5 and 10 clients, the accuracy, F1 score and the auc score of our approach almost remains same as that of FedAvg approach, while the FPR score has been increased to 0.1% and 0.13% respectively than the FedAvg approach.
Likewise, for Tuandromd dataset, our DW-FedAvg approach also provides almost similar score or greater score in all aspect in compare to the traditional FedAvg approach. This dataset has also been tested through two rounds-10 and 20. In round 10   2 , it is tested among 5, 10 and 20 clients. For 5 and 15 clients, our approach provides almost similar accuracy, F1 score, auc score and FPR score to that of FedAvg approach. For 10 clients, there is a significant increase of accuracy by 0.17%, F1 score by 0.1% and auc score by 0.02% and a slight decrease of FPR score by 0.07%. In round 20   3 , for 5 clients, our approach provides almost similar accuracy, F1 score, auc score and FPR score to that of FedAvg approach. For 10 clients, our DW-FedAvg approach provides a significant increase of the accuracy by 0.24%, F1 score by 0.14% and auc score by 0.02% and a slight decrease of FPR score by 0.07% than the FedAvg approach. For 20 clients, our approach provides an increase in auc score by 0.21%, whereas the accuracy, F1 score and the FPR score remains almost same as that of FedAvg approach. The FedAvg approach takes the average of the local models. Our DW-FedAvg approach adjusts the weight based on the model performance. Our approach rewards best performing models whereas penalizes the poor performing models. In case of too many local models with poor classifiers, our DW-FedAvg approach dynamically penalizes weights based on their performance, and thus minimum number of models are getting rewarded. Therefore, the dynamic average overall decreases the accuracy of the model. On the other hand, traditional FedAvg performs average on the local models and does not dynamically adjust the weights of the local models, thus it does not have any effect on accuracy degradation. In traditional FedAvg approach, simple averaging of the local models is done, and it results into a global model. Thus, equal priority is given to all the models. On the other hand, our DW-FedAvg approach dynamically adjusts the weight of the local models based on their performance. Our approach penalizes the poor performing classifiers, whereas rewards the best performing classifiers. By doing the simple average can create a bad global model for the fact that if a global model performs poorly. Our approach prioritizes the best performing models. As more priority is given to better performing models, the global model gives a better result in the accuracy of our approach. Finally, from the detailed comparison shows that our approach has outperformed traditional FedAvg for some clients in both the rounds for both the datasets. It also shows the advantage of dynamically adjusting the weights based on the best performing and poor performing models for both the rounds.

-----------------------------------------

Table: 2407.15569:S4.T2.2
Caption: Table 2: Evaluation in F1 score
References: We compared the performance of the models using the RAFT method and the baselines. Table 1 and Table 2 show the results for the EM score and F1 score respectively. In the HotpotQA[Oracle] experiment group, only oracle documents were provided as references for the model in the RAG experiments. For all other groups, distractor documents were included alongside the reference documents in the RAG experiments. Since the ”yes/no” QA of PubMedQA and QA of HotpotQA are both short-form, we also assessed the long-form QA in dataset PubMedQA. The experiment results are shown in Table 2 under the PubMedQA[long] group. The results in F1 score of long-form QA indicate that RAFT method brought about a 13% performance improvement for long-answer questions over zero-shot prompting baseline. However, compared to the DSF+RAG baseline, the performance gain was less prominent than for the short-form QA. This is because the content of long answers is more focused on induction and summarization, rather than definitive results derived from reasoning, as is common with short answers. The study of long-form QA with chain-of-thought needs further exploration. We also conducted evaluation on DuReader_robust to assess the effectiveness of the RAFT method on the Chinese datasets. Since the questions in this dataset heavily rely on information from reference documents, the gain brought by the use of DSF is only 7.43% over the zero-shot prompting baseline (in Table 2 comparing the ’zero-shot’ and ’DSF+zero-shot’ rows in the DuReader group). In this case, the use of RAG to supplement reference documents with the question is more effective, which obtains a 12.59% performance gain over the zero-shot baseline. After RAFT fine-tuning, the model’s ability to extract and process information, as well as its reasoning capability can be significantly improved. It achieves 44.34% and 19.9% performance gain in F1 score over zero-shot prompting baseline and DSF+RAG baseline respectively. These results demonstrate that the RAFT method performs exceptionally well on both English and Chinese datasets.

-----------------------------------------


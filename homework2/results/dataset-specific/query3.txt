Query: evaluation metrics for SQuAD dataset on question answering

Table: 2409.20075:S3.T1.7.7
Caption: Table 1: Comparison of Product Question Answering (PQA) datasets.
The document types are classified into Product Reviews (PR), Product Information (PI), and Product Analysis from professional users (PA) .
References: As indicated in Table 1, previous datasets consist of undetailed short documents.
Their questions and answers are directly collected from brief user QA comments without verification, leading to noise and even incorrect answers.
Our WorthBuying dataset contains much longer documents with more than 50k QDA tuples grounded on 735k informative documents.
We also contain the most detailed product categories than existing datasets.
Footnotes: Gupta et al. (2019b)

Mansi Gupta, Nitish Kulkarni, Raghuveer Chanda, Anirudha Rayasam, and Zachary C. Lipton. 2019b.


Amazonqa: A review-based question answering task.


In IJCAI 2019, pages 4996–5002. Bjerva et al. (2020)

Johannes Bjerva, Nikita Bhutani, Behzad Golshan, Wang-Chiew Tan, and Isabelle Augenstein. 2020.


Subjqa: A dataset for subjectivity and review comprehension.


In EMNLP 2020, pages 5480–5494. Shen et al. (2022)

Xiaoyu Shen, Gianni Barlacchi, Marco Del Tredici, Weiwei Cheng, and Adrià Gispert. 2022.


semiPQA: A study on product question answering over semi-structured data.


In Proceedings of The Fifth Workshop on e-Commerce and NLP (ECNLP 5), pages 111–120. Shen et al. (2023b)

Xiaoyu Shen, Akari Asai, Bill Byrne, and Adria De Gispert. 2023b.


xPQA: Cross-lingual product question answering in 12 languages.


In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pages 103–115, Toronto, Canada. Association for Computational Linguistics. Gao et al. (2019b)

Shen Gao, Zhaochun Ren, Yihong Zhao, Dongyan Zhao, Dawei Yin, and Rui Yan. 2019b.


Product-aware answer generation in e-commerce question-answering.


In WSDM 2019, pages 429–437. Chen et al. (2019b)

Shiqian Chen, Chenliang Li, Feng Ji, Wei Zhou, and Haiqing Chen. 2019b.


Review-driven answer generation for product-related questions in e-commerce.


In WSDM 2019, pages 411–419.

-----------------------------------------

Table: 2409.12941:S1.T1.1
Caption: Table 1: Comparison of FRAMES against other datasets. FRAMES provides a combination of evaluation samples to test the factuality, retrieval, and reasoning of RAG systems. The dataset also covers multi-hop/step questions along with temporal disambiguation.
References: Evaluating Retrieval-Augmented Generation (RAG) systems has become increasingly important as these models integrate retrieval mechanisms with generative capabilities to enhance factual accuracy and reasoning(Yu et al., 2024b). Existing benchmarks, such as NaturalQuestions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and ELI5 (Fan et al., 2019), have been used to evaluate RAG models, but they often focus on specific aspects like retrieval accuracy or single-turn question answering without considering the full complexity of real-world applications. For instance, NaturalQuestions primarily tests retrieval precision, while TriviaQA emphasizes factual correctness in trivia-style questions. ELI5, on the other hand, is designed for explainability but does not rigorously assess the multi-hop reasoning necessary for synthesizing information from multiple sources. These benchmarks, while valuable, tend to evaluate RAG systems in a piecemeal fashion, missing the comprehensive assessment needed to truly measure their end-to-end capabilities. We provide additional comparisons against other datasets in Table 1.
Footnotes: Lin et al. (2021)

Stephanie Lin, Jacob Hilton, and Owain Evans.


Truthfulqa: Measuring how models mimic human falsehoods.


arXiv preprint arXiv:2109.07958, 2021. Mihaylov et al. (2018)

Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.


Can a suit of armor conduct electricity? a new dataset for open book
question answering.


In EMNLP, 2018. Yang et al. (2018b)

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan
Salakhutdinov, and Christopher D Manning.


Hotpotqa: A dataset for diverse, explainable multi-hop question
answering.


arXiv preprint arXiv:1809.09600, 2018b. Chen et al. (2020)

Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Wang.


Hybridqa: A dataset of multi-hop question answering over tabular and
textual data.


arXiv preprint arXiv:2004.07347, 2020. Cobbe et al. (2021)

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
Christopher Hesse, and John Schulman.


Training verifiers to solve math word problems.


arXiv preprint arXiv:2110.14168, 2021. Tang & Yang (2024)

Yixuan Tang and Yi Yang.


Multihop-rag: Benchmarking retrieval-augmented generation for
multi-hop queries.


arXiv preprint arXiv:2401.15391, 2024. Schnitzler et al. (2024)

Julian Schnitzler, Xanh Ho, Jiahao Huang, Florian Boudin, Saku Sugawara, and
Akiko Aizawa.


Morehopqa: More than multi-hop reasoning.


arXiv preprint arXiv:2406.13397, 2024. Trivedi et al. (2022)

Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.


Musique: Multihop questions via single-hop question composition.


Trans. Assoc. Comput. Linguistics, 10:539–554,
2022.


doi: 10.1162/TACL\_A\_00475.


URL https://doi.org/10.1162/tacl_a_00475. Kwiatkowski et al. (2019)

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
Kenton Lee, et al.


Natural questions: a benchmark for question answering research.


Transactions of the Association for Computational Linguistics,
7:453–466, 2019. Joshi et al. (2017)

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.


Triviaqa: A large scale distantly supervised challenge dataset for
reading comprehension.


arXiv preprint arXiv:1705.03551, 2017. Fan et al. (2019)

Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and
Michael Auli.


Eli5: Long form question answering.


arXiv preprint arXiv:1907.09190, 2019.

-----------------------------------------

Table: 2402.16457:A1.T3.35
Caption: Table 3: 
Data statistics of RetrievalQA (questions need retrieval).
# Avg. Q, Ans, Doc Tokens means the average number of tokens of questions, answers, and top-5 retrieved documents, respectively.
We use the tiktoken python library to calculate the number of tokens.
References: We conduct a sanity check using various sizes of LLMs in Fig.1 and in Appendix A.5, showing that RetrievalQA is extremely hard for all models without access to external knowledge.
We present detailed data statistics in Table 3 and examples of the data in Table 11. However, different from Cases 1 and 3, for Case 2, theoretically, it is possible to collect data that guarantees the knowledge to answer the questions is not present in the models. For instance, new world knowledge occurred after model training and long-tail knowledge that did not (or rarely) appear in the training corpora. Therefore, we collect 1,271 questions (Case 2) that are guaranteed cannot be answered without external information. The data collection process is detailed in section 2.
The dataset statistics are shown in Table 3.
The examples of data instances are in Table 11.
Footnotes: Kasai et al. (2023)

Jungo Kasai, Keisuke Sakaguchi, yoichi takahashi, Ronan Le Bras, Akari Asai, Xinyan Velocity Yu, Dragomir Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui. 2023.


Realtime QA: What’s the answer right now?

In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Vu et al. (2023)

Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. 2023.


Freshllms: Refreshing large language models with search engine augmentation. Zhuang et al. (2023)

Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2023.


ToolQA: A dataset for LLM question answering with external tools.


In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Mallen et al. (2023)

Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.


When not to trust language models: Investigating effectiveness of parametric and non-parametric memories.


In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9802–9822, Toronto, Canada. Association for Computational Linguistics. Joshi et al. (2017)

Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017.


TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension.


In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada. Association for Computational Linguistics.

-----------------------------------------

Table: 2405.13576:S3.T2.3.1
Caption: Table 2: Summary of datasets. FlashRAG currently includes a variety of datasets of different tasks. The sample size of each dataset and the knowledge source of the answer are listed as references. "-" indicates that the knowledge source is common sense. The ∗∗\ast∗ symbol represents that the task of this dataset has been modified to fit the RAG scene.
References: As shown in Table 2, we collects and pre-processes 32 benchmark datasets, covering the majority of the datasets utilized in RAG works.
We researched and listed the sources of answers in each dataset for reference. For most datasets, the knowledge comes from Wikipedia, underscoring its importance in RAG tasks.
All datasets have been formatted into a unified JSONL structure, typically encapsulating four fields: ID, question, golden answer, and metadata. For multiple-choice datasets like MMLU [35, 36] and OpenBookQA [37], an additional "choices" field is provided as options. We have hosted the processed datasets on HuggingFace for easy access. Details on dataset processing can be found in the appendix.
Footnotes: [38]

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.


Natural questions: A benchmark for question answering research.


Transactions of the Association for Computational Linguistics, 7:452–466, 2019. [39]

Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.


TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension.


In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. [40]

Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi.


When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories.


arXiv preprint, 2022. [41]

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.


SQuAD: 100,000+ questions for machine comprehension of text.


In Jian Su, Kevin Duh, and Xavier Carreras, editors, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin, Texas, November 2016. Association for Computational Linguistics. [42]

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng.


MS MARCO: A human-generated MAchine reading COmprehension dataset, 2017. [43]

Tomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette.


The NarrativeQA reading comprehension challenge.


Transactions of the Association for Computational Linguistics, TBD:TBD, 2018. [44]

Yi Yang, Wen-tau Yih, and Christopher Meek.


WikiQA: A challenge dataset for open-domain question answering.


In Lluís Màrquez, Chris Callison-Burch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2013–2018, Lisbon, Portugal, September 2015. Association for Computational Linguistics. [45]

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.


Semantic parsing on Freebase from question-answer pairs.


In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. [46]

Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer.


AmbigQA: Answering ambiguous open-domain questions.


In EMNLP, 2020. [38]

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.


Natural questions: A benchmark for question answering research.


Transactions of the Association for Computational Linguistics, 7:452–466, 2019. [47]

Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi.


Social IQa: Commonsense reasoning about social interactions.


In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463–4473, Hong Kong, China, November 2019. Association for Computational Linguistics. [48]

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.


CommonsenseQA: A question answering challenge targeting commonsense knowledge.


In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149–4158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [49]

Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.


Boolq: Exploring the surprising difficulty of natural yes/no questions.


In NAACL, 2019. [50]

Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi.


Piqa: Reasoning about physical commonsense in natural language.


In AAAI Conference on Artificial Intelligence, 2019. [51]

Ashwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish Sabharwal, and Peter Clark.


How much coffee was consumed during emnlp 2019? fermi problems: A new reasoning challenge for ai.


arXiv preprint arXiv:2110.14207, 2021. [52]

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning.


HotpotQA: A dataset for diverse, explainable multi-hop question answering.


In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. [53]

Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa.


Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps.


In Proceedings of the 28th International Conference on Computational Linguistics, pages 6609–6625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. [54]

Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.


MuSiQue: Multihop questions via single-hop question composition.


Transactions of the Association for Computational Linguistics, 2022. [32]

Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis.


Measuring and narrowing the compositionality gap in language models.


In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5687–5711, Singapore, December 2023. Association for Computational Linguistics. [55]

Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang.


ASQA: Factoid questions meet long-form answers.


In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8273–8288, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. [56]

Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.


ELI5: Long form question answering.


In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for Computational Linguistics. [35]

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.


Measuring massive multitask language understanding.


Proceedings of the International Conference on Learning Representations (ICLR), 2021. [36]

Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt.


Aligning ai with shared human values.


Proceedings of the International Conference on Learning Representations (ICLR), 2021. [57]

Stephanie Lin, Jacob Hilton, and Owain Evans.


TruthfulQA: Measuring how models mimic human falsehoods.


In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214–3252, Dublin, Ireland, May 2022. Association for Computational Linguistics. [58]

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.


Hellaswag: Can a machine really finish your sentence?


In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. [59]

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.


Think you have solved question answering? try arc, the AI2 reasoning challenge.


CoRR, abs/1803.05457, 2018. [37]

Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.


Can a suit of armor conduct electricity? a new dataset for open book question answering.


In EMNLP, 2018. [60]

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum.


Robust disambiguation of named entities in text.


In Regina Barzilay and Mark Johnson, editors, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 782–792, Edinburgh, Scotland, UK., July 2011. Association for Computational Linguistics. [61]

Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel.


KILT: a benchmark for knowledge intensive language tasks.


In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523–2544, Online, June 2021. Association for Computational Linguistics. [62]

Simone Tedeschi, Simone Conia, Francesco Cecconi, and Roberto Navigli.


Named Entity Recognition for Entity Linking: What works and what’s next.


In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2584–2596, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. [61]

Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel.


KILT: a benchmark for knowledge intensive language tasks.


In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523–2544, Online, June 2021. Association for Computational Linguistics. [63]

Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Frédérique Laforest, and Elena Simperl.


T-rex: A large scale alignment of natural language with knowledge base triples.


In Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018., 2018. [61]

Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel.


KILT: a benchmark for knowledge intensive language tasks.


In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523–2544, Online, June 2021. Association for Computational Linguistics. [64]

Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer.


Zero-shot relation extraction via reading comprehension.


In Roger Levy and Lucia Specia, editors, Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333–342, Vancouver, Canada, August 2017. Association for Computational Linguistics. [61]

Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel.


KILT: a benchmark for knowledge intensive language tasks.


In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523–2544, Online, June 2021. Association for Computational Linguistics. [65]

James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.


FEVER: a large-scale dataset for fact extraction and VERification.


In NAACL-HLT, 2018. [61]

Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel.


KILT: a benchmark for knowledge intensive language tasks.


In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523–2544, Online, June 2021. Association for Computational Linguistics. [66]

Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston.


Wizard of Wikipedia: Knowledge-powered conversational agents.


In International Conference on Learning Representations, 2019. [61]

Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel.


KILT: a benchmark for knowledge intensive language tasks.


In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523–2544, Online, June 2021. Association for Computational Linguistics. [67]

Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neervannan, and Graham Neubig.


Wikiasp: A dataset for multi-domain aspect-based summarization.


Transactions of the Association for Computational Linguistics (TACL), 2020.

-----------------------------------------

Table: 2211.16971:S4.T2
Caption: Table 2: Roundtrip evaluation of our QA datasets’ quality, using an off-the-shelf QA model. The RoBERTa model was trained on SQuAD 2.0. Best results indicated in bold text.
References: As shown in Table 2, the synthetic data is of high quality, reaching similar levels to SQuAD 2.0, which was manually created by humans. Furthermore, Table 3 shows examples of the synthetic data produced and used. The generated questions are both fluent and of interest, and the answers are both precise and correct. The first question is slightly stilted, but still easily understandable.

-----------------------------------------

Table: 2402.07867:A0.T14.1
Caption: Table 14: Statistics of datasets.
References: Datasets. We use three benchmark question-answering datasets in our evaluation: Natural Questions (NQ) [38], HotpotQA [39], and MS-MARCO [40], where each dataset has a knowledge database. The knowledge databases of NQ and HotpotQA are collected from Wikipedia, which contains 2,681,468 and 5,233,329 texts, respectively. The knowledge database of MS-MARCO is collected from web documents using the MicroSoft Bing search engine [84], which contains 8,841,823 texts. Each dataset also contains a set of questions. Table 14 (in Appendix) shows statistics of datasets.
Footnotes: [38]

T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., “Natural questions: a benchmark for question answering research,” TACL, vol. 7, pp. 452–466, 2019. [39]

Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning, “Hotpotqa: A dataset for diverse, explainable multi-hop question answering,” in EMNLP, 2018. [40]

T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and L. Deng, “Ms marco: A human generated machine reading comprehension dataset,” choice, vol. 2640, p. 660, 2016.

-----------------------------------------

Table: 2010.06028:A2.T7
Caption: Table 7: Performance on SQuAD 1.1 development set when training with LM-filtered synthetically generated question-answer pairs on IMDB corpus. Bold values indicate best performance per each model (row-wise). Our baseline EM and F1 numbers (on SQuAD 1.1 training set) are 80.78 and 88.20, respectively.

References: While the performance of the RC models on the target domains is important, weak performance on the source domain could inhibit the use of our proposed methods in applications that require strong performance in both source and target domains.
Tab. 7 shows EM/F1 scores of the bert-base-uncased RC models trained with synthetic data generated from the IMDB corpus on SQuAD 1.1 dev set. We can observe that adding synthetic samples to the SQuAD training set always improves the performance on the dev set compared to using the SQuAD training set only. In fact, with QAGen2S, impressive 3.1(EM)/2.2(F1) gains are achieved. Synthetic only samples from the same model outperform the SQuAD baseline.
Similar to previous domain adaptation results, we observe that QAGen2S outperforms QAGen, and QAGen exceeds AQGen.

-----------------------------------------

Table: 2111.10056:S2.T2
Caption: Table 2: Samples of images and question-answer pairs from the mentioned Datasets. Q = Question, A = Answer. The datasets are presented in chronological order.
References: The performance metrics used in the proposed medical VQA tasks can be categorized into classification-based metrics and language-based metrics. The classification-based metrics are the general metrics in classification tasks such as accuracy and F1 score. They treat the answer as a classification result and calculate the exact match accuracy, precision, recall, and e.t.c. All eight tasks in this paper use classification-based metrics as part of their performance metrics. The Language metrics are the general metrics for sentence evaluation tasks (e.g., translation, image captioning). The tasks using language-based metrics include VQA-Med-2018, VQA-Med-2019, PathVQA, VQA-Med-2020, and VQA-Med-2021. All of those four tasks use the BLEU [67], which measures the similarity of the phrases (n-grams) between two sentences. However, the BLEU is originally a metric for machine translation and is also used in medical report generation tasks [51]. As shown in Table 2, the ground truth answers in medical VQA are obviously shorter than those of machine translation or medical report generation tasks. Also, for some questions, the semantically positive or negative is more important than the word match. It suggests that BLEU may be an inappropriate metric for current medical VQA datasets. However, the BLEU can still be useful when the answer corpus of the future medical VQA becomes extensive and comprehensive sentences. Besides dataset design, question diversity raises a challenge in method development. To answer the diverse questions, the medical VQA systems require various reasoning abilities. For example, as a sample shown in Table 2, to answer question “What is the function of the rightmost organ in this picture?”, the model should understand the region described, identify the organ in the region of interest, and finally answer the function of the organ. Besides the basic image and language understanding, medical domain knowledge is a critical ability required, which includes knowledge of anatomical structures, medical procedures, diseases, medical imaging modalities, treatment options, and clinical practices. More specially, for the question categories in Table 1, Modality, Plane need knowledge about radiology examination; Organ, System, Abnormality, Object/Condition Presence need knowledge about human anatomy and medicine; Positional Reasoning, Color, Size, Attribute Other, Counting need general knowledge and reasoning; Knowledge Graph need to combine the upon knowledge with the knowledge triplets given in dataset. Therefore, to address question categories correspondingly, the future medical VQA should be equipped with computer-aid diagnosis, general language understanding, reasoning, knowledge integration, and contextual understanding.

-----------------------------------------

Table: 2404.15821:S2.T1
Caption: Table 1: Different features of related works. The table presents an overview of some readily available evaluation tools for evaluating tabular synthetic data. “Ease of Use” denotes how easily the tool can be integrated into an experimental setting. Configurability denotes if the tool allows bundling and customising metrics. Extendibility denotes if new custom metrics can easily be added, and Ranking denotes if the tool can help if multiple synthetic datasets are to be decided between. “Mixed Type Strategy” denotes the primary strategies the tool uses for taking care of numerical and categorical data. The number of metrics in SynthCity is approximate because a few of them are distinct implementations of the same metric (e.g. using different classifiers). For SynthEval, the “18+12” metrics signify that we have 18 different metric modules, but some of them produce supplementary results.
References: A few ecosystems for evaluating synthetic data already exist. Some that have usable Python implementations are listed in Table 1 above. Synthcity by [16] hosts a variety of metrics in addition to an intuitive interface for accessing a selection of generative models. However, categoricals are treated as ordinal integers, making the evaluation slightly biased towards numerical values (see Figure 1). Output values are mapped to the zero-one interval, which makes them clear to read but hides away some details. There is potential for confusion when metrics with the same units are mixed and some are to be “maximised” and others “minimised”. SDmetrics222Also available as a module in their SDGym framework: https://docs.sdv.dev/sdgym by [17] allows users to generate quality and diagnostic reports, as well as column-level figures for visual comparison. The selection of metrics is mostly limited to high-level dataset similarity and integrity with only a few recognised metrics appearing. The framework requires a metadata file, which is non-trivial to create and compile, and was thus not included in the figure. Table Evaluator by [18] is a library to evaluate how similar synthesised datasets are to real data through quantitative and visual evaluation. The evaluation is at large based on the performance of classifier models, together with a few other statistical methods. Metrics such as the correlation matrix are adapted to work for categorical data and one-hot encoding is used for nearest neighbour calculations. The tool produces a summary score that can be used for a quick ranking of datasets. The performance varies quite a bit for heterogeneous data. DataSynthesizer was developed by [4] and includes two metrics that check the quality of synthesised data visually using pairwise mutual information and marginal distributions. Numerical data are made discrete for this purpose by binning.
Footnotes: Granulation: Ease of Use; ⋆⁣⋆⁣⋆⋆⋆⋆\star\star\star few lines of code to get all necessary results, ⋆⁣⋆⋆⋆\star\star minor work required e.g. dataset encoding or looping over metrics, ⋆⋆\star major preprocessing, coding, or additional files are needed. Config.; ⋆⁣⋆⁣⋆⋆⋆⋆\star\star\star metrics can be selected as desired AND metrics have accessible options, ⋆⁣⋆⋆⋆\star\star either one of previous, ⋆⋆\star none of previous.

-----------------------------------------

Table: 2409.08788:S3.T2.1.1
Caption: Table II: Evaluation of ECG-ReGen paired with an off-the-shelf LLM on the ECG-QA dataset against supervised approaches. ‘S’ refers to Single.
References: Table II presents the performance of various methods on three ECG question-answering tasks. Our proposed ECG-ReGen-QA approach, operating in a zero-shot setting, demonstrates competitive performance compared to supervised methods. Notably, our method paired with Gemini Flash 1.5 and GPT-4o mini achieves the highest scores on the S-Choose task (58.5258.5258.5258.52% and 58.6658.6658.6658.66%, respectively), surpassing all supervised models. For the S-Verify task, our approach performs comparably to the best supervised model (M3AE), with scores ranging from 71.9971.9971.9971.99% to 72.7972.7972.7972.79%. While ECG-ReGen-QA shows lower performance on the S-Query task (open-ended questions and answers) compared to the top supervised models, it still significantly outperforms the per Q-type majority baseline and other supervised models like the Blind and Deaf Transformers. These results are particularly impressive considering that our method operates in a zero-shot manner, requiring no task-specific fine-tuning. This demonstrates the effectiveness of our retrieval-augmented approach in leveraging off-the-shelf language models for ECG analysis. Likewise, the performance gap between different considered language models is relatively small, suggesting that the choice of LLM may not be crucial given that retrieved samples with self-supervised ECG model are similar to the test case.
Footnotes: [6]

Jungwoo Oh, Gyubok Lee, Seongsu Bae, Joon-myoung Kwon, and Edward Choi,


“Ecg-qa: A comprehensive question answering dataset combined with electrocardiogram,”


Advances in Neural Information Processing Systems, vol. 36, 2024.

-----------------------------------------

Query: ROC-AUC evaluation for MIMIC-III dataset

Table: 2404.10110:S7.T3
Caption: TABLE III: Computational overhead for reaching target AUC of ROC.
References: 3) Saving computational cost:
To evaluate the computation efficiency of the proposed HSGD method, we analyze the memory and FLOPs consumptions. As shown in Table III, our proposed method always consumes the least memory and FLOPs to achieve the target test AUC of ROC. For example, to reach 0.9 test AUC of ROC (the maximum target achieved by the worst baseline) on the OrganAMNIST dataset, the proposed method consumes 890 MBMB\mathrm{M}\mathrm{B} memory and 11.57 GFLOPsGFLOPs\mathrm{G}\mathrm{F}\mathrm{L}\mathrm{O}\mathrm{P}\mathrm{s}. Compared with baselines, it saves memory consumption by 66%, 54%, 42%, and 48%, and FLOPs consumption by 66%, 54%, 46%, and 52%. This is because the memory and FLOPs consumptions are determined by the consumption in each iteration and the total iteration to reach the target requirements. For the proposed method and all baselines, we run the same model, so that their computational consumptions in each iteration are similar (that of C-HSGD and C-TDCD are slightly smaller because they use the compression method), and the total consumptions to achieve the target is mainly determined by the total training iteration. Since our proposed method convergence fast, it reduces the computational cost.

-----------------------------------------

Table: 2204.10505:S4.T2
Caption: TABLE II: ROC-AUC Score
References: From the graph of the performance, it can be observed that each of the clients’ models performed well when evaluated against the test data belonging to the same source as the data with which it was trained. But they failed to perform as well when they were provided data from a different source. The same observation can be made for both the AUC-PRC table (TABLE III) as well as the AUC-ROC table (TABLE II).

-----------------------------------------

Table: 2409.11375:S4.T1.3
Caption: Table 1. Analyzing the Performance with Different Encoder Networks: Comparison of our framework with baseline methods (ResNet-50) on the test sets from three datasets, evaluating multi-class classification performance in terms of accuracy, AUC-ROC, AUC-PR, and F1-score.
References: We evaluated different encoder networks against the baseline ResNet-50 model in our extensive experiment. To ensure consistency, we utilized the same encoder from the SSL networks and employed transfer learning to transfer the learned weights. For downstream classification tasks, we also integrated a classifier network. Each supervised network underwent fine-tuning individually for every dataset, followed by evaluations on both the respective test set and the test sets from other datasets. This comprehensive evaluation allowed us to observe the network’s performance on previously unseen test data and to observe its generalization capability. The performance of each model was then compared with that of the baseline model ResNet-50, which underwent training on each dataset and subsequent evaluation on all three test sets. Throughout this experimental process, we applied data augmentation techniques to enhance robustness. Performance metrics including Accuracy, AUC-ROC, AUC-PR, and F1-Score were employed to measure effectiveness. Analysis of Table 1 revealed the consistent performance of the self-supervised fine-tuning approach over the baseline model, with the SwinV2-based classifier demonstrating the most reliable performance across all test sets, particularly in scenarios with smaller datasets such as Dataset-2 and Dataset-3. While Dataset-1, with its larger sample size, yielded similar performance between the proposed framework and the baseline model, it’s noteworthy that our method showcased superior performance on smaller datasets as well. The generalization capabilities during off-domain evaluation, when the model was fine-tuned on one dataset and evaluated on other test sets, significant performance boosts were observed in our proposed method. For instance, while fine-tuning Dataset-1, our Multi-OCT-SelfNet-SwinV2 model achieved AUC-ROC scores of 0.79, 0.97, and 0.90, along with AUC-PR of 0.58, 0.94, and 0.70 on Test Set-1, Test Set-2, and Test Set-3, respectively. In contrast, the baseline model attained AUC-ROC scores of 0.65, 0.98, and 0.59, along with AUC-PR of 0.39, 0.95, and 0.57 on the respective test sets. These results underscore the superior generalization capability of our proposed method, highlighting its potential for robust performance across diverse datasets. Table 3 presents the results of the on-domain evaluation for our Multi-OCT-SelfNet framework utilizing the SwinV2 backbone, excluding the data fusion step in the pre-training phase. This analysis aims to quantify the impact of data fusion on performance enhancement. Notably, for Dataset-2, the achieved accuracy, AUC-ROC, AUC-PR, and F1-Score were 0.74, 0.89, 0.80, and 0.74, respectively. Upon incorporating data fusion, substantial improvements were observed, with scores reaching 0.86, 0.97, 0.94, and 0.86, respectively, indicating a significant boost in performance which can be seen from previous Table 1. Similarly, for Dataset-3, the scores stood at 0.58, 0.67, 0.35, and 0.47, while data fusion resulted in notable enhancements, yielding scores of 0.86, 0.89, 0.49, and 0.83, respectively, thus demonstrating consistent performance improvements. Conversely, for Dataset-1, where the dataset size was considerably larger and already facilitated robust training, the impact of data fusion on performance enhancement was relatively modest. Although there were slight improvements in accuracy, AUC-ROC, AUC-PR, and F1-Score, from 0.89, 0.97, 0.88, and 0.90 to 0.91, 0.97, 0.89, and 0.90, respectively, these enhancements were not as substantial. When comparing these results to the previous experiment, where class imbalance was not addressed, as shown in Table 1, it is evident that correcting the imbalance causes a slight decline in performance for both models. Despite this reduction, our proposed model still outperforms the baseline in most cases, achieving a higher score between the two. This indicates that even with the trade-off of a minor performance drop, our model has a notable advantage in overall performance.

-----------------------------------------

Table: 2204.10505:S4.T3
Caption: TABLE III: ROC-PRC Score
References: From the graph of the performance, it can be observed that each of the clients’ models performed well when evaluated against the test data belonging to the same source as the data with which it was trained. But they failed to perform as well when they were provided data from a different source. The same observation can be made for both the AUC-PRC table (TABLE III) as well as the AUC-ROC table (TABLE II).

-----------------------------------------

Table: 2407.16470:A3.T5.6.6.6
Caption: ROC-AUC results for severity hallucination ranking across HRL and LRL directions. 

References: In the same way as in the binary detection setting, the validation results Table 4 allowed to select the otpimal prompt for each LLM, and then evaluate this best prompt across the test set, using here the ROC AUC score.
Testing results aredisplayed Table 5, and presents ROC AUC scores for all methods per translation direction. For HRLs, embeddings’ high performance remains consistent with the binary hallucination approach. However, BLASER-QE remains the state-of-the-art in overall performance for severity ranking. The generalizability of these results requires further evaluation due to significant class imbalances in the dataset. Notably, in 11 of the 18 language directions, fewer than five samples are present in at least one hallucination severity category, see Appendix B.


-----------------------------------------

Table: 2409.11375:S4.T4.3
Caption: Table 4. Analyzing the Impact of Self-Supervised Pre-Training: Comparing Our Framework with SwinV2 Classifier on Test Sets from Three Datasets. Evaluation Includes AUC-ROC, Accuracy, AUC-PR, and F1-Score for Multi-Class Classification Performance.
References: Nevertheless, when evaluated on other test sets for off-domain evaluation, the model exhibited poorer performance in terms of generalization, underscoring the crucial role of self-supervised pre-training in enhancing model robustness and adaptability across diverse datasets.
The results presented in Table 4 indicate that while the on-domain performance for Dataset-1 remains largely consistent, a slight deterioration is observed in the off-domain evaluation. Conversely, for Dataset-2 and Dataset-3, which are smaller datasets, both the on-domain and off-domain evaluation performances exhibit significant deterioration without the self-supervised step.
The grouped bar chart in Figure 8 compares AUC-ROC scores between self-supervised and non-self-supervised methodologies. Across all three datasets and their corresponding test sets, the non-self-supervised approach consistently shows a decline in performance, highlighting the significant impact of the self-supervised methodology.

-----------------------------------------

Table: 2112.05321:S3.T3
Caption: TABLE III: Compare ROC AUC for training directly without any pretrain process, PMFL with 3 clients, and PMFL with 5 clients when treat Pneumonia as target test task.
References: Besides, we also compare the performance of PMFL algorithm with 3 clients and 5 clients. In Fig. 6, the blue line shows the result of 3-clients while the red line denotes 5-clients case and the black line is the result of training directly. At first, we could conclude that the final performance after 10 epochs of training with 3-clients PMFL is extremely similar to the 5-clients PMFL’s. However, apparently, the standard deviation of the blue line is smaller. It means that 3-clients PMFL is more stable than 5-clients PMFL. The results from table III show this more clearly. Both PMFL algorithms perform better than training directly, and their final ROC AUC scores are almost the same while the standard deviation of 3-clients PMFL is 0.00290.00290.0029 which is slightly smaller than 0.00330.00330.0033 of 5-clients. And this also makes sense, because when you pretrain the model with more different tasks, the heterogeneous property would make a bigger influence.

-----------------------------------------

Table: 2202.02124:S5.T1
Caption: Table 1: Results for the crop type classification evaluation tasks. All results are averaged from 10 runs and reported with the accompanying standard error. We report the area under the receiver operating characteristic curve (AUC ROC) and the F1 score using a threshold of 0.5 to classify a prediction as the positive or negative class. We highlight the  first and second best metrics for each task. TIML achieves the highest F1 score of any model on the Brazil task and the best AUC ROC and F1 scores when averaged across the 3 tasks. We highlight the improvement of TIML relative to other transfer-learning models, showing it is able to leverage task structure to significantly increase performance on the CropHarvest dataset.
References: We show the model results for TIML, its ablations and all baseline models when trained on the CropHarvest dataset in Table 1. Like Tseng et al. (2021b), we report the AUC ROC score and the F1 score calculated using a threshold of 0.5. Overall, TIML is the best performing algorithm on the CropHarvest dataset, achieving the highest F1 and AUC ROC scores when averaged across all tasks. TIML is consistently the best performing algorithm on every task. In particular, TIML is the only transfer learning model that outperforms a randomly-initialized model in the challenging Brazil task, where there are only 26 positive datapoints.

-----------------------------------------

Table: 2305.11386:S4.T1
Caption: Table 1. Statistics for all four cohorts: Most Populous States, Heterogeneous States, MIMIC-III IID and MIMIC-III non-IID.
References: MIMIC data: We generated two different cohorts from the MIMIC-III dataset, where we assign MIMIC-III patients to five different FL clients. For the non-IID cohort, we synthesize non-IDD clients through a Dirichlet distribution as proposed in (Ezzeldin et al., 2021; Hsu et al., 2019), while the IID cohort was randomly generated following a uniform distribution. Similar to the Synthea scenario, we focus on race as our sensitive attribute, and the outcome prediction task relates to the standard task studied in the literature (Harutyunyan et al., 2019) related to predicting the mortality of patients admitted to the ICU, during their stay. We present more details about these cohort extractions in Appendix B and provide more details and statistics about the four cohorts in Table 1.

-----------------------------------------

Table: 2107.06580:A1.T4
Caption: Table 4: Worst-performing client in federation (ROC AUC)
References: An additional metric which can be used to measure the performance of a classification model is the area under the curve of the receiver operating characteristic. We evaluate our method in the same fashion as previously; analyzing both the average performance across clients as well as the worst performing client in the federation. These results are displayed in Table  3  and  4 . These graphs corroborate the findings in the main text, and highlight that the strong performance of  iFedAvg  is independent of the chosen metric. Interestingly, the gap between our method and vanilla federated averaging shrinks marginally, which can be explained by the sensitivity of F1 score to individual incorrect samples. Similarly to the F1 score metric, we present all distributions of ROC AUC performance as violin plots (EVD Prognosis: Figure  9 , EVD Diagnosis: Figure 10 , VSN: Figure  11 , HAR: Figure  12 ). One instance that stands out is the relatively poor performance of  APFL  for EVD Prognosis and Diagnosis, as the personalized method does not manage to outperform vanilla federated averaging. We suspect that  APFL  is overfitting due to the strongly heterogeneous nature of the datasets, which can be observed in its similarity to Local training performance. In conclusion,  iFedAvg  shows impressive performance in various settings and measured by different metrics. Especially on datasets which benefit from personalization due to one or multiple outlier clients, our method is particularly effective compared with vanilla federated averaging.

-----------------------------------------

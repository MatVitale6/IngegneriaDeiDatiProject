{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "API_ENDPOINT = 'http://localhost:8080/search'\n",
    "QUERY = 'f1 score on wdc dataset'\n",
    "\n",
    "QUERIES = [\n",
    "    \"f1 score on dataset wdc\",\n",
    "    \"precision and recall for deep learning on ImageNet dataset\",\n",
    "    \"accuracy comparison for federated learning models\",\n",
    "    \"f1 and accuracy on CIFAR-10 classification\",\n",
    "    \"ROC-AUC evaluation for MIMIC-III dataset\",\n",
    "    \"evaluation metrics for SQuAD dataset on question answering\",\n",
    "    \"evaluation of BERT embeddings for similarity tasks\",\n",
    "    \"metrics comparison for GPT models on summarization\",\n",
    "    \"training convergence for federated learning models\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id(url):\n",
    "    match = re.search(r'/(\\d{4}\\.\\d{5})', url)\n",
    "    return match.group(1) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a single text file that consolidates all the necessary data. This file will be sent to GPT-4 for ranking purposes.\n",
    "\n",
    "file_path = \"generic-metrics/query1-bis.txt\"\n",
    "payload = {\n",
    "    \"inputString\":QUERY,\n",
    "    \"resultCount\":10,\n",
    "    \"resourceType\":\"json\"\n",
    "}\n",
    "\n",
    "tables = requests.post(API_ENDPOINT, data=payload)\n",
    "tables = tables.json()\n",
    "\n",
    "with open(file_path, 'wt') as f:\n",
    "    f.write(f\"Query: {QUERY}\\n\")\n",
    "    for table in tables:\n",
    "        f.write(f\"\\nTable: {extract_id(table['link'])}:{table['tableId']}\\n\")\n",
    "        f.write(f\"Caption: {table['caption']}\\n\")\n",
    "        f.write(f\"References: {table['references']}\\n\") if table['references'] else None\n",
    "        f.write(f\"Footnotes: {table['footnotes']}\\n\") if table['footnotes'] else None\n",
    "        f.write(\"\\n-----------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates raw json ranking file from HTTP/POST to our system\n",
    "\n",
    "file_idx = 1\n",
    "for query in QUERIES:\n",
    "    \n",
    "    payload = {\n",
    "        \"inputString\":query,\n",
    "        \"resultCount\":10,\n",
    "        \"resourceType\":\"json\"\n",
    "    }\n",
    "\n",
    "    tables = requests.post(API_ENDPOINT, data=payload)\n",
    "    tables = tables.json()\n",
    "\n",
    "    data = {}\n",
    "    table_idx = 1\n",
    "\n",
    "    for table in tables:\n",
    "        data[table_idx] = f\"{extract_id(table['link'])}:{table['tableId']}\"\n",
    "        table_idx += 1\n",
    "\n",
    "\n",
    "    with open(f'query{file_idx}-raw.json', 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    file_idx += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the position of the first relevant element (according to the truth source)\n",
    "\n",
    "def position_of_first_relevant_result(lucene_json, truth_json): \n",
    "    with open(truth_json, 'r') as f:\n",
    "        gpt_rankings = json.load(f)\n",
    "    \n",
    "    most_relevant_id = gpt_rankings['1']\n",
    "\n",
    "    with open(lucene_json, 'r') as f:\n",
    "        search_rankings = json.load(f)\n",
    "    \n",
    "    for key, value in search_rankings.items():\n",
    "        if value == most_relevant_id:\n",
    "            return int(key)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Reciprocal Rank (MRR)\n",
    "\n",
    "$$\n",
    "MRR = \\frac{1}{|Q|}\\sum_{i=1}^{|Q|}\\frac{1}{rank_{i}}\n",
    "$$\n",
    "\n",
    "Dove $Q$ è il numero di query e $rank_{i}$ la posizione del primo elemento rilevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6208112874779541\n"
     ]
    }
   ],
   "source": [
    "RAWS_PATH = [\n",
    "    \"generic-metrics/query1-raw.json\",\n",
    "    \"generic-metrics/query2-raw.json\",\n",
    "    \"generic-metrics/query3-raw.json\",\n",
    "    \"dataset-specific/query1-raw.json\",\n",
    "    \"dataset-specific/query2-raw.json\",\n",
    "    \"dataset-specific/query3-raw.json\",\n",
    "    \"model-specific/query1-raw.json\",\n",
    "    \"model-specific/query2-raw.json\",\n",
    "    \"model-specific/query3-raw.json\"\n",
    "]\n",
    "\n",
    "GPT_PATH = [\n",
    "    \"generic-metrics/query1-gpt.json\",\n",
    "    \"generic-metrics/query2-gpt.json\",\n",
    "    \"generic-metrics/query3-gpt.json\",\n",
    "    \"dataset-specific/query1-gpt.json\",\n",
    "    \"dataset-specific/query2-gpt.json\",\n",
    "    \"dataset-specific/query3-gpt.json\",\n",
    "    \"model-specific/query1-gpt.json\",\n",
    "    \"model-specific/query2-gpt.json\",\n",
    "    \"model-specific/query3-gpt.json\"\n",
    "]\n",
    "\n",
    "\n",
    "def calculate_mrr(source, truth):\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for raw, gpt in zip(source, truth):\n",
    "        reciprocal_rank = 1 / position_of_first_relevant_result(raw, gpt)\n",
    "        reciprocal_ranks.append(reciprocal_rank)\n",
    "\n",
    "    return sum(reciprocal_ranks) / len(QUERIES)\n",
    "\n",
    "mrr = calculate_mrr(RAWS_PATH, GPT_PATH)\n",
    "print(mrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Habemus MRR\n",
    "\n",
    "0.6208112874779541 è il valore calcolato confrontando i ranking di GPT-4 con i ranking del nostro sistema sulle 9 query definite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confrontiamo il risultato ottenuto con una **baseline random**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_json_values(input_file, output_file):\n",
    "    # Load the JSON file\n",
    "    with open(input_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Shuffle the values\n",
    "    keys = list(data.keys())\n",
    "    values = list(data.values())\n",
    "    random.shuffle(values)\n",
    "    \n",
    "    # Create a new JSON object with shuffled values\n",
    "    shuffled_data = {key: value for key, value in zip(keys, values)}\n",
    "    \n",
    "    # Save the shuffled JSON to the output file\n",
    "    with open(output_file, 'w') as file:\n",
    "        json.dump(shuffled_data, file, indent=4)\n",
    "\n",
    "def shuffle():        \n",
    "    for raw in RAWS_PATH:\n",
    "        random_file = os.path.join(os.path.dirname(raw), os.path.basename(raw).replace('raw', 'rnd'))\n",
    "        shuffle_json_values(raw, random_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22698412698412695, 0.1845238095238095, 0.3630952380952381, 0.1974867724867725, 0.45185185185185184, 0.5408730158730158, 0.451984126984127, 0.2887125220458554, 0.5566137566137566, 0.3900793650793651]\n",
      "Average: 0.36522045855379187\n"
     ]
    }
   ],
   "source": [
    "RND_PATH = [\n",
    "    \"generic-metrics/query1-rnd.json\",\n",
    "    \"generic-metrics/query2-rnd.json\",\n",
    "    \"generic-metrics/query3-rnd.json\",\n",
    "    \"dataset-specific/query1-rnd.json\",\n",
    "    \"dataset-specific/query2-rnd.json\",\n",
    "    \"dataset-specific/query3-rnd.json\",\n",
    "    \"model-specific/query1-rnd.json\",\n",
    "    \"model-specific/query2-rnd.json\",\n",
    "    \"model-specific/query3-rnd.json\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    shuffle()\n",
    "    results.append(calculate_mrr(RAWS_PATH, RND_PATH))\n",
    "\n",
    "print(results)\n",
    "print(f\"Average: {sum(results)/len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "daje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Discounted Cumulative Gain (NDCG)\n",
    "\n",
    "$$\n",
    "NDCG = \\frac{DCG}{IDCG}\n",
    "$$\n",
    "\n",
    "$$\n",
    "DCG = \\sum_{i=1}^{n}\\frac{relevance_{i}}{\\log_{2}(i+1)}\n",
    "$$\n",
    "\n",
    "Dove\n",
    "* $relevance_{i}$ è il *relevance score* di un elemento alla posizione $i$ nel ranking\n",
    "* $i$ è il rank (iniziando da 1)\n",
    "\n",
    "--- \n",
    "\n",
    "* Il $DCG$ è calcolato usando i *relevance score* del nostro sistema\n",
    "* Il $IDCG$ è calcolato allo stesso modo del $DCG$ ma usando i *relevance score* della **ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_relevance_score(lucene_json, truth_json):\n",
    "    with open(lucene_json, 'r') as f1, open(truth_json, 'r') as f2:\n",
    "        lucene_ranking = json.load(f1)\n",
    "        truth_ranking = json.load(f2)\n",
    "    \n",
    "    n = len(truth_ranking)\n",
    "    relevance_scores = {value: n - int(key) + 1 for key, value in truth_ranking.items()}\n",
    "\n",
    "    ranking = {}\n",
    "    for key, value in lucene_ranking.items():\n",
    "        relevance_score = relevance_scores.get(value)\n",
    "        ranking[key] = {\"item\": value, \"relevance_score\": relevance_score}\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dcg(ranking):\n",
    "    dcg = 0.0\n",
    "    for i, key in enumerate(ranking, start=1):\n",
    "        relevance_score = ranking[key][\"relevance_score\"]\n",
    "        dcg += relevance_score / math.log2(i + 1)\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idcg(ranking):\n",
    "    sorted_relevance = sorted((v[\"relevance_score\"] for v in ranking.values()), reverse=True)\n",
    "    \n",
    "    idcg = 0.0\n",
    "    for i, relevance_score in enumerate(sorted_relevance, start=1):\n",
    "        idcg += relevance_score / math.log2(i + 1)\n",
    "\n",
    "    return idcg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG for generic-metrics/query1.json: 0.9831306821489963\n",
      "NDCG for generic-metrics/query2.json: 0.972737265142328\n",
      "NDCG for generic-metrics/query3.json: 0.8789381085401112\n",
      "NDCG for dataset-specific/query1.json: 0.9986372961784811\n",
      "NDCG for dataset-specific/query2.json: 0.9133597294931859\n",
      "NDCG for dataset-specific/query3.json: 0.6975149037011815\n",
      "NDCG for model-specific/query1.json: 1.0\n",
      "NDCG for model-specific/query2.json: 0.9320920269228142\n",
      "NDCG for model-specific/query3.json: 0.9076303300494618\n",
      "Average on all files: 0.9204489269085067\n"
     ]
    }
   ],
   "source": [
    "ndcg_values = []\n",
    "\n",
    "for raw, gpt in zip(RAWS_PATH, GPT_PATH):\n",
    "    ranking = assign_relevance_score(raw, gpt)\n",
    "    dcg = compute_dcg(ranking)\n",
    "    idcg = compute_idcg(ranking)\n",
    "\n",
    "    ndcg = dcg / idcg\n",
    "    print(f\"NDCG for {raw.replace('-raw', '')}: {ndcg}\")\n",
    "    ndcg_values.append(ndcg)\n",
    "\n",
    "print(f\"Average on all files: {sum(ndcg_values)/len(ndcg_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Habemus NDCG\n",
    "avg = 0.9204489269085067 sembra pure bono che te devo di :D\n",
    "\n",
    "Ma proviamolo con il **random baseline**!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average on 10 shuffles: 0.8351258677669422\n"
     ]
    }
   ],
   "source": [
    "avg_values = []\n",
    "for i in range(0, 10):\n",
    "    shuffle()\n",
    "    ndcg_values = []\n",
    "\n",
    "    for raw, gpt in zip(RAWS_PATH, RND_PATH):\n",
    "        ranking = assign_relevance_score(raw, gpt)\n",
    "        dcg = compute_dcg(ranking)\n",
    "        idcg = compute_idcg(ranking)\n",
    "\n",
    "        ndcg = dcg / idcg\n",
    "        ndcg_values.append(ndcg)\n",
    "\n",
    "    avg_values.append(sum(ndcg_values)/len(ndcg_values))\n",
    "\n",
    "print(f\"Average on 10 shuffles: {sum(avg_values)/len(avg_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hai capito eh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text ltx_font_bold" id="id6.6.6">
     Yao Mu
     <sup class="ltx_sup" id="id6.6.6.1">
      <span class="ltx_text ltx_font_medium" id="id6.6.6.1.1">
       1
      </span>
     </sup>
     , Qinglong Zhang
     <sup class="ltx_sup" id="id6.6.6.2">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id6.6.6.2.1">
       2
      </span>
     </sup>
     , Mengkang Hu
     <sup class="ltx_sup" id="id6.6.6.3">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id6.6.6.3.1">
       1
      </span>
     </sup>
     , Wenhai Wang
     <sup class="ltx_sup" id="id6.6.6.4">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id6.6.6.4.1">
       2
      </span>
     </sup>
     , Mingyu Ding
     <math alttext="{}^{\ ,1}" class="ltx_math_unparsed" display="inline" id="id5.5.5.m5.1">
      <semantics id="id5.5.5.m5.1a">
       <msup id="id5.5.5.m5.1.1">
        <mi id="id5.5.5.m5.1.1a">
        </mi>
        <mrow id="id5.5.5.m5.1.1.1">
         <mo id="id5.5.5.m5.1.1.1.1">
          ,
         </mo>
         <mn id="id5.5.5.m5.1.1.1.2">
          1
         </mn>
        </mrow>
       </msup>
       <annotation encoding="application/x-tex" id="id5.5.5.m5.1b">
        {}^{\ ,1}
       </annotation>
      </semantics>
     </math>
     , Jun Jin
     <sup class="ltx_sup" id="id6.6.6.5">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id6.6.6.5.1">
       3
      </span>
     </sup>
     ,
    </span>
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_bold" id="id11.11.11">
     Bin Wang
     <sup class="ltx_sup" id="id11.11.11.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id11.11.11.1.1">
       3
      </span>
     </sup>
     , Jifeng Dai
     <sup class="ltx_sup" id="id11.11.11.2">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id11.11.11.2.1">
       2
      </span>
     </sup>
     , Yu Qiao
     <sup class="ltx_sup" id="id11.11.11.3">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id11.11.11.3.1">
       2
      </span>
     </sup>
     , Ping Luo
     <sup class="ltx_sup" id="id11.11.11.4">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id11.11.11.4.1">
       ‚àó,1,2
      </span>
     </sup>
     <br class="ltx_break"/>
     <sup class="ltx_sup" id="id11.11.11.5">
      <span class="ltx_text ltx_font_medium" id="id11.11.11.5.1">
       1
      </span>
     </sup>
    </span>
    The University of Hong Kong,
    <sup class="ltx_sup" id="id14.14.id1">
     2
    </sup>
    Shanghai AI Laboratory,
    <sup class="ltx_sup" id="id15.15.id2">
     3
    </sup>
    Noah‚Äôs Ark Laboratory
    <br class="ltx_break"/>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_text ltx_font_bold" id="id16.16.id1">
     Corresponding authors: Mingyu Ding and Ping Luo ({dingmyu, pluo.lhi}@gmail.com)
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id17.id1">
   Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments.
In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities.
To achieve this, we have made the following efforts:
(i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the "Chain of Thoughts" mode for effective embodied planning.
(ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning.
(iii) We introduce a paradigm for extracting task-related features from LLM-generated planning queries to form a closed loop between high-level planning and low-level control.
Extensive experiments show the effectiveness of EmbodiedGPT on embodied tasks, including embodied planning, embodied control, visual captioning, and visual question answering.
Notably, EmbodiedGPT significantly enhances the success rate of the embodied control task by extracting more effective features. It has achieved a remarkable 1.6 times increase in success rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset.
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Embodied AI tasks, e.g., embodied planning, embodied VQA, and embodied control, aim to imbue robots with the ability to perceive, reason, and act within their environment, enabling them to perform long-horizon plans and execute actions autonomously based on real-time observations.
Recently, large language models (LLMs) such as GPT4
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    and PaLM-E
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib2" title="">
      2
     </a>
     ]
    </cite>
    , have shown promising language understanding, reasoning, and "chain-of-thought" capabilities.
Such advances may open new possibilities for developing robots capable of processing natural language instructions, performing multi-modal "chain-of-thought", and planning actions in physical environments.
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Large-scale datasets play important roles in training large language models.
For example, OpenCLIP trains its ViT-G/14 model on the LAION-2B dataset
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib3" title="">
      3
     </a>
     ]
    </cite>
    , which contains 2B image-language pairs.
Unlike general-purpose visual language tasks that can get a huge amount of weakly labeled image-caption pairs from the Internet, embodied AI tasks require egocentric data in robotics domains.
Also, structured language instructions are needed for precise planning, which usually requires huge manual efforts and costs.
This poses a challenging problem in collecting high-quality embodied multi-modal data.
Some researchers
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib4" title="">
      4
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib6" title="">
      6
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ]
    </cite>
    explore creating large-scale embodied datasets with simulators, but a significant gap remains between simulation and the real world.
Recent works
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib8" title="">
      8
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib9" title="">
      9
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ]
    </cite>
    also explore adapting the pre-trained LLMs to a new domain by efficient tuning strategies like LoRA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib11" title="">
      11
     </a>
     ]
    </cite>
    .
However, several open questions still remain: how to apply LLMs to the field of robotics which may face large domain gaps; how to leverage the "chain-of-thought" capability for structured planning; and how to use the output language plan for downstream manipulation tasks in an end-to-end manner.
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="240" id="S1.F1.g1" src="/html/2305.15021/assets/x1.png" width="456"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">
      Figure 1
     </span>
     :
    </span>
    <span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">
     EmbodiedGPT‚Äôs capabilities for video captioning, multi-turn question answering, embodied planning, and low-level control. The plans given by EmbodiedGPT are highly executable and incorporate task-specific features, leading to a significant improvement in the success rate of embodied control tasks, outperforming both R3M
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib12" title="">
       12
      </a>
      ]
     </cite>
     (a video-language contrastive learned model) and BLIP-2
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib13" title="">
       13
      </a>
      ]
     </cite>
     (a multi-modal foundation model) on Franka Kitchen
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib14" title="">
       14
      </a>
      ]
     </cite>
     and Meta-World
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib15" title="">
       15
      </a>
      ]
     </cite>
     environments.
    </span>
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    To solve the above challenges, in this work, we first build a large-scale embodied planning dataset, termed EgoCOT, which features chain-of-thought planning instructions.
It contains carefully selected egocentric videos from the Ego4D dataset
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ]
    </cite>
    and corresponding high-quality step-by-step language instructions, which are machine-generated, then semantics-based filtered, and finally human-verified.
Additionally, we also create the EgoVQA dataset as an extension of the Ego4D dataset, focusing on egocentric human-object interaction video question-answering tasks, which aims to offer a wider range of egocentric multi-modal data.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    Based on our EgoCOT and EgoVQA, we present an end-to-end multi-modal embodied foundation model called EmbodiedGPT, which can interact with the physical world in a more natural and intuitive manner, and perform many embodied tasks, as shown in Figure
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    , such as embodied planning, embodied VQA, and embodied control.
EmbodiedGPT comprises four integrated modules that work together, including i) a frozen vision model for encoding visual features of current observations, ii) a frozen language model used to execute natural language for question answering, captioning, and embodied planning tasks, iii) an embodied-former with a language mapping layer for aligning the visual and embodied instructions and extracting task-relevant instance-level features with the generated planning for low-level control, and iv) a policy network, which is responsible for producing low-level actions based on the task-relevant features, enabling the agent to effectively interact with the environment. To further enhance EmbodiedGPT ‚Äôs performance in generating reliable planning containing sub-goal sequences, we implement prefix tuning on the frozen language model to encourage the generation of more executable planning.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    Our method possesses the following core advantages: i) the generated planning exhibits strong executability and granularity at the object part level, such as the gripper of a robotic arm or the handle of a door, manifested in sub-goal sequences. ii) the proposed EgoCOT dataset is built based on an open-source large-scale dataset, which offers greater scalability compared to the PaLM-E
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib2" title="">
      2
     </a>
     ]
    </cite>
    model trained on proprietary robot data. And both the EgoCOT dataset, and the EmbodiedGPT model will be open-sourced.
iii) EmbodiedGPT forms a closed-loop from high-level planning to low-level control, which enables seamless integration of high-level planning and low-level control, providing efficient task performance and adaptability to a wide range of tasks. To achieve this, we utilize the embodied-former to query task-relevant instance-level features through cross-attention between visual observations and generated embodied planning. This enables the policy network to complete low-level control tasks with fewer than 25 demonstrations.
   </p>
  </div>
  <div class="ltx_para" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    The contributions can be summarized as follows:
(i) We build an end-to-end multi-modal foundation model EmbodiedGPT for embodied AI, which is featured with "chain-of-thought" capability, empowering embodied agents to interact with the physical world in a more natural and intuitive manner.
(ii) We develop two datasets, EgoCOT and EgoVQA, consisting of 200M annotated videos from the Ego4D dataset with corresponding detailed planning instructions and VQA data. The datasets are first machine-generated, then semantics-based filtered, and finally human-verified for quality control.
(iii) We introduce EmbodiedGPT a cost-effective training approach and a paradigm for extracting task-relevant features from LLM-generated planning queries, thereby forming a closed loop between high-level planning and low-level control.
We demonstrate our approach‚Äôs effectiveness by achieving state-of-the-art or comparable performance on multiple embodied tasks, including embodied control, embodied planning, video captioning, and video QA. Notably, in comparison to BLIP-2
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib17" title="">
      17
     </a>
     ]
    </cite>
    fine-tuned on the Ego4D dataset and R3M
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ]
    </cite>
    specifically designed for manipulation tasks, EmbodiedGPT outperforms both models on the Franka Kitchen
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib14" title="">
      14
     </a>
     ]
    </cite>
    benchmark with a margin of 22.1% and 5.5%, respectively. Similarly, on the Meta-World
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib14" title="">
      14
     </a>
     ]
    </cite>
    benchmark, EmbodiedGPT surpasses both models with margins of 22.5% and 4.2%, respectively.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    Vision Language Pre-training with large scale foundation model
   </h3>
   <div class="ltx_para" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     Vision-Language Pre-training focuses on strengthening the link between visual observation and natural language. The goal is to develop models that can better understand and process visual content, such as recognizing objects and actions, and generating descriptive text.
As models become larger, the computational expense for end-to-end pre-training rises, leading to the need for modular vision-language pre-training methods. These methods smartly use pre-trained models, keeping them ‚Äòfrozen‚Äô during vision language pre-training to save on computational costs. For example, models like Uniter
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib18" title="">
       18
      </a>
      ]
     </cite>
     , Oscar
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib19" title="">
       19
      </a>
      ]
     </cite>
     , VinVL
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib20" title="">
       20
      </a>
      ]
     </cite>
     , and LiT
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib21" title="">
       21
      </a>
      ]
     </cite>
     freeze the image encoder, while Frozen
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib22" title="">
       22
      </a>
      ]
     </cite>
     and VGPT
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib23" title="">
       23
      </a>
      ]
     </cite>
     freeze the language model. Furthermore, Flamingo
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib24" title="">
       24
      </a>
      ]
     </cite>
     and BLIP-2
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib17" title="">
       17
      </a>
      ]
     </cite>
     use both frozen image encoders and language models, providing a balance between performance and computational efficiency.
Due to the lack of open-source data for multi-modal embodied planning, previous works struggled to perform detailed task decomposition and lacked the ability to generate precise and executable plans.
To tackle this issue, we create the EgoCOT dataset and develop an embodied chain-of-thought vision language pre-training framework to enhance the capacity of multi-modal models for embodied reasoning and planning.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Egocentric Video Datasets.
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     Egocentric videos, which are captured using wearable cameras, provide a natural perspective of daily activities and pose several challenging research questions
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib25" title="">
       25
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib26" title="">
       26
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib27" title="">
       27
      </a>
      ]
     </cite>
     . Several egocentric video datasets have been created over the years, including
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib29" title="">
       29
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib30" title="">
       30
      </a>
      ]
     </cite>
     . However, the collection of egocentric videos is expensive, and previous datasets tend to be small-scale and domain-specific. Recently, a massive egocentric video dataset, Ego4D
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib16" title="">
       16
      </a>
      ]
     </cite>
     , has been released and has been used for embodied representation learning. The dataset comprises 3,670 hours of videos collected by 931 people from 74 locations across 9 countries, with videos accompanied by narrations. For embodied AI tasks, learning from large and diverse egocentric human videos has emerged as a promising approach to acquiring a generally useful visual representation for controlling such tasks. For example, R3M
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib12" title="">
       12
      </a>
      ]
     </cite>
     developed a sparse and compact visual representation using the Ego4D human video dataset through a combination of time-contrastive learning and video-language alignment. VIP
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib31" title="">
       31
      </a>
      ]
     </cite>
     , learns general-purpose reward functions for goal-conditioned robotic manipulation using the Ego4D dataset.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.3
    </span>
    Large Foundation Model Assistant System
   </h3>
   <div class="ltx_para" id="S2.SS3.p1">
    <p class="ltx_p" id="S2.SS3.p1.1">
     Recent advancements in large-scale multi-modal language models (LLMs), such as GPT-3
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib32" title="">
       32
      </a>
      ]
     </cite>
     and GPT-4
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib1" title="">
       1
      </a>
      ]
     </cite>
     , have resulted in the creation of various models that can understand multiple modes of information. Two main approaches are used in this field: systematic collaboration and end-to-end trained models.
Systematic collaboration approaches involve coordinating multiple vision models or tools with language models to combine visual information with textual descriptions. Examples include models like Visual ChatGPT
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib33" title="">
       33
      </a>
      ]
     </cite>
     , MM-REACT
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib34" title="">
       34
      </a>
      ]
     </cite>
     , and HuggingGPT
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib35" title="">
       35
      </a>
      ]
     </cite>
     . However, this approach is limited by the accuracy and capacity of fixed modular models, which can lead to an accumulation of errors. On the other hand, end-to-end models aim to provide unified models for multi-modal tasks. For example, Flamingo
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib24" title="">
       24
      </a>
      ]
     </cite>
     combines vision and language by freezing pre-trained vision encoders and language models.
BLIP-2
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib13" title="">
       13
      </a>
      ]
     </cite>
     introduces Q-Former to align visual features from frozen visual encoders with large language models. Recently, models such as MiniGPT-4
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib36" title="">
       36
      </a>
      ]
     </cite>
     and LLaVA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib37" title="">
       37
      </a>
      ]
     </cite>
     align instruction-tuned language models with visual features from frozen visual backbones. VideoChat
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib38" title="">
       38
      </a>
      ]
     </cite>
     , mPLUG-Owl
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib39" title="">
       39
      </a>
      ]
     </cite>
     and X-LLM
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib40" title="">
       40
      </a>
      ]
     </cite>
     , further expand support for video input.
PaLM-E
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib41" title="">
       41
      </a>
      ]
     </cite>
     is the first large embodied multi-modal model, which directly incorporates features from sensor modalities to improve real-world performance and is trained with their large-scale everyday robot data
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib42" title="">
       42
      </a>
      ]
     </cite>
     . Compared to PaLM-E, EmbodiedGPT is more compact, with a size of only 10B and offers additional support for video captioning, video QA and making planning according to a demonstration video. Furthermore, we form a closed-loop system that spans from high-level planning to low-level control.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Method
  </h2>
  <figure class="ltx_figure" id="S3.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="226" id="S3.F2.g1" src="/html/2305.15021/assets/x2.png" width="456"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">
      Figure 2
     </span>
     :
    </span>
    <span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">
     Overall framework of EmbodiedGPT. The black arrow shows the vision-language planning process, while the red arrow represents that we leverage the queried language plans for better policy learning in low-level control tasks.
    </span>
   </figcaption>
  </figure>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    The goal of the embodied foundation model is to imitate human-like perception and interaction with the environment by accurately perceiving the environment, identifying relevant objects, analyzing their spatial relationships, and formulating a detailed task plan.
To achieve this, the EmbodiedGPT employs a pre-trained vision transformer as the visual encoder and a pre-trained LLaMA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib43" title="">
      43
     </a>
     ]
    </cite>
    model as the language model. As shown in Figure
    <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‚Ä£ 3 Method ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    , the embodied-former acts as a bridge between the visual and language domains, it first extracts compact visual features from the output of the vision model through attention-based interaction involving visual tokens, text queries, and learnable embodied queries and then maps it to the language modality through a language mapping layer.
These embeddings are sent to the frozen LLaMA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib43" title="">
      43
     </a>
     ]
    </cite>
    language model for visual caption, visual QA, and embodied planning.
The generated planning is then used to query highly relevant features from the general visual tokens encoded by the visual model via the embodied-former. These features are utilized to generate low-level control commands for task execution through the downstream policy network. To enhance performance across a range of embodied tasks, we introduce a novel video-language pre-training paradigm that leverages a cognitive chain of thought to produce embodied planning from egocentric video inputs. We formulate this task as a standard VQA (Visual Question Answering) task, using "how to do the task that + original caption" as the question and embodied planning as the answer. This framework enriches the data of embodied planning and standard visual QA tasks, encouraging the embodied-former to capture task-specific features that are more suitable for embodied control tasks.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Framework
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     The training process consists of three stages, each designed to incrementally develop reasoning and planning capabilities. The first two stages focus on pre-training in basic cognitive and responsive skills, while the third stage involves training the embodied AI task with egocentric video-text data on EgoCOT. In the first stage, we focus on image-text conversation alignment pre-training, which involves using three datasets: COCO Caption
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib44" title="">
       44
      </a>
      ]
     </cite>
     , 595 thousand finely filtered image-text pairs from CC3M
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib45" title="">
       45
      </a>
      ]
     </cite>
     , and 491 thousand filtered image-text pairs obtained by re-captioning LAION-400M using BLIP-2
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib17" title="">
       17
      </a>
      ]
     </cite>
     . The primary goal of this stage is to pre-train the Embodied-former and language projection while keeping the vision and language model parameters frozen to save computational resources.
In the second stage, our goal is to enhance the model‚Äôs ability to comprehend and generate more complex sentences and improve its reasoning skills. We achieve this by updating the language projection and prefix language adapter and utilizing the "Complex_Reasoning_77k" and multi-turn conversation datasets provided by "LLaVA_Instruct_150K"
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib46" title="">
       46
      </a>
      ]
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">
      Embodied "chain-of-thought" training with EgoCOT
     </span>
     : During the third stage, we first use Conv3D
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib47" title="">
       47
      </a>
      ]
     </cite>
     to transfer the pre-trained vision model from stage 2 to the video encoder, with a time offset of 2 and a total frame count of 8 for the videos. Then, we introduce the ‚Äôchain-of-thought‚Äô vision language pre-training paradigm where the model takes 8 keyframes of the video as input, along with the task description, embodied planning, and structured verb-noun pairs summary to reason with a prompt, such as Listing
     <a class="ltx_ref" href="#LST1" title="Listing 1 ‚Ä£ 3.1 Framework ‚Ä£ 3 Method ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     . To avoid overfitting, we provide a prompt set that has different instructions with the same meaning. In this stage, we fine-tune the patch embedding, the language projection layer, and the prefix language adapter to better capture temporal information.
    </p>
   </div>
   <figure class="ltx_float ltx_lstlisting" id="LST1">
    <div class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="LST1.1">
     <div class="ltx_listing_data">
      <a download="" href="data:text/plain;base64,V2F0Y2ggdGhpcyB2aWRlbywgaWRlbnRpZnkgdGhlIGFjdGlvbnMgYW5kIGRldmlzZSBhIHBsYW4gdXNpbmcgY2hhaW4tb2YtdGhvdWdodC4gRXh0cmFjdCBkZXRhaWxlZCBhY3Rpb25zIHVzaW5nIHRoaXMgc2NoZW1hOgpUYXNrOiB7InRhc2sgZGVzY3JpcHRpb24ifQpQbGFuOiB7InBsYW4gd2l0aCBjaGFpbi1vZi10aG91Z2h0In0gQWN0aW9uczoge3sibnVtYmVyIn06IHsndmVyYid9KHsnbm91bid9KX0u">
       ‚¨á
      </a>
     </div>
     <div class="ltx_listingline" id="lstnumberx1">
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.1" style="font-size:70%;">
       Watch
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.2" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.3" style="font-size:70%;">
       this
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.4" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.5" style="font-size:70%;">
       video
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx1.6" style="font-size:70%;">
       ,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.7" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.8" style="font-size:70%;">
       identify
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.9" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.10" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.11" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.12" style="font-size:70%;">
       actions
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.13" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx1.14" style="font-size:70%;">
       and
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.15" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.16" style="font-size:70%;">
       devise
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.17" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.18" style="font-size:70%;">
       a
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.19" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.20" style="font-size:70%;">
       plan
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.21" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.22" style="font-size:70%;">
       using
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.23" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.24" style="font-size:70%;">
       chain
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx1.25" style="font-size:70%;">
       -
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.26" style="font-size:70%;">
       of
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx1.27" style="font-size:70%;">
       -
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.28" style="font-size:70%;">
       thought
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx1.29" style="font-size:70%;">
       .
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.30" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.31" style="font-size:70%;">
       Extract
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.32" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.33" style="font-size:70%;">
       detailed
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.34" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.35" style="font-size:70%;">
       actions
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.36" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.37" style="font-size:70%;">
       using
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.38" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.39" style="font-size:70%;">
       this
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.40" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.41" style="font-size:70%;">
       schema
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx1.42" style="font-size:70%;">
       :
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx2">
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.1" style="font-size:70%;">
       Task
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx2.2" style="font-size:70%;">
       :
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.3" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx2.4" style="font-size:70%;">
       {
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx2.5" style="font-size:70%;">
       "task
       <span class="ltx_text ltx_lst_space" id="lstnumberx2.5.1">
       </span>
       description"
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx2.6" style="font-size:70%;">
       }
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx3">
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.1" style="font-size:70%;">
       Plan
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx3.2" style="font-size:70%;">
       :
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.3" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx3.4" style="font-size:70%;">
       {
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx3.5" style="font-size:70%;">
       "plan
       <span class="ltx_text ltx_lst_space" id="lstnumberx3.5.1">
       </span>
       with
       <span class="ltx_text ltx_lst_space" id="lstnumberx3.5.2">
       </span>
       chain-of-thought"
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx3.6" style="font-size:70%;">
       }
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.7" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.8" style="font-size:70%;">
       Actions
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx3.9" style="font-size:70%;">
       :
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.10" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx3.11" style="font-size:70%;">
       {{
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx3.12" style="font-size:70%;">
       "number"
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx3.13" style="font-size:70%;">
       }:
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.14" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx3.15" style="font-size:70%;">
       {
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx3.16" style="font-size:70%;">
       ‚Äôverb‚Äô
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx3.17" style="font-size:70%;">
       }({
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx3.18" style="font-size:70%;">
       ‚Äônoun‚Äô
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx3.19" style="font-size:70%;">
       })}.
      </span>
     </div>
    </div>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_float">
      Listing¬†1:
     </span>
     Prompt we used for chain-of-thought pre-training.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Model Architecture
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.23">
     The Embodied-former, denoted as
     <math alttext="\mathcal{E}(\cdot)" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1">
      <semantics id="S3.SS2.p1.1.m1.1a">
       <mrow id="S3.SS2.p1.1.m1.1.2" xref="S3.SS2.p1.1.m1.1.2.cmml">
        <mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.1.m1.1.2.2" xref="S3.SS2.p1.1.m1.1.2.2.cmml">
         ‚Ñ∞
        </mi>
        <mo id="S3.SS2.p1.1.m1.1.2.1" lspace="0em" rspace="0em" xref="S3.SS2.p1.1.m1.1.2.1.cmml">
         ‚Äã
        </mo>
        <mrow id="S3.SS2.p1.1.m1.1.2.3.2" xref="S3.SS2.p1.1.m1.1.2.cmml">
         <mo id="S3.SS2.p1.1.m1.1.2.3.2.1" stretchy="false" xref="S3.SS2.p1.1.m1.1.2.cmml">
          (
         </mo>
         <mo id="S3.SS2.p1.1.m1.1.1" lspace="0em" rspace="0em" xref="S3.SS2.p1.1.m1.1.1.cmml">
          ‚ãÖ
         </mo>
         <mo id="S3.SS2.p1.1.m1.1.2.3.2.2" stretchy="false" xref="S3.SS2.p1.1.m1.1.2.cmml">
          )
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b">
        <apply id="S3.SS2.p1.1.m1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.2">
         <times id="S3.SS2.p1.1.m1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.2.1">
         </times>
         <ci id="S3.SS2.p1.1.m1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.2.2">
          ‚Ñ∞
         </ci>
         <ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">
          ‚ãÖ
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">
        \mathcal{E}(\cdot)
       </annotation>
      </semantics>
     </math>
     , serves as a bridge between visual input
     <math alttext="x_{\mathrm{vis}}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1">
      <semantics id="S3.SS2.p1.2.m2.1a">
       <msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">
        <mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">
         x
        </mi>
        <mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">
         vis
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b">
        <apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">
          ùë•
         </ci>
         <ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">
          vis
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">
        x_{\mathrm{vis}}
       </annotation>
      </semantics>
     </math>
     and the frozen language model, acting as an information bottleneck that delivers the most relevant visual data to the language model.
The Embodied-former consists of two sub-modules: one for extracting features from the image input, denoted as
     <math alttext="\mathcal{E}_{\mathrm{vis}}:x_{\mathrm{vis}}\rightarrow y_{\mathrm{vis}}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1">
      <semantics id="S3.SS2.p1.3.m3.1a">
       <mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">
        <msub id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">
         <mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.3.m3.1.1.2.2" xref="S3.SS2.p1.3.m3.1.1.2.2.cmml">
          ‚Ñ∞
         </mi>
         <mi id="S3.SS2.p1.3.m3.1.1.2.3" xref="S3.SS2.p1.3.m3.1.1.2.3.cmml">
          vis
         </mi>
        </msub>
        <mo id="S3.SS2.p1.3.m3.1.1.1" lspace="0.278em" rspace="0.278em" xref="S3.SS2.p1.3.m3.1.1.1.cmml">
         :
        </mo>
        <mrow id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">
         <msub id="S3.SS2.p1.3.m3.1.1.3.2" xref="S3.SS2.p1.3.m3.1.1.3.2.cmml">
          <mi id="S3.SS2.p1.3.m3.1.1.3.2.2" xref="S3.SS2.p1.3.m3.1.1.3.2.2.cmml">
           x
          </mi>
          <mi id="S3.SS2.p1.3.m3.1.1.3.2.3" xref="S3.SS2.p1.3.m3.1.1.3.2.3.cmml">
           vis
          </mi>
         </msub>
         <mo id="S3.SS2.p1.3.m3.1.1.3.1" stretchy="false" xref="S3.SS2.p1.3.m3.1.1.3.1.cmml">
          ‚Üí
         </mo>
         <msub id="S3.SS2.p1.3.m3.1.1.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.cmml">
          <mi id="S3.SS2.p1.3.m3.1.1.3.3.2" xref="S3.SS2.p1.3.m3.1.1.3.3.2.cmml">
           y
          </mi>
          <mi id="S3.SS2.p1.3.m3.1.1.3.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.3.cmml">
           vis
          </mi>
         </msub>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b">
        <apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">
         <ci id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1">
          :
         </ci>
         <apply id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">
          <csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.p1.3.m3.1.1.2">
           subscript
          </csymbol>
          <ci id="S3.SS2.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2.2">
           ‚Ñ∞
          </ci>
          <ci id="S3.SS2.p1.3.m3.1.1.2.3.cmml" xref="S3.SS2.p1.3.m3.1.1.2.3">
           vis
          </ci>
         </apply>
         <apply id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">
          <ci id="S3.SS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3.1">
           ‚Üí
          </ci>
          <apply id="S3.SS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2">
           <csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.3.2.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2">
            subscript
           </csymbol>
           <ci id="S3.SS2.p1.3.m3.1.1.3.2.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2.2">
            ùë•
           </ci>
           <ci id="S3.SS2.p1.3.m3.1.1.3.2.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2.3">
            vis
           </ci>
          </apply>
          <apply id="S3.SS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3">
           <csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3">
            subscript
           </csymbol>
           <ci id="S3.SS2.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3.2">
            ùë¶
           </ci>
           <ci id="S3.SS2.p1.3.m3.1.1.3.3.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3.3">
            vis
           </ci>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">
        \mathcal{E}_{\mathrm{vis}}:x_{\mathrm{vis}}\rightarrow y_{\mathrm{vis}}
       </annotation>
      </semantics>
     </math>
     , and another for extracting features from the text input, denoted as
     <math alttext="\mathcal{E}_{\mathrm{txt}}:x_{\mathrm{txt}}\rightarrow y_{\mathrm{txt}}" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1">
      <semantics id="S3.SS2.p1.4.m4.1a">
       <mrow id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">
        <msub id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">
         <mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.4.m4.1.1.2.2" xref="S3.SS2.p1.4.m4.1.1.2.2.cmml">
          ‚Ñ∞
         </mi>
         <mi id="S3.SS2.p1.4.m4.1.1.2.3" xref="S3.SS2.p1.4.m4.1.1.2.3.cmml">
          txt
         </mi>
        </msub>
        <mo id="S3.SS2.p1.4.m4.1.1.1" lspace="0.278em" rspace="0.278em" xref="S3.SS2.p1.4.m4.1.1.1.cmml">
         :
        </mo>
        <mrow id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">
         <msub id="S3.SS2.p1.4.m4.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.3.2.cmml">
          <mi id="S3.SS2.p1.4.m4.1.1.3.2.2" xref="S3.SS2.p1.4.m4.1.1.3.2.2.cmml">
           x
          </mi>
          <mi id="S3.SS2.p1.4.m4.1.1.3.2.3" xref="S3.SS2.p1.4.m4.1.1.3.2.3.cmml">
           txt
          </mi>
         </msub>
         <mo id="S3.SS2.p1.4.m4.1.1.3.1" stretchy="false" xref="S3.SS2.p1.4.m4.1.1.3.1.cmml">
          ‚Üí
         </mo>
         <msub id="S3.SS2.p1.4.m4.1.1.3.3" xref="S3.SS2.p1.4.m4.1.1.3.3.cmml">
          <mi id="S3.SS2.p1.4.m4.1.1.3.3.2" xref="S3.SS2.p1.4.m4.1.1.3.3.2.cmml">
           y
          </mi>
          <mi id="S3.SS2.p1.4.m4.1.1.3.3.3" xref="S3.SS2.p1.4.m4.1.1.3.3.3.cmml">
           txt
          </mi>
         </msub>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b">
        <apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">
         <ci id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1">
          :
         </ci>
         <apply id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">
          <csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.2.1.cmml" xref="S3.SS2.p1.4.m4.1.1.2">
           subscript
          </csymbol>
          <ci id="S3.SS2.p1.4.m4.1.1.2.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2.2">
           ‚Ñ∞
          </ci>
          <ci id="S3.SS2.p1.4.m4.1.1.2.3.cmml" xref="S3.SS2.p1.4.m4.1.1.2.3">
           txt
          </ci>
         </apply>
         <apply id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">
          <ci id="S3.SS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3.1">
           ‚Üí
          </ci>
          <apply id="S3.SS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2">
           <csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.3.2.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2">
            subscript
           </csymbol>
           <ci id="S3.SS2.p1.4.m4.1.1.3.2.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2.2">
            ùë•
           </ci>
           <ci id="S3.SS2.p1.4.m4.1.1.3.2.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2.3">
            txt
           </ci>
          </apply>
          <apply id="S3.SS2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3.3">
           <csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.3.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3.3">
            subscript
           </csymbol>
           <ci id="S3.SS2.p1.4.m4.1.1.3.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.3.2">
            ùë¶
           </ci>
           <ci id="S3.SS2.p1.4.m4.1.1.3.3.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3.3.3">
            txt
           </ci>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">
        \mathcal{E}_{\mathrm{txt}}:x_{\mathrm{txt}}\rightarrow y_{\mathrm{txt}}
       </annotation>
      </semantics>
     </math>
     .
We employ
     <math alttext="N" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1">
      <semantics id="S3.SS2.p1.5.m5.1a">
       <mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">
        N
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b">
        <ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">
         ùëÅ
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">
        N
       </annotation>
      </semantics>
     </math>
     learnable embodied query embeddings
     <math alttext="y_{\mathrm{query}}" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1">
      <semantics id="S3.SS2.p1.6.m6.1a">
       <msub id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">
        <mi id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">
         y
        </mi>
        <mi id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml">
         query
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b">
        <apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">
          ùë¶
         </ci>
         <ci id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3">
          query
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">
        y_{\mathrm{query}}
       </annotation>
      </semantics>
     </math>
     as the input of
     <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m7.1">
      <semantics id="S3.SS2.p1.7.m7.1a">
       <mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">
        ‚Ñ∞
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b">
        <ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">
         ‚Ñ∞
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">
        \mathcal{E}
       </annotation>
      </semantics>
     </math>
     to interact with
     <math alttext="x_{\mathrm{vis}}" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m8.1">
      <semantics id="S3.SS2.p1.8.m8.1a">
       <msub id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">
        <mi id="S3.SS2.p1.8.m8.1.1.2" xref="S3.SS2.p1.8.m8.1.1.2.cmml">
         x
        </mi>
        <mi id="S3.SS2.p1.8.m8.1.1.3" xref="S3.SS2.p1.8.m8.1.1.3.cmml">
         vis
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b">
        <apply id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.8.m8.1.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.8.m8.1.1.2.cmml" xref="S3.SS2.p1.8.m8.1.1.2">
          ùë•
         </ci>
         <ci id="S3.SS2.p1.8.m8.1.1.3.cmml" xref="S3.SS2.p1.8.m8.1.1.3">
          vis
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">
        x_{\mathrm{vis}}
       </annotation>
      </semantics>
     </math>
     through cross-attention layers and with
     <math alttext="x_{\mathrm{txt}}" class="ltx_Math" display="inline" id="S3.SS2.p1.9.m9.1">
      <semantics id="S3.SS2.p1.9.m9.1a">
       <msub id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml">
        <mi id="S3.SS2.p1.9.m9.1.1.2" xref="S3.SS2.p1.9.m9.1.1.2.cmml">
         x
        </mi>
        <mi id="S3.SS2.p1.9.m9.1.1.3" xref="S3.SS2.p1.9.m9.1.1.3.cmml">
         txt
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b">
        <apply id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.9.m9.1.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.9.m9.1.1.2.cmml" xref="S3.SS2.p1.9.m9.1.1.2">
          ùë•
         </ci>
         <ci id="S3.SS2.p1.9.m9.1.1.3.cmml" xref="S3.SS2.p1.9.m9.1.1.3">
          txt
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">
        x_{\mathrm{txt}}
       </annotation>
      </semantics>
     </math>
     through self-attention layers. We denote the output query representation as
     <math alttext="z\in\mathbb{R}^{N\times D}" class="ltx_Math" display="inline" id="S3.SS2.p1.10.m10.1">
      <semantics id="S3.SS2.p1.10.m10.1a">
       <mrow id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml">
        <mi id="S3.SS2.p1.10.m10.1.1.2" xref="S3.SS2.p1.10.m10.1.1.2.cmml">
         z
        </mi>
        <mo id="S3.SS2.p1.10.m10.1.1.1" xref="S3.SS2.p1.10.m10.1.1.1.cmml">
         ‚àà
        </mo>
        <msup id="S3.SS2.p1.10.m10.1.1.3" xref="S3.SS2.p1.10.m10.1.1.3.cmml">
         <mi id="S3.SS2.p1.10.m10.1.1.3.2" xref="S3.SS2.p1.10.m10.1.1.3.2.cmml">
          ‚Ñù
         </mi>
         <mrow id="S3.SS2.p1.10.m10.1.1.3.3" xref="S3.SS2.p1.10.m10.1.1.3.3.cmml">
          <mi id="S3.SS2.p1.10.m10.1.1.3.3.2" xref="S3.SS2.p1.10.m10.1.1.3.3.2.cmml">
           N
          </mi>
          <mo id="S3.SS2.p1.10.m10.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.10.m10.1.1.3.3.1.cmml">
           √ó
          </mo>
          <mi id="S3.SS2.p1.10.m10.1.1.3.3.3" xref="S3.SS2.p1.10.m10.1.1.3.3.3.cmml">
           D
          </mi>
         </mrow>
        </msup>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b">
        <apply id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1">
         <in id="S3.SS2.p1.10.m10.1.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1.1">
         </in>
         <ci id="S3.SS2.p1.10.m10.1.1.2.cmml" xref="S3.SS2.p1.10.m10.1.1.2">
          ùëß
         </ci>
         <apply id="S3.SS2.p1.10.m10.1.1.3.cmml" xref="S3.SS2.p1.10.m10.1.1.3">
          <csymbol cd="ambiguous" id="S3.SS2.p1.10.m10.1.1.3.1.cmml" xref="S3.SS2.p1.10.m10.1.1.3">
           superscript
          </csymbol>
          <ci id="S3.SS2.p1.10.m10.1.1.3.2.cmml" xref="S3.SS2.p1.10.m10.1.1.3.2">
           ‚Ñù
          </ci>
          <apply id="S3.SS2.p1.10.m10.1.1.3.3.cmml" xref="S3.SS2.p1.10.m10.1.1.3.3">
           <times id="S3.SS2.p1.10.m10.1.1.3.3.1.cmml" xref="S3.SS2.p1.10.m10.1.1.3.3.1">
           </times>
           <ci id="S3.SS2.p1.10.m10.1.1.3.3.2.cmml" xref="S3.SS2.p1.10.m10.1.1.3.3.2">
            ùëÅ
           </ci>
           <ci id="S3.SS2.p1.10.m10.1.1.3.3.3.cmml" xref="S3.SS2.p1.10.m10.1.1.3.3.3">
            ùê∑
           </ci>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">
        z\in\mathbb{R}^{N\times D}
       </annotation>
      </semantics>
     </math>
     , where
     <math alttext="D" class="ltx_Math" display="inline" id="S3.SS2.p1.11.m11.1">
      <semantics id="S3.SS2.p1.11.m11.1a">
       <mi id="S3.SS2.p1.11.m11.1.1" xref="S3.SS2.p1.11.m11.1.1.cmml">
        D
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m11.1b">
        <ci id="S3.SS2.p1.11.m11.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1">
         ùê∑
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.11.m11.1c">
        D
       </annotation>
      </semantics>
     </math>
     is the dimensionality of the embeddings. The dimension of
     <math alttext="z" class="ltx_Math" display="inline" id="S3.SS2.p1.12.m12.1">
      <semantics id="S3.SS2.p1.12.m12.1a">
       <mi id="S3.SS2.p1.12.m12.1.1" xref="S3.SS2.p1.12.m12.1.1.cmml">
        z
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m12.1b">
        <ci id="S3.SS2.p1.12.m12.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1">
         ùëß
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.12.m12.1c">
        z
       </annotation>
      </semantics>
     </math>
     is significantly smaller than that of the visual features.
The output query embeddings are then transformed to
     <math alttext="z^{{}^{\prime}}\in\mathbb{R}^{N\times D^{{}^{\prime}}}" class="ltx_Math" display="inline" id="S3.SS2.p1.13.m13.1">
      <semantics id="S3.SS2.p1.13.m13.1a">
       <mrow id="S3.SS2.p1.13.m13.1.1" xref="S3.SS2.p1.13.m13.1.1.cmml">
        <msup id="S3.SS2.p1.13.m13.1.1.2" xref="S3.SS2.p1.13.m13.1.1.2.cmml">
         <mi id="S3.SS2.p1.13.m13.1.1.2.2" xref="S3.SS2.p1.13.m13.1.1.2.2.cmml">
          z
         </mi>
         <msup id="S3.SS2.p1.13.m13.1.1.2.3" xref="S3.SS2.p1.13.m13.1.1.2.3.cmml">
          <mi id="S3.SS2.p1.13.m13.1.1.2.3a" xref="S3.SS2.p1.13.m13.1.1.2.3.cmml">
          </mi>
          <mo id="S3.SS2.p1.13.m13.1.1.2.3.1" xref="S3.SS2.p1.13.m13.1.1.2.3.1.cmml">
           ‚Ä≤
          </mo>
         </msup>
        </msup>
        <mo id="S3.SS2.p1.13.m13.1.1.1" xref="S3.SS2.p1.13.m13.1.1.1.cmml">
         ‚àà
        </mo>
        <msup id="S3.SS2.p1.13.m13.1.1.3" xref="S3.SS2.p1.13.m13.1.1.3.cmml">
         <mi id="S3.SS2.p1.13.m13.1.1.3.2" xref="S3.SS2.p1.13.m13.1.1.3.2.cmml">
          ‚Ñù
         </mi>
         <mrow id="S3.SS2.p1.13.m13.1.1.3.3" xref="S3.SS2.p1.13.m13.1.1.3.3.cmml">
          <mi id="S3.SS2.p1.13.m13.1.1.3.3.2" xref="S3.SS2.p1.13.m13.1.1.3.3.2.cmml">
           N
          </mi>
          <mo id="S3.SS2.p1.13.m13.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.13.m13.1.1.3.3.1.cmml">
           √ó
          </mo>
          <msup id="S3.SS2.p1.13.m13.1.1.3.3.3" xref="S3.SS2.p1.13.m13.1.1.3.3.3.cmml">
           <mi id="S3.SS2.p1.13.m13.1.1.3.3.3.2" xref="S3.SS2.p1.13.m13.1.1.3.3.3.2.cmml">
            D
           </mi>
           <msup id="S3.SS2.p1.13.m13.1.1.3.3.3.3" xref="S3.SS2.p1.13.m13.1.1.3.3.3.3.cmml">
            <mi id="S3.SS2.p1.13.m13.1.1.3.3.3.3a" xref="S3.SS2.p1.13.m13.1.1.3.3.3.3.cmml">
            </mi>
            <mo id="S3.SS2.p1.13.m13.1.1.3.3.3.3.1" xref="S3.SS2.p1.13.m13.1.1.3.3.3.3.1.cmml">
             ‚Ä≤
            </mo>
           </msup>
          </msup>
         </mrow>
        </msup>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.13.m13.1b">
        <apply id="S3.SS2.p1.13.m13.1.1.cmml" xref="S3.SS2.p1.13.m13.1.1">
         <in id="S3.SS2.p1.13.m13.1.1.1.cmml" xref="S3.SS2.p1.13.m13.1.1.1">
         </in>
         <apply id="S3.SS2.p1.13.m13.1.1.2.cmml" xref="S3.SS2.p1.13.m13.1.1.2">
          <csymbol cd="ambiguous" id="S3.SS2.p1.13.m13.1.1.2.1.cmml" xref="S3.SS2.p1.13.m13.1.1.2">
           superscript
          </csymbol>
          <ci id="S3.SS2.p1.13.m13.1.1.2.2.cmml" xref="S3.SS2.p1.13.m13.1.1.2.2">
           ùëß
          </ci>
          <apply id="S3.SS2.p1.13.m13.1.1.2.3.cmml" xref="S3.SS2.p1.13.m13.1.1.2.3">
           <ci id="S3.SS2.p1.13.m13.1.1.2.3.1.cmml" xref="S3.SS2.p1.13.m13.1.1.2.3.1">
            ‚Ä≤
           </ci>
          </apply>
         </apply>
         <apply id="S3.SS2.p1.13.m13.1.1.3.cmml" xref="S3.SS2.p1.13.m13.1.1.3">
          <csymbol cd="ambiguous" id="S3.SS2.p1.13.m13.1.1.3.1.cmml" xref="S3.SS2.p1.13.m13.1.1.3">
           superscript
          </csymbol>
          <ci id="S3.SS2.p1.13.m13.1.1.3.2.cmml" xref="S3.SS2.p1.13.m13.1.1.3.2">
           ‚Ñù
          </ci>
          <apply id="S3.SS2.p1.13.m13.1.1.3.3.cmml" xref="S3.SS2.p1.13.m13.1.1.3.3">
           <times id="S3.SS2.p1.13.m13.1.1.3.3.1.cmml" xref="S3.SS2.p1.13.m13.1.1.3.3.1">
           </times>
           <ci id="S3.SS2.p1.13.m13.1.1.3.3.2.cmml" xref="S3.SS2.p1.13.m13.1.1.3.3.2">
            ùëÅ
           </ci>
           <apply id="S3.SS2.p1.13.m13.1.1.3.3.3.cmml" xref="S3.SS2.p1.13.m13.1.1.3.3.3">
            <csymbol cd="ambiguous" id="S3.SS2.p1.13.m13.1.1.3.3.3.1.cmml" xref="S3.SS2.p1.13.m13.1.1.3.3.3">
             superscript
            </csymbol>
            <ci id="S3.SS2.p1.13.m13.1.1.3.3.3.2.cmml" xref="S3.SS2.p1.13.m13.1.1.3.3.3.2">
             ùê∑
            </ci>
            <apply id="S3.SS2.p1.13.m13.1.1.3.3.3.3.cmml" xref="S3.SS2.p1.13.m13.1.1.3.3.3.3">
             <ci id="S3.SS2.p1.13.m13.1.1.3.3.3.3.1.cmml" xref="S3.SS2.p1.13.m13.1.1.3.3.3.3.1">
              ‚Ä≤
             </ci>
            </apply>
           </apply>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.13.m13.1c">
        z^{{}^{\prime}}\in\mathbb{R}^{N\times D^{{}^{\prime}}}
       </annotation>
      </semantics>
     </math>
     , which have the same dimensionality
     <math alttext="D^{{}^{\prime}}" class="ltx_Math" display="inline" id="S3.SS2.p1.14.m14.1">
      <semantics id="S3.SS2.p1.14.m14.1a">
       <msup id="S3.SS2.p1.14.m14.1.1" xref="S3.SS2.p1.14.m14.1.1.cmml">
        <mi id="S3.SS2.p1.14.m14.1.1.2" xref="S3.SS2.p1.14.m14.1.1.2.cmml">
         D
        </mi>
        <msup id="S3.SS2.p1.14.m14.1.1.3" xref="S3.SS2.p1.14.m14.1.1.3.cmml">
         <mi id="S3.SS2.p1.14.m14.1.1.3a" xref="S3.SS2.p1.14.m14.1.1.3.cmml">
         </mi>
         <mo id="S3.SS2.p1.14.m14.1.1.3.1" xref="S3.SS2.p1.14.m14.1.1.3.1.cmml">
          ‚Ä≤
         </mo>
        </msup>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.14.m14.1b">
        <apply id="S3.SS2.p1.14.m14.1.1.cmml" xref="S3.SS2.p1.14.m14.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.14.m14.1.1.1.cmml" xref="S3.SS2.p1.14.m14.1.1">
          superscript
         </csymbol>
         <ci id="S3.SS2.p1.14.m14.1.1.2.cmml" xref="S3.SS2.p1.14.m14.1.1.2">
          ùê∑
         </ci>
         <apply id="S3.SS2.p1.14.m14.1.1.3.cmml" xref="S3.SS2.p1.14.m14.1.1.3">
          <ci id="S3.SS2.p1.14.m14.1.1.3.1.cmml" xref="S3.SS2.p1.14.m14.1.1.3.1">
           ‚Ä≤
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.14.m14.1c">
        D^{{}^{\prime}}
       </annotation>
      </semantics>
     </math>
     as the LLM‚Äôs text embedding in the language modality. This transformation is performed by a mapping function denoted as
     <math alttext="M:z\rightarrow z^{{}^{\prime}}" class="ltx_Math" display="inline" id="S3.SS2.p1.15.m15.1">
      <semantics id="S3.SS2.p1.15.m15.1a">
       <mrow id="S3.SS2.p1.15.m15.1.1" xref="S3.SS2.p1.15.m15.1.1.cmml">
        <mi id="S3.SS2.p1.15.m15.1.1.2" xref="S3.SS2.p1.15.m15.1.1.2.cmml">
         M
        </mi>
        <mo id="S3.SS2.p1.15.m15.1.1.1" lspace="0.278em" rspace="0.278em" xref="S3.SS2.p1.15.m15.1.1.1.cmml">
         :
        </mo>
        <mrow id="S3.SS2.p1.15.m15.1.1.3" xref="S3.SS2.p1.15.m15.1.1.3.cmml">
         <mi id="S3.SS2.p1.15.m15.1.1.3.2" xref="S3.SS2.p1.15.m15.1.1.3.2.cmml">
          z
         </mi>
         <mo id="S3.SS2.p1.15.m15.1.1.3.1" stretchy="false" xref="S3.SS2.p1.15.m15.1.1.3.1.cmml">
          ‚Üí
         </mo>
         <msup id="S3.SS2.p1.15.m15.1.1.3.3" xref="S3.SS2.p1.15.m15.1.1.3.3.cmml">
          <mi id="S3.SS2.p1.15.m15.1.1.3.3.2" xref="S3.SS2.p1.15.m15.1.1.3.3.2.cmml">
           z
          </mi>
          <msup id="S3.SS2.p1.15.m15.1.1.3.3.3" xref="S3.SS2.p1.15.m15.1.1.3.3.3.cmml">
           <mi id="S3.SS2.p1.15.m15.1.1.3.3.3a" xref="S3.SS2.p1.15.m15.1.1.3.3.3.cmml">
           </mi>
           <mo id="S3.SS2.p1.15.m15.1.1.3.3.3.1" xref="S3.SS2.p1.15.m15.1.1.3.3.3.1.cmml">
            ‚Ä≤
           </mo>
          </msup>
         </msup>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.15.m15.1b">
        <apply id="S3.SS2.p1.15.m15.1.1.cmml" xref="S3.SS2.p1.15.m15.1.1">
         <ci id="S3.SS2.p1.15.m15.1.1.1.cmml" xref="S3.SS2.p1.15.m15.1.1.1">
          :
         </ci>
         <ci id="S3.SS2.p1.15.m15.1.1.2.cmml" xref="S3.SS2.p1.15.m15.1.1.2">
          ùëÄ
         </ci>
         <apply id="S3.SS2.p1.15.m15.1.1.3.cmml" xref="S3.SS2.p1.15.m15.1.1.3">
          <ci id="S3.SS2.p1.15.m15.1.1.3.1.cmml" xref="S3.SS2.p1.15.m15.1.1.3.1">
           ‚Üí
          </ci>
          <ci id="S3.SS2.p1.15.m15.1.1.3.2.cmml" xref="S3.SS2.p1.15.m15.1.1.3.2">
           ùëß
          </ci>
          <apply id="S3.SS2.p1.15.m15.1.1.3.3.cmml" xref="S3.SS2.p1.15.m15.1.1.3.3">
           <csymbol cd="ambiguous" id="S3.SS2.p1.15.m15.1.1.3.3.1.cmml" xref="S3.SS2.p1.15.m15.1.1.3.3">
            superscript
           </csymbol>
           <ci id="S3.SS2.p1.15.m15.1.1.3.3.2.cmml" xref="S3.SS2.p1.15.m15.1.1.3.3.2">
            ùëß
           </ci>
           <apply id="S3.SS2.p1.15.m15.1.1.3.3.3.cmml" xref="S3.SS2.p1.15.m15.1.1.3.3.3">
            <ci id="S3.SS2.p1.15.m15.1.1.3.3.3.1.cmml" xref="S3.SS2.p1.15.m15.1.1.3.3.3.1">
             ‚Ä≤
            </ci>
           </apply>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.15.m15.1c">
        M:z\rightarrow z^{{}^{\prime}}
       </annotation>
      </semantics>
     </math>
     , which is accomplished by a linear projection via a fully-connected (FC) layer. The projected embeddings,
     <math alttext="z^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p1.16.m16.1">
      <semantics id="S3.SS2.p1.16.m16.1a">
       <msup id="S3.SS2.p1.16.m16.1.1" xref="S3.SS2.p1.16.m16.1.1.cmml">
        <mi id="S3.SS2.p1.16.m16.1.1.2" xref="S3.SS2.p1.16.m16.1.1.2.cmml">
         z
        </mi>
        <mo id="S3.SS2.p1.16.m16.1.1.3" xref="S3.SS2.p1.16.m16.1.1.3.cmml">
         ‚Ä≤
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.16.m16.1b">
        <apply id="S3.SS2.p1.16.m16.1.1.cmml" xref="S3.SS2.p1.16.m16.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.16.m16.1.1.1.cmml" xref="S3.SS2.p1.16.m16.1.1">
          superscript
         </csymbol>
         <ci id="S3.SS2.p1.16.m16.1.1.2.cmml" xref="S3.SS2.p1.16.m16.1.1.2">
          ùëß
         </ci>
         <ci id="S3.SS2.p1.16.m16.1.1.3.cmml" xref="S3.SS2.p1.16.m16.1.1.3">
          ‚Ä≤
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.16.m16.1c">
        z^{\prime}
       </annotation>
      </semantics>
     </math>
     , serve as "soft visual prompts for the language model," decoupling the whole interaction into visual-query interaction and query-text interaction. The final embodied planning is inferred by the language model with
     <math alttext="z^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p1.17.m17.1">
      <semantics id="S3.SS2.p1.17.m17.1a">
       <msup id="S3.SS2.p1.17.m17.1.1" xref="S3.SS2.p1.17.m17.1.1.cmml">
        <mi id="S3.SS2.p1.17.m17.1.1.2" xref="S3.SS2.p1.17.m17.1.1.2.cmml">
         z
        </mi>
        <mo id="S3.SS2.p1.17.m17.1.1.3" xref="S3.SS2.p1.17.m17.1.1.3.cmml">
         ‚Ä≤
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.17.m17.1b">
        <apply id="S3.SS2.p1.17.m17.1.1.cmml" xref="S3.SS2.p1.17.m17.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.17.m17.1.1.1.cmml" xref="S3.SS2.p1.17.m17.1.1">
          superscript
         </csymbol>
         <ci id="S3.SS2.p1.17.m17.1.1.2.cmml" xref="S3.SS2.p1.17.m17.1.1.2">
          ùëß
         </ci>
         <ci id="S3.SS2.p1.17.m17.1.1.3.cmml" xref="S3.SS2.p1.17.m17.1.1.3">
          ‚Ä≤
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.17.m17.1c">
        z^{\prime}
       </annotation>
      </semantics>
     </math>
     and text prompt(shown as Listing
     <a class="ltx_ref" href="#LST1" title="Listing 1 ‚Ä£ 3.1 Framework ‚Ä£ 3 Method ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     ) as input. For low-level control which aims to generate actions to interact with the environment, the embodied plan
     <math alttext="x_{\mathrm{plan}}" class="ltx_Math" display="inline" id="S3.SS2.p1.18.m18.1">
      <semantics id="S3.SS2.p1.18.m18.1a">
       <msub id="S3.SS2.p1.18.m18.1.1" xref="S3.SS2.p1.18.m18.1.1.cmml">
        <mi id="S3.SS2.p1.18.m18.1.1.2" xref="S3.SS2.p1.18.m18.1.1.2.cmml">
         x
        </mi>
        <mi id="S3.SS2.p1.18.m18.1.1.3" xref="S3.SS2.p1.18.m18.1.1.3.cmml">
         plan
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.18.m18.1b">
        <apply id="S3.SS2.p1.18.m18.1.1.cmml" xref="S3.SS2.p1.18.m18.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.18.m18.1.1.1.cmml" xref="S3.SS2.p1.18.m18.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.18.m18.1.1.2.cmml" xref="S3.SS2.p1.18.m18.1.1.2">
          ùë•
         </ci>
         <ci id="S3.SS2.p1.18.m18.1.1.3.cmml" xref="S3.SS2.p1.18.m18.1.1.3">
          plan
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.18.m18.1c">
        x_{\mathrm{plan}}
       </annotation>
      </semantics>
     </math>
     is used as input text for embodied-former to query the task-relevant instance level features
     <math alttext="z_{\mathrm{instance}}=\mathcal{E}(x_{\mathrm{vis}},x_{\mathrm{plan}},y_{\mathrm{query}})" class="ltx_Math" display="inline" id="S3.SS2.p1.19.m19.3">
      <semantics id="S3.SS2.p1.19.m19.3a">
       <mrow id="S3.SS2.p1.19.m19.3.3" xref="S3.SS2.p1.19.m19.3.3.cmml">
        <msub id="S3.SS2.p1.19.m19.3.3.5" xref="S3.SS2.p1.19.m19.3.3.5.cmml">
         <mi id="S3.SS2.p1.19.m19.3.3.5.2" xref="S3.SS2.p1.19.m19.3.3.5.2.cmml">
          z
         </mi>
         <mi id="S3.SS2.p1.19.m19.3.3.5.3" xref="S3.SS2.p1.19.m19.3.3.5.3.cmml">
          instance
         </mi>
        </msub>
        <mo id="S3.SS2.p1.19.m19.3.3.4" xref="S3.SS2.p1.19.m19.3.3.4.cmml">
         =
        </mo>
        <mrow id="S3.SS2.p1.19.m19.3.3.3" xref="S3.SS2.p1.19.m19.3.3.3.cmml">
         <mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.19.m19.3.3.3.5" xref="S3.SS2.p1.19.m19.3.3.3.5.cmml">
          ‚Ñ∞
         </mi>
         <mo id="S3.SS2.p1.19.m19.3.3.3.4" lspace="0em" rspace="0em" xref="S3.SS2.p1.19.m19.3.3.3.4.cmml">
          ‚Äã
         </mo>
         <mrow id="S3.SS2.p1.19.m19.3.3.3.3.3" xref="S3.SS2.p1.19.m19.3.3.3.3.4.cmml">
          <mo id="S3.SS2.p1.19.m19.3.3.3.3.3.4" stretchy="false" xref="S3.SS2.p1.19.m19.3.3.3.3.4.cmml">
           (
          </mo>
          <msub id="S3.SS2.p1.19.m19.1.1.1.1.1.1" xref="S3.SS2.p1.19.m19.1.1.1.1.1.1.cmml">
           <mi id="S3.SS2.p1.19.m19.1.1.1.1.1.1.2" xref="S3.SS2.p1.19.m19.1.1.1.1.1.1.2.cmml">
            x
           </mi>
           <mi id="S3.SS2.p1.19.m19.1.1.1.1.1.1.3" xref="S3.SS2.p1.19.m19.1.1.1.1.1.1.3.cmml">
            vis
           </mi>
          </msub>
          <mo id="S3.SS2.p1.19.m19.3.3.3.3.3.5" xref="S3.SS2.p1.19.m19.3.3.3.3.4.cmml">
           ,
          </mo>
          <msub id="S3.SS2.p1.19.m19.2.2.2.2.2.2" xref="S3.SS2.p1.19.m19.2.2.2.2.2.2.cmml">
           <mi id="S3.SS2.p1.19.m19.2.2.2.2.2.2.2" xref="S3.SS2.p1.19.m19.2.2.2.2.2.2.2.cmml">
            x
           </mi>
           <mi id="S3.SS2.p1.19.m19.2.2.2.2.2.2.3" xref="S3.SS2.p1.19.m19.2.2.2.2.2.2.3.cmml">
            plan
           </mi>
          </msub>
          <mo id="S3.SS2.p1.19.m19.3.3.3.3.3.6" xref="S3.SS2.p1.19.m19.3.3.3.3.4.cmml">
           ,
          </mo>
          <msub id="S3.SS2.p1.19.m19.3.3.3.3.3.3" xref="S3.SS2.p1.19.m19.3.3.3.3.3.3.cmml">
           <mi id="S3.SS2.p1.19.m19.3.3.3.3.3.3.2" xref="S3.SS2.p1.19.m19.3.3.3.3.3.3.2.cmml">
            y
           </mi>
           <mi id="S3.SS2.p1.19.m19.3.3.3.3.3.3.3" xref="S3.SS2.p1.19.m19.3.3.3.3.3.3.3.cmml">
            query
           </mi>
          </msub>
          <mo id="S3.SS2.p1.19.m19.3.3.3.3.3.7" stretchy="false" xref="S3.SS2.p1.19.m19.3.3.3.3.4.cmml">
           )
          </mo>
         </mrow>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.19.m19.3b">
        <apply id="S3.SS2.p1.19.m19.3.3.cmml" xref="S3.SS2.p1.19.m19.3.3">
         <eq id="S3.SS2.p1.19.m19.3.3.4.cmml" xref="S3.SS2.p1.19.m19.3.3.4">
         </eq>
         <apply id="S3.SS2.p1.19.m19.3.3.5.cmml" xref="S3.SS2.p1.19.m19.3.3.5">
          <csymbol cd="ambiguous" id="S3.SS2.p1.19.m19.3.3.5.1.cmml" xref="S3.SS2.p1.19.m19.3.3.5">
           subscript
          </csymbol>
          <ci id="S3.SS2.p1.19.m19.3.3.5.2.cmml" xref="S3.SS2.p1.19.m19.3.3.5.2">
           ùëß
          </ci>
          <ci id="S3.SS2.p1.19.m19.3.3.5.3.cmml" xref="S3.SS2.p1.19.m19.3.3.5.3">
           instance
          </ci>
         </apply>
         <apply id="S3.SS2.p1.19.m19.3.3.3.cmml" xref="S3.SS2.p1.19.m19.3.3.3">
          <times id="S3.SS2.p1.19.m19.3.3.3.4.cmml" xref="S3.SS2.p1.19.m19.3.3.3.4">
          </times>
          <ci id="S3.SS2.p1.19.m19.3.3.3.5.cmml" xref="S3.SS2.p1.19.m19.3.3.3.5">
           ‚Ñ∞
          </ci>
          <vector id="S3.SS2.p1.19.m19.3.3.3.3.4.cmml" xref="S3.SS2.p1.19.m19.3.3.3.3.3">
           <apply id="S3.SS2.p1.19.m19.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.19.m19.1.1.1.1.1.1">
            <csymbol cd="ambiguous" id="S3.SS2.p1.19.m19.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.19.m19.1.1.1.1.1.1">
             subscript
            </csymbol>
            <ci id="S3.SS2.p1.19.m19.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.19.m19.1.1.1.1.1.1.2">
             ùë•
            </ci>
            <ci id="S3.SS2.p1.19.m19.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.19.m19.1.1.1.1.1.1.3">
             vis
            </ci>
           </apply>
           <apply id="S3.SS2.p1.19.m19.2.2.2.2.2.2.cmml" xref="S3.SS2.p1.19.m19.2.2.2.2.2.2">
            <csymbol cd="ambiguous" id="S3.SS2.p1.19.m19.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p1.19.m19.2.2.2.2.2.2">
             subscript
            </csymbol>
            <ci id="S3.SS2.p1.19.m19.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p1.19.m19.2.2.2.2.2.2.2">
             ùë•
            </ci>
            <ci id="S3.SS2.p1.19.m19.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p1.19.m19.2.2.2.2.2.2.3">
             plan
            </ci>
           </apply>
           <apply id="S3.SS2.p1.19.m19.3.3.3.3.3.3.cmml" xref="S3.SS2.p1.19.m19.3.3.3.3.3.3">
            <csymbol cd="ambiguous" id="S3.SS2.p1.19.m19.3.3.3.3.3.3.1.cmml" xref="S3.SS2.p1.19.m19.3.3.3.3.3.3">
             subscript
            </csymbol>
            <ci id="S3.SS2.p1.19.m19.3.3.3.3.3.3.2.cmml" xref="S3.SS2.p1.19.m19.3.3.3.3.3.3.2">
             ùë¶
            </ci>
            <ci id="S3.SS2.p1.19.m19.3.3.3.3.3.3.3.cmml" xref="S3.SS2.p1.19.m19.3.3.3.3.3.3.3">
             query
            </ci>
           </apply>
          </vector>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.19.m19.3c">
        z_{\mathrm{instance}}=\mathcal{E}(x_{\mathrm{vis}},x_{\mathrm{plan}},y_{\mathrm{query}})
       </annotation>
      </semantics>
     </math>
     .
Subsequently, the agent is capable of generating control commands, such as the turning angle of the servo, represented as
     <math alttext="a=g(z_{\mathrm{instance}},z_{\mathrm{global}})" class="ltx_Math" display="inline" id="S3.SS2.p1.20.m20.2">
      <semantics id="S3.SS2.p1.20.m20.2a">
       <mrow id="S3.SS2.p1.20.m20.2.2" xref="S3.SS2.p1.20.m20.2.2.cmml">
        <mi id="S3.SS2.p1.20.m20.2.2.4" xref="S3.SS2.p1.20.m20.2.2.4.cmml">
         a
        </mi>
        <mo id="S3.SS2.p1.20.m20.2.2.3" xref="S3.SS2.p1.20.m20.2.2.3.cmml">
         =
        </mo>
        <mrow id="S3.SS2.p1.20.m20.2.2.2" xref="S3.SS2.p1.20.m20.2.2.2.cmml">
         <mi id="S3.SS2.p1.20.m20.2.2.2.4" xref="S3.SS2.p1.20.m20.2.2.2.4.cmml">
          g
         </mi>
         <mo id="S3.SS2.p1.20.m20.2.2.2.3" lspace="0em" rspace="0em" xref="S3.SS2.p1.20.m20.2.2.2.3.cmml">
          ‚Äã
         </mo>
         <mrow id="S3.SS2.p1.20.m20.2.2.2.2.2" xref="S3.SS2.p1.20.m20.2.2.2.2.3.cmml">
          <mo id="S3.SS2.p1.20.m20.2.2.2.2.2.3" stretchy="false" xref="S3.SS2.p1.20.m20.2.2.2.2.3.cmml">
           (
          </mo>
          <msub id="S3.SS2.p1.20.m20.1.1.1.1.1.1" xref="S3.SS2.p1.20.m20.1.1.1.1.1.1.cmml">
           <mi id="S3.SS2.p1.20.m20.1.1.1.1.1.1.2" xref="S3.SS2.p1.20.m20.1.1.1.1.1.1.2.cmml">
            z
           </mi>
           <mi id="S3.SS2.p1.20.m20.1.1.1.1.1.1.3" xref="S3.SS2.p1.20.m20.1.1.1.1.1.1.3.cmml">
            instance
           </mi>
          </msub>
          <mo id="S3.SS2.p1.20.m20.2.2.2.2.2.4" xref="S3.SS2.p1.20.m20.2.2.2.2.3.cmml">
           ,
          </mo>
          <msub id="S3.SS2.p1.20.m20.2.2.2.2.2.2" xref="S3.SS2.p1.20.m20.2.2.2.2.2.2.cmml">
           <mi id="S3.SS2.p1.20.m20.2.2.2.2.2.2.2" xref="S3.SS2.p1.20.m20.2.2.2.2.2.2.2.cmml">
            z
           </mi>
           <mi id="S3.SS2.p1.20.m20.2.2.2.2.2.2.3" xref="S3.SS2.p1.20.m20.2.2.2.2.2.2.3.cmml">
            global
           </mi>
          </msub>
          <mo id="S3.SS2.p1.20.m20.2.2.2.2.2.5" stretchy="false" xref="S3.SS2.p1.20.m20.2.2.2.2.3.cmml">
           )
          </mo>
         </mrow>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.20.m20.2b">
        <apply id="S3.SS2.p1.20.m20.2.2.cmml" xref="S3.SS2.p1.20.m20.2.2">
         <eq id="S3.SS2.p1.20.m20.2.2.3.cmml" xref="S3.SS2.p1.20.m20.2.2.3">
         </eq>
         <ci id="S3.SS2.p1.20.m20.2.2.4.cmml" xref="S3.SS2.p1.20.m20.2.2.4">
          ùëé
         </ci>
         <apply id="S3.SS2.p1.20.m20.2.2.2.cmml" xref="S3.SS2.p1.20.m20.2.2.2">
          <times id="S3.SS2.p1.20.m20.2.2.2.3.cmml" xref="S3.SS2.p1.20.m20.2.2.2.3">
          </times>
          <ci id="S3.SS2.p1.20.m20.2.2.2.4.cmml" xref="S3.SS2.p1.20.m20.2.2.2.4">
           ùëî
          </ci>
          <interval closure="open" id="S3.SS2.p1.20.m20.2.2.2.2.3.cmml" xref="S3.SS2.p1.20.m20.2.2.2.2.2">
           <apply id="S3.SS2.p1.20.m20.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.20.m20.1.1.1.1.1.1">
            <csymbol cd="ambiguous" id="S3.SS2.p1.20.m20.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.20.m20.1.1.1.1.1.1">
             subscript
            </csymbol>
            <ci id="S3.SS2.p1.20.m20.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.20.m20.1.1.1.1.1.1.2">
             ùëß
            </ci>
            <ci id="S3.SS2.p1.20.m20.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.20.m20.1.1.1.1.1.1.3">
             instance
            </ci>
           </apply>
           <apply id="S3.SS2.p1.20.m20.2.2.2.2.2.2.cmml" xref="S3.SS2.p1.20.m20.2.2.2.2.2.2">
            <csymbol cd="ambiguous" id="S3.SS2.p1.20.m20.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p1.20.m20.2.2.2.2.2.2">
             subscript
            </csymbol>
            <ci id="S3.SS2.p1.20.m20.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p1.20.m20.2.2.2.2.2.2.2">
             ùëß
            </ci>
            <ci id="S3.SS2.p1.20.m20.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p1.20.m20.2.2.2.2.2.2.3">
             global
            </ci>
           </apply>
          </interval>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.20.m20.2c">
        a=g(z_{\mathrm{instance}},z_{\mathrm{global}})
       </annotation>
      </semantics>
     </math>
     . This function combines both the instance-specific information
     <math alttext="z_{\mathrm{instance}}" class="ltx_Math" display="inline" id="S3.SS2.p1.21.m21.1">
      <semantics id="S3.SS2.p1.21.m21.1a">
       <msub id="S3.SS2.p1.21.m21.1.1" xref="S3.SS2.p1.21.m21.1.1.cmml">
        <mi id="S3.SS2.p1.21.m21.1.1.2" xref="S3.SS2.p1.21.m21.1.1.2.cmml">
         z
        </mi>
        <mi id="S3.SS2.p1.21.m21.1.1.3" xref="S3.SS2.p1.21.m21.1.1.3.cmml">
         instance
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.21.m21.1b">
        <apply id="S3.SS2.p1.21.m21.1.1.cmml" xref="S3.SS2.p1.21.m21.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.21.m21.1.1.1.cmml" xref="S3.SS2.p1.21.m21.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.21.m21.1.1.2.cmml" xref="S3.SS2.p1.21.m21.1.1.2">
          ùëß
         </ci>
         <ci id="S3.SS2.p1.21.m21.1.1.3.cmml" xref="S3.SS2.p1.21.m21.1.1.3">
          instance
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.21.m21.1c">
        z_{\mathrm{instance}}
       </annotation>
      </semantics>
     </math>
     and the global context
     <math alttext="z_{\mathrm{global}}" class="ltx_Math" display="inline" id="S3.SS2.p1.22.m22.1">
      <semantics id="S3.SS2.p1.22.m22.1a">
       <msub id="S3.SS2.p1.22.m22.1.1" xref="S3.SS2.p1.22.m22.1.1.cmml">
        <mi id="S3.SS2.p1.22.m22.1.1.2" xref="S3.SS2.p1.22.m22.1.1.2.cmml">
         z
        </mi>
        <mi id="S3.SS2.p1.22.m22.1.1.3" xref="S3.SS2.p1.22.m22.1.1.3.cmml">
         global
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.22.m22.1b">
        <apply id="S3.SS2.p1.22.m22.1.1.cmml" xref="S3.SS2.p1.22.m22.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.22.m22.1.1.1.cmml" xref="S3.SS2.p1.22.m22.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.22.m22.1.1.2.cmml" xref="S3.SS2.p1.22.m22.1.1.2">
          ùëß
         </ci>
         <ci id="S3.SS2.p1.22.m22.1.1.3.cmml" xref="S3.SS2.p1.22.m22.1.1.3">
          global
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.22.m22.1c">
        z_{\mathrm{global}}
       </annotation>
      </semantics>
     </math>
     . The global context is inferred using a ResNet50 model
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib48" title="">
       48
      </a>
      ]
     </cite>
     that has been pre-trained on ImageNet
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib49" title="">
       49
      </a>
      ]
     </cite>
     , employing global average pooling.
Here,
     <math alttext="g(\cdot)" class="ltx_Math" display="inline" id="S3.SS2.p1.23.m23.1">
      <semantics id="S3.SS2.p1.23.m23.1a">
       <mrow id="S3.SS2.p1.23.m23.1.2" xref="S3.SS2.p1.23.m23.1.2.cmml">
        <mi id="S3.SS2.p1.23.m23.1.2.2" xref="S3.SS2.p1.23.m23.1.2.2.cmml">
         g
        </mi>
        <mo id="S3.SS2.p1.23.m23.1.2.1" lspace="0em" rspace="0em" xref="S3.SS2.p1.23.m23.1.2.1.cmml">
         ‚Äã
        </mo>
        <mrow id="S3.SS2.p1.23.m23.1.2.3.2" xref="S3.SS2.p1.23.m23.1.2.cmml">
         <mo id="S3.SS2.p1.23.m23.1.2.3.2.1" stretchy="false" xref="S3.SS2.p1.23.m23.1.2.cmml">
          (
         </mo>
         <mo id="S3.SS2.p1.23.m23.1.1" lspace="0em" rspace="0em" xref="S3.SS2.p1.23.m23.1.1.cmml">
          ‚ãÖ
         </mo>
         <mo id="S3.SS2.p1.23.m23.1.2.3.2.2" stretchy="false" xref="S3.SS2.p1.23.m23.1.2.cmml">
          )
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.23.m23.1b">
        <apply id="S3.SS2.p1.23.m23.1.2.cmml" xref="S3.SS2.p1.23.m23.1.2">
         <times id="S3.SS2.p1.23.m23.1.2.1.cmml" xref="S3.SS2.p1.23.m23.1.2.1">
         </times>
         <ci id="S3.SS2.p1.23.m23.1.2.2.cmml" xref="S3.SS2.p1.23.m23.1.2.2">
          ùëî
         </ci>
         <ci id="S3.SS2.p1.23.m23.1.1.cmml" xref="S3.SS2.p1.23.m23.1.1">
          ‚ãÖ
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.23.m23.1c">
        g(\cdot)
       </annotation>
      </semantics>
     </math>
     represents the policy network, which is a Multi-Layer Perceptron (MLP)
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib50" title="">
       50
      </a>
      ]
     </cite>
     mapping function. The output of the policy network consists of specific executable actions, such as positions and velocities in the Cartesian coordinate system. More implementation details can be found in Appendix A.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Training Settings
   </h3>
   <div class="ltx_para" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     We employ the same pre-trained image encoder as BLIP-2
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib17" title="">
       17
      </a>
      ]
     </cite>
     . Specifically, we utilize the ViT-G/14 model from EVA-CLIP
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib51" title="">
       51
      </a>
      ]
     </cite>
     and remove its last layer, using the output features of the second last layer instead.
For the frozen language model, we adopt a pre-trained LLaMA-7B
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib43" title="">
       43
      </a>
      ]
     </cite>
     model and fine-tune it using the ShareGPT dataset and a GPT-4 generated 52K English instruction-following dataset
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib52" title="">
       52
      </a>
      ]
     </cite>
     . We then utilize the well-fine-tuned language model as the frozen language model for vision-language pre-training.
Additionally, we convert the data type of parameters of the frozen ViT
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib53" title="">
       53
      </a>
      ]
     </cite>
     and language model to FP16 during pre-training to increase efficiency.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.4
    </span>
    Creating EgoCOT and EgoVQA Dataset
   </h3>
   <div class="ltx_para" id="S3.SS4.p1">
    <p class="ltx_p" id="S3.SS4.p1.1">
     For our EgoCOT dataset, we obtain basic data from the Ego4D dataset
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib16" title="">
       16
      </a>
      ]
     </cite>
     , which includes
     <math alttext="9,645" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.2">
      <semantics id="S3.SS4.p1.1.m1.2a">
       <mrow id="S3.SS4.p1.1.m1.2.3.2" xref="S3.SS4.p1.1.m1.2.3.1.cmml">
        <mn id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">
         9
        </mn>
        <mo id="S3.SS4.p1.1.m1.2.3.2.1" xref="S3.SS4.p1.1.m1.2.3.1.cmml">
         ,
        </mo>
        <mn id="S3.SS4.p1.1.m1.2.2" xref="S3.SS4.p1.1.m1.2.2.cmml">
         645
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.2b">
        <list id="S3.SS4.p1.1.m1.2.3.1.cmml" xref="S3.SS4.p1.1.m1.2.3.2">
         <cn id="S3.SS4.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS4.p1.1.m1.1.1">
          9
         </cn>
         <cn id="S3.SS4.p1.1.m1.2.2.cmml" type="integer" xref="S3.SS4.p1.1.m1.2.2">
          645
         </cn>
        </list>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.2c">
        9,645
       </annotation>
      </semantics>
     </math>
     untrimmed videos of various durations ranging from 5 seconds to 7 hours.
To prepare the data for our purposes, we conducted two stages of data cleaning to prepare our data. In the first stage, we filtered out videos with missing or very short narrations (which made up 7.4% and 0.9% of the text, respectively), as well as those with unsure tags (which accounted for 4.0% of the text). We also excluded videos without human-object interaction, such as watching TV or walking. After this stage, we were left with 2.9 thousand hours of video, containing 3.85 million narrations, from 129 different scenarios covering 2927 hours of video.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS4.p2">
    <p class="ltx_p" id="S3.SS4.p2.5">
     To generate pairs of captions, embodied plannings, and corresponding video segments with time intervals, we utilized the EgoVLP framework
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib54" title="">
       54
      </a>
      ]
     </cite>
     to segment the video. The narrations are organized as a sequence of sentences
     <math alttext="{\mathcal{T}_{0},\cdots,\mathcal{T}_{n}}" class="ltx_Math" display="inline" id="S3.SS4.p2.1.m1.3">
      <semantics id="S3.SS4.p2.1.m1.3a">
       <mrow id="S3.SS4.p2.1.m1.3.3.2" xref="S3.SS4.p2.1.m1.3.3.3.cmml">
        <msub id="S3.SS4.p2.1.m1.2.2.1.1" xref="S3.SS4.p2.1.m1.2.2.1.1.cmml">
         <mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.1.m1.2.2.1.1.2" xref="S3.SS4.p2.1.m1.2.2.1.1.2.cmml">
          ùíØ
         </mi>
         <mn id="S3.SS4.p2.1.m1.2.2.1.1.3" xref="S3.SS4.p2.1.m1.2.2.1.1.3.cmml">
          0
         </mn>
        </msub>
        <mo id="S3.SS4.p2.1.m1.3.3.2.3" xref="S3.SS4.p2.1.m1.3.3.3.cmml">
         ,
        </mo>
        <mi id="S3.SS4.p2.1.m1.1.1" mathvariant="normal" xref="S3.SS4.p2.1.m1.1.1.cmml">
         ‚ãØ
        </mi>
        <mo id="S3.SS4.p2.1.m1.3.3.2.4" xref="S3.SS4.p2.1.m1.3.3.3.cmml">
         ,
        </mo>
        <msub id="S3.SS4.p2.1.m1.3.3.2.2" xref="S3.SS4.p2.1.m1.3.3.2.2.cmml">
         <mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.1.m1.3.3.2.2.2" xref="S3.SS4.p2.1.m1.3.3.2.2.2.cmml">
          ùíØ
         </mi>
         <mi id="S3.SS4.p2.1.m1.3.3.2.2.3" xref="S3.SS4.p2.1.m1.3.3.2.2.3.cmml">
          n
         </mi>
        </msub>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.3b">
        <list id="S3.SS4.p2.1.m1.3.3.3.cmml" xref="S3.SS4.p2.1.m1.3.3.2">
         <apply id="S3.SS4.p2.1.m1.2.2.1.1.cmml" xref="S3.SS4.p2.1.m1.2.2.1.1">
          <csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS4.p2.1.m1.2.2.1.1">
           subscript
          </csymbol>
          <ci id="S3.SS4.p2.1.m1.2.2.1.1.2.cmml" xref="S3.SS4.p2.1.m1.2.2.1.1.2">
           ùíØ
          </ci>
          <cn id="S3.SS4.p2.1.m1.2.2.1.1.3.cmml" type="integer" xref="S3.SS4.p2.1.m1.2.2.1.1.3">
           0
          </cn>
         </apply>
         <ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">
          ‚ãØ
         </ci>
         <apply id="S3.SS4.p2.1.m1.3.3.2.2.cmml" xref="S3.SS4.p2.1.m1.3.3.2.2">
          <csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.3.3.2.2.1.cmml" xref="S3.SS4.p2.1.m1.3.3.2.2">
           subscript
          </csymbol>
          <ci id="S3.SS4.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS4.p2.1.m1.3.3.2.2.2">
           ùíØ
          </ci>
          <ci id="S3.SS4.p2.1.m1.3.3.2.2.3.cmml" xref="S3.SS4.p2.1.m1.3.3.2.2.3">
           ùëõ
          </ci>
         </apply>
        </list>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.3c">
        {\mathcal{T}_{0},\cdots,\mathcal{T}_{n}}
       </annotation>
      </semantics>
     </math>
     with precise timestamps
     <math alttext="{t_{0},\cdots,t_{n}}" class="ltx_Math" display="inline" id="S3.SS4.p2.2.m2.3">
      <semantics id="S3.SS4.p2.2.m2.3a">
       <mrow id="S3.SS4.p2.2.m2.3.3.2" xref="S3.SS4.p2.2.m2.3.3.3.cmml">
        <msub id="S3.SS4.p2.2.m2.2.2.1.1" xref="S3.SS4.p2.2.m2.2.2.1.1.cmml">
         <mi id="S3.SS4.p2.2.m2.2.2.1.1.2" xref="S3.SS4.p2.2.m2.2.2.1.1.2.cmml">
          t
         </mi>
         <mn id="S3.SS4.p2.2.m2.2.2.1.1.3" xref="S3.SS4.p2.2.m2.2.2.1.1.3.cmml">
          0
         </mn>
        </msub>
        <mo id="S3.SS4.p2.2.m2.3.3.2.3" xref="S3.SS4.p2.2.m2.3.3.3.cmml">
         ,
        </mo>
        <mi id="S3.SS4.p2.2.m2.1.1" mathvariant="normal" xref="S3.SS4.p2.2.m2.1.1.cmml">
         ‚ãØ
        </mi>
        <mo id="S3.SS4.p2.2.m2.3.3.2.4" xref="S3.SS4.p2.2.m2.3.3.3.cmml">
         ,
        </mo>
        <msub id="S3.SS4.p2.2.m2.3.3.2.2" xref="S3.SS4.p2.2.m2.3.3.2.2.cmml">
         <mi id="S3.SS4.p2.2.m2.3.3.2.2.2" xref="S3.SS4.p2.2.m2.3.3.2.2.2.cmml">
          t
         </mi>
         <mi id="S3.SS4.p2.2.m2.3.3.2.2.3" xref="S3.SS4.p2.2.m2.3.3.2.2.3.cmml">
          n
         </mi>
        </msub>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.3b">
        <list id="S3.SS4.p2.2.m2.3.3.3.cmml" xref="S3.SS4.p2.2.m2.3.3.2">
         <apply id="S3.SS4.p2.2.m2.2.2.1.1.cmml" xref="S3.SS4.p2.2.m2.2.2.1.1">
          <csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.2.2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.2.2.1.1">
           subscript
          </csymbol>
          <ci id="S3.SS4.p2.2.m2.2.2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.2.2.1.1.2">
           ùë°
          </ci>
          <cn id="S3.SS4.p2.2.m2.2.2.1.1.3.cmml" type="integer" xref="S3.SS4.p2.2.m2.2.2.1.1.3">
           0
          </cn>
         </apply>
         <ci id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">
          ‚ãØ
         </ci>
         <apply id="S3.SS4.p2.2.m2.3.3.2.2.cmml" xref="S3.SS4.p2.2.m2.3.3.2.2">
          <csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.3.3.2.2.1.cmml" xref="S3.SS4.p2.2.m2.3.3.2.2">
           subscript
          </csymbol>
          <ci id="S3.SS4.p2.2.m2.3.3.2.2.2.cmml" xref="S3.SS4.p2.2.m2.3.3.2.2.2">
           ùë°
          </ci>
          <ci id="S3.SS4.p2.2.m2.3.3.2.2.3.cmml" xref="S3.SS4.p2.2.m2.3.3.2.2.3">
           ùëõ
          </ci>
         </apply>
        </list>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.3c">
        {t_{0},\cdots,t_{n}}
       </annotation>
      </semantics>
     </math>
     that indicate when a described event occurred.
For each narration
     <math alttext="\mathcal{T}_{i}" class="ltx_Math" display="inline" id="S3.SS4.p2.3.m3.1">
      <semantics id="S3.SS4.p2.3.m3.1a">
       <msub id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml">
        <mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml">
         ùíØ
        </mi>
        <mi id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b">
        <apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">
         <csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2">
          ùíØ
         </ci>
         <ci id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3">
          ùëñ
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">
        \mathcal{T}_{i}
       </annotation>
      </semantics>
     </math>
     with timestamp
     <math alttext="t_{i}" class="ltx_Math" display="inline" id="S3.SS4.p2.4.m4.1">
      <semantics id="S3.SS4.p2.4.m4.1a">
       <msub id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml">
        <mi id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml">
         t
        </mi>
        <mi id="S3.SS4.p2.4.m4.1.1.3" xref="S3.SS4.p2.4.m4.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b">
        <apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">
         <csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2">
          ùë°
         </ci>
         <ci id="S3.SS4.p2.4.m4.1.1.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3">
          ùëñ
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">
        t_{i}
       </annotation>
      </semantics>
     </math>
     , we paired it with a clip
     <math alttext="\mathcal{V}_{i}" class="ltx_Math" display="inline" id="S3.SS4.p2.5.m5.1">
      <semantics id="S3.SS4.p2.5.m5.1a">
       <msub id="S3.SS4.p2.5.m5.1.1" xref="S3.SS4.p2.5.m5.1.1.cmml">
        <mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.5.m5.1.1.2" xref="S3.SS4.p2.5.m5.1.1.2.cmml">
         ùí±
        </mi>
        <mi id="S3.SS4.p2.5.m5.1.1.3" xref="S3.SS4.p2.5.m5.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m5.1b">
        <apply id="S3.SS4.p2.5.m5.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1">
         <csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.1.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS4.p2.5.m5.1.1.2.cmml" xref="S3.SS4.p2.5.m5.1.1.2">
          ùí±
         </ci>
         <ci id="S3.SS4.p2.5.m5.1.1.3.cmml" xref="S3.SS4.p2.5.m5.1.1.3">
          ùëñ
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.5.m5.1c">
        \mathcal{V}_{i}
       </annotation>
      </semantics>
     </math>
     by determining its start and end time points:
    </p>
    <table class="ltx_equation ltx_eqn_table" id="S3.E1">
     <tbody>
      <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
       <td class="ltx_eqn_cell ltx_eqn_center_padleft">
       </td>
       <td class="ltx_eqn_cell ltx_align_center">
        <math alttext="[t_{i}^{start},t_{i}^{end}]=[t_{i}-\beta_{i}/2\alpha,~{}t_{i}+\beta_{i}/2\alpha]," class="ltx_Math" display="block" id="S3.E1.m1.1">
         <semantics id="S3.E1.m1.1a">
          <mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">
           <mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">
            <mrow id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.3.cmml">
             <mo id="S3.E1.m1.1.1.1.1.2.2.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.2.3.cmml">
              [
             </mo>
             <msubsup id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">
              <mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml">
               t
              </mi>
              <mi id="S3.E1.m1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml">
               i
              </mi>
              <mrow id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">
               <mi id="S3.E1.m1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml">
                s
               </mi>
               <mo id="S3.E1.m1.1.1.1.1.1.1.1.3.1" lspace="0em" rspace="0em" xref="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml">
                ‚Äã
               </mo>
               <mi id="S3.E1.m1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml">
                t
               </mi>
               <mo id="S3.E1.m1.1.1.1.1.1.1.1.3.1a" lspace="0em" rspace="0em" xref="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml">
                ‚Äã
               </mo>
               <mi id="S3.E1.m1.1.1.1.1.1.1.1.3.4" xref="S3.E1.m1.1.1.1.1.1.1.1.3.4.cmml">
                a
               </mi>
               <mo id="S3.E1.m1.1.1.1.1.1.1.1.3.1b" lspace="0em" rspace="0em" xref="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml">
                ‚Äã
               </mo>
               <mi id="S3.E1.m1.1.1.1.1.1.1.1.3.5" xref="S3.E1.m1.1.1.1.1.1.1.1.3.5.cmml">
                r
               </mi>
               <mo id="S3.E1.m1.1.1.1.1.1.1.1.3.1c" lspace="0em" rspace="0em" xref="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml">
                ‚Äã
               </mo>
               <mi id="S3.E1.m1.1.1.1.1.1.1.1.3.6" xref="S3.E1.m1.1.1.1.1.1.1.1.3.6.cmml">
                t
               </mi>
              </mrow>
             </msubsup>
             <mo id="S3.E1.m1.1.1.1.1.2.2.4" xref="S3.E1.m1.1.1.1.1.2.3.cmml">
              ,
             </mo>
             <msubsup id="S3.E1.m1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.cmml">
              <mi id="S3.E1.m1.1.1.1.1.2.2.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2.cmml">
               t
              </mi>
              <mi id="S3.E1.m1.1.1.1.1.2.2.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.2.2.3.cmml">
               i
              </mi>
              <mrow id="S3.E1.m1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.2.3.cmml">
               <mi id="S3.E1.m1.1.1.1.1.2.2.2.3.2" xref="S3.E1.m1.1.1.1.1.2.2.2.3.2.cmml">
                e
               </mi>
               <mo id="S3.E1.m1.1.1.1.1.2.2.2.3.1" lspace="0em" rspace="0em" xref="S3.E1.m1.1.1.1.1.2.2.2.3.1.cmml">
                ‚Äã
               </mo>
               <mi id="S3.E1.m1.1.1.1.1.2.2.2.3.3" xref="S3.E1.m1.1.1.1.1.2.2.2.3.3.cmml">
                n
               </mi>
               <mo id="S3.E1.m1.1.1.1.1.2.2.2.3.1a" lspace="0em" rspace="0em" xref="S3.E1.m1.1.1.1.1.2.2.2.3.1.cmml">
                ‚Äã
               </mo>
               <mi id="S3.E1.m1.1.1.1.1.2.2.2.3.4" xref="S3.E1.m1.1.1.1.1.2.2.2.3.4.cmml">
                d
               </mi>
              </mrow>
             </msubsup>
             <mo id="S3.E1.m1.1.1.1.1.2.2.5" stretchy="false" xref="S3.E1.m1.1.1.1.1.2.3.cmml">
              ]
             </mo>
            </mrow>
            <mo id="S3.E1.m1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.5.cmml">
             =
            </mo>
            <mrow id="S3.E1.m1.1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.1.4.3.cmml">
             <mo id="S3.E1.m1.1.1.1.1.4.2.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.4.3.cmml">
              [
             </mo>
             <mrow id="S3.E1.m1.1.1.1.1.3.1.1" xref="S3.E1.m1.1.1.1.1.3.1.1.cmml">
              <msub id="S3.E1.m1.1.1.1.1.3.1.1.2" xref="S3.E1.m1.1.1.1.1.3.1.1.2.cmml">
               <mi id="S3.E1.m1.1.1.1.1.3.1.1.2.2" xref="S3.E1.m1.1.1.1.1.3.1.1.2.2.cmml">
                t
               </mi>
               <mi id="S3.E1.m1.1.1.1.1.3.1.1.2.3" xref="S3.E1.m1.1.1.1.1.3.1.1.2.3.cmml">
                i
               </mi>
              </msub>
              <mo id="S3.E1.m1.1.1.1.1.3.1.1.1" xref="S3.E1.m1.1.1.1.1.3.1.1.1.cmml">
               ‚àí
              </mo>
              <mrow id="S3.E1.m1.1.1.1.1.3.1.1.3" xref="S3.E1.m1.1.1.1.1.3.1.1.3.cmml">
               <mrow id="S3.E1.m1.1.1.1.1.3.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.1.1.3.2.cmml">
                <msub id="S3.E1.m1.1.1.1.1.3.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.1.1.3.2.2.cmml">
                 <mi id="S3.E1.m1.1.1.1.1.3.1.1.3.2.2.2" xref="S3.E1.m1.1.1.1.1.3.1.1.3.2.2.2.cmml">
                  Œ≤
                 </mi>
                 <mi id="S3.E1.m1.1.1.1.1.3.1.1.3.2.2.3" xref="S3.E1.m1.1.1.1.1.3.1.1.3.2.2.3.cmml">
                  i
                 </mi>
                </msub>
                <mo id="S3.E1.m1.1.1.1.1.3.1.1.3.2.1" xref="S3.E1.m1.1.1.1.1.3.1.1.3.2.1.cmml">
                 /
                </mo>
                <mn id="S3.E1.m1.1.1.1.1.3.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.1.1.3.2.3.cmml">
                 2
                </mn>
               </mrow>
               <mo id="S3.E1.m1.1.1.1.1.3.1.1.3.1" lspace="0em" rspace="0em" xref="S3.E1.m1.1.1.1.1.3.1.1.3.1.cmml">
                ‚Äã
               </mo>
               <mi id="S3.E1.m1.1.1.1.1.3.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.1.1.3.3.cmml">
                Œ±
               </mi>
              </mrow>
             </mrow>
             <mo id="S3.E1.m1.1.1.1.1.4.2.4" rspace="0.497em" xref="S3.E1.m1.1.1.1.1.4.3.cmml">
              ,
             </mo>
             <mrow id="S3.E1.m1.1.1.1.1.4.2.2" xref="S3.E1.m1.1.1.1.1.4.2.2.cmml">
              <msub id="S3.E1.m1.1.1.1.1.4.2.2.2" xref="S3.E1.m1.1.1.1.1.4.2.2.2.cmml">
               <mi id="S3.E1.m1.1.1.1.1.4.2.2.2.2" xref="S3.E1.m1.1.1.1.1.4.2.2.2.2.cmml">
                t
               </mi>
               <mi id="S3.E1.m1.1.1.1.1.4.2.2.2.3" xref="S3.E1.m1.1.1.1.1.4.2.2.2.3.cmml">
                i
               </mi>
              </msub>
              <mo id="S3.E1.m1.1.1.1.1.4.2.2.1" xref="S3.E1.m1.1.1.1.1.4.2.2.1.cmml">
               +
              </mo>
              <mrow id="S3.E1.m1.1.1.1.1.4.2.2.3" xref="S3.E1.m1.1.1.1.1.4.2.2.3.cmml">
               <mrow id="S3.E1.m1.1.1.1.1.4.2.2.3.2" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2.cmml">
                <msub id="S3.E1.m1.1.1.1.1.4.2.2.3.2.2" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2.2.cmml">
                 <mi id="S3.E1.m1.1.1.1.1.4.2.2.3.2.2.2" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2.2.2.cmml">
                  Œ≤
                 </mi>
                 <mi id="S3.E1.m1.1.1.1.1.4.2.2.3.2.2.3" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2.2.3.cmml">
                  i
                 </mi>
                </msub>
                <mo id="S3.E1.m1.1.1.1.1.4.2.2.3.2.1" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2.1.cmml">
                 /
                </mo>
                <mn id="S3.E1.m1.1.1.1.1.4.2.2.3.2.3" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2.3.cmml">
                 2
                </mn>
               </mrow>
               <mo id="S3.E1.m1.1.1.1.1.4.2.2.3.1" lspace="0em" rspace="0em" xref="S3.E1.m1.1.1.1.1.4.2.2.3.1.cmml">
                ‚Äã
               </mo>
               <mi id="S3.E1.m1.1.1.1.1.4.2.2.3.3" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.cmml">
                Œ±
               </mi>
              </mrow>
             </mrow>
             <mo id="S3.E1.m1.1.1.1.1.4.2.5" stretchy="false" xref="S3.E1.m1.1.1.1.1.4.3.cmml">
              ]
             </mo>
            </mrow>
           </mrow>
           <mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">
            ,
           </mo>
          </mrow>
          <annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b">
           <apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1">
            <eq id="S3.E1.m1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.5">
            </eq>
            <interval closure="closed" id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2">
             <apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">
              <csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">
               superscript
              </csymbol>
              <apply id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">
               <csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">
                subscript
               </csymbol>
               <ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2">
                ùë°
               </ci>
               <ci id="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3">
                ùëñ
               </ci>
              </apply>
              <apply id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">
               <times id="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.1">
               </times>
               <ci id="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2">
                ùë†
               </ci>
               <ci id="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3">
                ùë°
               </ci>
               <ci id="S3.E1.m1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.4">
                ùëé
               </ci>
               <ci id="S3.E1.m1.1.1.1.1.1.1.1.3.5.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.5">
                ùëü
               </ci>
               <ci id="S3.E1.m1.1.1.1.1.1.1.1.3.6.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.6">
                ùë°
               </ci>
              </apply>
             </apply>
             <apply id="S3.E1.m1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2">
              <csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2">
               superscript
              </csymbol>
              <apply id="S3.E1.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2">
               <csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2">
                subscript
               </csymbol>
               <ci id="S3.E1.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2">
                ùë°
               </ci>
               <ci id="S3.E1.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2.3">
                ùëñ
               </ci>
              </apply>
              <apply id="S3.E1.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3">
               <times id="S3.E1.m1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3.1">
               </times>
               <ci id="S3.E1.m1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3.2">
                ùëí
               </ci>
               <ci id="S3.E1.m1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3.3">
                ùëõ
               </ci>
               <ci id="S3.E1.m1.1.1.1.1.2.2.2.3.4.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3.4">
                ùëë
               </ci>
              </apply>
             </apply>
            </interval>
            <interval closure="closed" id="S3.E1.m1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2">
             <apply id="S3.E1.m1.1.1.1.1.3.1.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1">
              <minus id="S3.E1.m1.1.1.1.1.3.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.1">
              </minus>
              <apply id="S3.E1.m1.1.1.1.1.3.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.2">
               <csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.2">
                subscript
               </csymbol>
               <ci id="S3.E1.m1.1.1.1.1.3.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.2.2">
                ùë°
               </ci>
               <ci id="S3.E1.m1.1.1.1.1.3.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.2.3">
                ùëñ
               </ci>
              </apply>
              <apply id="S3.E1.m1.1.1.1.1.3.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.3">
               <times id="S3.E1.m1.1.1.1.1.3.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.3.1">
               </times>
               <apply id="S3.E1.m1.1.1.1.1.3.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.3.2">
                <divide id="S3.E1.m1.1.1.1.1.3.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.3.2.1">
                </divide>
                <apply id="S3.E1.m1.1.1.1.1.3.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.3.2.2">
                 <csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.3.2.2">
                  subscript
                 </csymbol>
                 <ci id="S3.E1.m1.1.1.1.1.3.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.3.2.2.2">
                  ùõΩ
                 </ci>
                 <ci id="S3.E1.m1.1.1.1.1.3.1.1.3.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.3.2.2.3">
                  ùëñ
                 </ci>
                </apply>
                <cn id="S3.E1.m1.1.1.1.1.3.1.1.3.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.3.1.1.3.2.3">
                 2
                </cn>
               </apply>
               <ci id="S3.E1.m1.1.1.1.1.3.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.1.1.3.3">
                ùõº
               </ci>
              </apply>
             </apply>
             <apply id="S3.E1.m1.1.1.1.1.4.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2">
              <plus id="S3.E1.m1.1.1.1.1.4.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.1">
              </plus>
              <apply id="S3.E1.m1.1.1.1.1.4.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.2">
               <csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.4.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.2">
                subscript
               </csymbol>
               <ci id="S3.E1.m1.1.1.1.1.4.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.2.2">
                ùë°
               </ci>
               <ci id="S3.E1.m1.1.1.1.1.4.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.2.3">
                ùëñ
               </ci>
              </apply>
              <apply id="S3.E1.m1.1.1.1.1.4.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3">
               <times id="S3.E1.m1.1.1.1.1.4.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.1">
               </times>
               <apply id="S3.E1.m1.1.1.1.1.4.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2">
                <divide id="S3.E1.m1.1.1.1.1.4.2.2.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2.1">
                </divide>
                <apply id="S3.E1.m1.1.1.1.1.4.2.2.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2.2">
                 <csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.4.2.2.3.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2.2">
                  subscript
                 </csymbol>
                 <ci id="S3.E1.m1.1.1.1.1.4.2.2.3.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2.2.2">
                  ùõΩ
                 </ci>
                 <ci id="S3.E1.m1.1.1.1.1.4.2.2.3.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2.2.3">
                  ùëñ
                 </ci>
                </apply>
                <cn id="S3.E1.m1.1.1.1.1.4.2.2.3.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2.3">
                 2
                </cn>
               </apply>
               <ci id="S3.E1.m1.1.1.1.1.4.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3">
                ùõº
               </ci>
              </apply>
             </apply>
            </interval>
           </apply>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.E1.m1.1c">
           [t_{i}^{start},t_{i}^{end}]=[t_{i}-\beta_{i}/2\alpha,~{}t_{i}+\beta_{i}/2\alpha],
          </annotation>
         </semantics>
        </math>
       </td>
       <td class="ltx_eqn_cell ltx_eqn_center_padright">
       </td>
       <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
        <span class="ltx_tag ltx_tag_equation ltx_align_right">
         (1)
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <p class="ltx_p" id="S3.SS4.p2.9">
     where
     <math alttext="\beta_{i}=\sum_{j=0}^{n-1}\left(t_{j+1}-t_{j}\right)/n" class="ltx_Math" display="inline" id="S3.SS4.p2.6.m1.1">
      <semantics id="S3.SS4.p2.6.m1.1a">
       <mrow id="S3.SS4.p2.6.m1.1.1" xref="S3.SS4.p2.6.m1.1.1.cmml">
        <msub id="S3.SS4.p2.6.m1.1.1.3" xref="S3.SS4.p2.6.m1.1.1.3.cmml">
         <mi id="S3.SS4.p2.6.m1.1.1.3.2" xref="S3.SS4.p2.6.m1.1.1.3.2.cmml">
          Œ≤
         </mi>
         <mi id="S3.SS4.p2.6.m1.1.1.3.3" xref="S3.SS4.p2.6.m1.1.1.3.3.cmml">
          i
         </mi>
        </msub>
        <mo id="S3.SS4.p2.6.m1.1.1.2" rspace="0.111em" xref="S3.SS4.p2.6.m1.1.1.2.cmml">
         =
        </mo>
        <mrow id="S3.SS4.p2.6.m1.1.1.1" xref="S3.SS4.p2.6.m1.1.1.1.cmml">
         <msubsup id="S3.SS4.p2.6.m1.1.1.1.2" xref="S3.SS4.p2.6.m1.1.1.1.2.cmml">
          <mo id="S3.SS4.p2.6.m1.1.1.1.2.2.2" rspace="0em" xref="S3.SS4.p2.6.m1.1.1.1.2.2.2.cmml">
           ‚àë
          </mo>
          <mrow id="S3.SS4.p2.6.m1.1.1.1.2.2.3" xref="S3.SS4.p2.6.m1.1.1.1.2.2.3.cmml">
           <mi id="S3.SS4.p2.6.m1.1.1.1.2.2.3.2" xref="S3.SS4.p2.6.m1.1.1.1.2.2.3.2.cmml">
            j
           </mi>
           <mo id="S3.SS4.p2.6.m1.1.1.1.2.2.3.1" xref="S3.SS4.p2.6.m1.1.1.1.2.2.3.1.cmml">
            =
           </mo>
           <mn id="S3.SS4.p2.6.m1.1.1.1.2.2.3.3" xref="S3.SS4.p2.6.m1.1.1.1.2.2.3.3.cmml">
            0
           </mn>
          </mrow>
          <mrow id="S3.SS4.p2.6.m1.1.1.1.2.3" xref="S3.SS4.p2.6.m1.1.1.1.2.3.cmml">
           <mi id="S3.SS4.p2.6.m1.1.1.1.2.3.2" xref="S3.SS4.p2.6.m1.1.1.1.2.3.2.cmml">
            n
           </mi>
           <mo id="S3.SS4.p2.6.m1.1.1.1.2.3.1" xref="S3.SS4.p2.6.m1.1.1.1.2.3.1.cmml">
            ‚àí
           </mo>
           <mn id="S3.SS4.p2.6.m1.1.1.1.2.3.3" xref="S3.SS4.p2.6.m1.1.1.1.2.3.3.cmml">
            1
           </mn>
          </mrow>
         </msubsup>
         <mrow id="S3.SS4.p2.6.m1.1.1.1.1" xref="S3.SS4.p2.6.m1.1.1.1.1.cmml">
          <mrow id="S3.SS4.p2.6.m1.1.1.1.1.1.1" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.cmml">
           <mo id="S3.SS4.p2.6.m1.1.1.1.1.1.1.2" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.cmml">
            (
           </mo>
           <mrow id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.cmml">
            <msub id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.cmml">
             <mi id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.2" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.2.cmml">
              t
             </mi>
             <mrow id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3.cmml">
              <mi id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3.2" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3.2.cmml">
               j
              </mi>
              <mo id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3.1" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3.1.cmml">
               +
              </mo>
              <mn id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3.3" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3.3.cmml">
               1
              </mn>
             </mrow>
            </msub>
            <mo id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.1" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.1.cmml">
             ‚àí
            </mo>
            <msub id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.3" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.3.cmml">
             <mi id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.3.2" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.3.2.cmml">
              t
             </mi>
             <mi id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.3.3" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.3.3.cmml">
              j
             </mi>
            </msub>
           </mrow>
           <mo id="S3.SS4.p2.6.m1.1.1.1.1.1.1.3" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.cmml">
            )
           </mo>
          </mrow>
          <mo id="S3.SS4.p2.6.m1.1.1.1.1.2" xref="S3.SS4.p2.6.m1.1.1.1.1.2.cmml">
           /
          </mo>
          <mi id="S3.SS4.p2.6.m1.1.1.1.1.3" xref="S3.SS4.p2.6.m1.1.1.1.1.3.cmml">
           n
          </mi>
         </mrow>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.6.m1.1b">
        <apply id="S3.SS4.p2.6.m1.1.1.cmml" xref="S3.SS4.p2.6.m1.1.1">
         <eq id="S3.SS4.p2.6.m1.1.1.2.cmml" xref="S3.SS4.p2.6.m1.1.1.2">
         </eq>
         <apply id="S3.SS4.p2.6.m1.1.1.3.cmml" xref="S3.SS4.p2.6.m1.1.1.3">
          <csymbol cd="ambiguous" id="S3.SS4.p2.6.m1.1.1.3.1.cmml" xref="S3.SS4.p2.6.m1.1.1.3">
           subscript
          </csymbol>
          <ci id="S3.SS4.p2.6.m1.1.1.3.2.cmml" xref="S3.SS4.p2.6.m1.1.1.3.2">
           ùõΩ
          </ci>
          <ci id="S3.SS4.p2.6.m1.1.1.3.3.cmml" xref="S3.SS4.p2.6.m1.1.1.3.3">
           ùëñ
          </ci>
         </apply>
         <apply id="S3.SS4.p2.6.m1.1.1.1.cmml" xref="S3.SS4.p2.6.m1.1.1.1">
          <apply id="S3.SS4.p2.6.m1.1.1.1.2.cmml" xref="S3.SS4.p2.6.m1.1.1.1.2">
           <csymbol cd="ambiguous" id="S3.SS4.p2.6.m1.1.1.1.2.1.cmml" xref="S3.SS4.p2.6.m1.1.1.1.2">
            superscript
           </csymbol>
           <apply id="S3.SS4.p2.6.m1.1.1.1.2.2.cmml" xref="S3.SS4.p2.6.m1.1.1.1.2">
            <csymbol cd="ambiguous" id="S3.SS4.p2.6.m1.1.1.1.2.2.1.cmml" xref="S3.SS4.p2.6.m1.1.1.1.2">
             subscript
            </csymbol>
            <sum id="S3.SS4.p2.6.m1.1.1.1.2.2.2.cmml" xref="S3.SS4.p2.6.m1.1.1.1.2.2.2">
            </sum>
            <apply id="S3.SS4.p2.6.m1.1.1.1.2.2.3.cmml" xref="S3.SS4.p2.6.m1.1.1.1.2.2.3">
             <eq id="S3.SS4.p2.6.m1.1.1.1.2.2.3.1.cmml" xref="S3.SS4.p2.6.m1.1.1.1.2.2.3.1">
             </eq>
             <ci id="S3.SS4.p2.6.m1.1.1.1.2.2.3.2.cmml" xref="S3.SS4.p2.6.m1.1.1.1.2.2.3.2">
              ùëó
             </ci>
             <cn id="S3.SS4.p2.6.m1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.SS4.p2.6.m1.1.1.1.2.2.3.3">
              0
             </cn>
            </apply>
           </apply>
           <apply id="S3.SS4.p2.6.m1.1.1.1.2.3.cmml" xref="S3.SS4.p2.6.m1.1.1.1.2.3">
            <minus id="S3.SS4.p2.6.m1.1.1.1.2.3.1.cmml" xref="S3.SS4.p2.6.m1.1.1.1.2.3.1">
            </minus>
            <ci id="S3.SS4.p2.6.m1.1.1.1.2.3.2.cmml" xref="S3.SS4.p2.6.m1.1.1.1.2.3.2">
             ùëõ
            </ci>
            <cn id="S3.SS4.p2.6.m1.1.1.1.2.3.3.cmml" type="integer" xref="S3.SS4.p2.6.m1.1.1.1.2.3.3">
             1
            </cn>
           </apply>
          </apply>
          <apply id="S3.SS4.p2.6.m1.1.1.1.1.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1">
           <divide id="S3.SS4.p2.6.m1.1.1.1.1.2.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1.2">
           </divide>
           <apply id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1">
            <minus id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.1">
            </minus>
            <apply id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2">
             <csymbol cd="ambiguous" id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2">
              subscript
             </csymbol>
             <ci id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.2">
              ùë°
             </ci>
             <apply id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3">
              <plus id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3.1">
              </plus>
              <ci id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3.2">
               ùëó
              </ci>
              <cn id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3.3.cmml" type="integer" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.2.3.3">
               1
              </cn>
             </apply>
            </apply>
            <apply id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.3">
             <csymbol cd="ambiguous" id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.3">
              subscript
             </csymbol>
             <ci id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.3.2">
              ùë°
             </ci>
             <ci id="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1.1.1.1.3.3">
              ùëó
             </ci>
            </apply>
           </apply>
           <ci id="S3.SS4.p2.6.m1.1.1.1.1.3.cmml" xref="S3.SS4.p2.6.m1.1.1.1.1.3">
            ùëõ
           </ci>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.6.m1.1c">
        \beta_{i}=\sum_{j=0}^{n-1}\left(t_{j+1}-t_{j}\right)/n
       </annotation>
      </semantics>
     </math>
     is an adjustable parameter equal to the average temporal distance between consecutive narrations in a given video. Conversely,
     <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS4.p2.7.m2.1">
      <semantics id="S3.SS4.p2.7.m2.1a">
       <mi id="S3.SS4.p2.7.m2.1.1" xref="S3.SS4.p2.7.m2.1.1.cmml">
        Œ±
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.7.m2.1b">
        <ci id="S3.SS4.p2.7.m2.1.1.cmml" xref="S3.SS4.p2.7.m2.1.1">
         ùõº
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.7.m2.1c">
        \alpha
       </annotation>
      </semantics>
     </math>
     is a scale factor computed as the average of all
     <math alttext="\beta_{i}" class="ltx_Math" display="inline" id="S3.SS4.p2.8.m3.1">
      <semantics id="S3.SS4.p2.8.m3.1a">
       <msub id="S3.SS4.p2.8.m3.1.1" xref="S3.SS4.p2.8.m3.1.1.cmml">
        <mi id="S3.SS4.p2.8.m3.1.1.2" xref="S3.SS4.p2.8.m3.1.1.2.cmml">
         Œ≤
        </mi>
        <mi id="S3.SS4.p2.8.m3.1.1.3" xref="S3.SS4.p2.8.m3.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.8.m3.1b">
        <apply id="S3.SS4.p2.8.m3.1.1.cmml" xref="S3.SS4.p2.8.m3.1.1">
         <csymbol cd="ambiguous" id="S3.SS4.p2.8.m3.1.1.1.cmml" xref="S3.SS4.p2.8.m3.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS4.p2.8.m3.1.1.2.cmml" xref="S3.SS4.p2.8.m3.1.1.2">
          ùõΩ
         </ci>
         <ci id="S3.SS4.p2.8.m3.1.1.3.cmml" xref="S3.SS4.p2.8.m3.1.1.3">
          ùëñ
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.8.m3.1c">
        \beta_{i}
       </annotation>
      </semantics>
     </math>
     across all videos in the EgoCOT dataset (
     <math alttext="\alpha=4.9" class="ltx_Math" display="inline" id="S3.SS4.p2.9.m4.1">
      <semantics id="S3.SS4.p2.9.m4.1a">
       <mrow id="S3.SS4.p2.9.m4.1.1" xref="S3.SS4.p2.9.m4.1.1.cmml">
        <mi id="S3.SS4.p2.9.m4.1.1.2" xref="S3.SS4.p2.9.m4.1.1.2.cmml">
         Œ±
        </mi>
        <mo id="S3.SS4.p2.9.m4.1.1.1" xref="S3.SS4.p2.9.m4.1.1.1.cmml">
         =
        </mo>
        <mn id="S3.SS4.p2.9.m4.1.1.3" xref="S3.SS4.p2.9.m4.1.1.3.cmml">
         4.9
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.9.m4.1b">
        <apply id="S3.SS4.p2.9.m4.1.1.cmml" xref="S3.SS4.p2.9.m4.1.1">
         <eq id="S3.SS4.p2.9.m4.1.1.1.cmml" xref="S3.SS4.p2.9.m4.1.1.1">
         </eq>
         <ci id="S3.SS4.p2.9.m4.1.1.2.cmml" xref="S3.SS4.p2.9.m4.1.1.2">
          ùõº
         </ci>
         <cn id="S3.SS4.p2.9.m4.1.1.3.cmml" type="float" xref="S3.SS4.p2.9.m4.1.1.3">
          4.9
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.9.m4.1c">
        \alpha=4.9
       </annotation>
      </semantics>
     </math>
     seconds).
For each video segment, we provide prompts and corresponding captions for ChatGPT
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib55" title="">
       55
      </a>
      ]
     </cite>
     to generate a reasonable and detailed embodied planning. The caption is typically a brief introduction such as "C opens a drawer."
We use the ChatGPT to generate a chain of thought according to the caption and organize it into a list of verb-noun pairs, such as
     <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS4.p2.9.1" style="color:#666666;">
      "plans: grasp the handle with the gripper and pull the handle; actions: 1. grasp(handle, gripper) 2. pull(handle)."
     </span>
     The prompt we used to generate EgoCOT dataset is shown in Listing
     <a class="ltx_ref" href="#LST2" title="Listing 2 ‚Ä£ 3.4 Creating EgoCOT and EgoVQA Dataset ‚Ä£ 3 Method ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     . To enhance the diversity of generated chain of thoughts, we employ a temperature parameter of 0.9 and a top-p parameter of 0.95. For each prompt, we perform five sampling iterations.
    </p>
   </div>
   <figure class="ltx_float ltx_lstlisting" id="LST2">
    <div class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="LST2.1">
     <div class="ltx_listing_data">
      <a download="" href="data:text/plain;base64,WW91IG5lZWQgdG8gZ2VuZXJhdGUgcGxhbnMgd2l0aCBjaGFpbiBvZiB0aG91Z2h0IGZvciBlYWNoIHRhc2ssIGFuZCB0aGVuIGV4dHJhY3QgIGRldGFpbGVkIGFjdGlvbnMgKGNvbGxvY2F0aW9uIG9mIG5vdW5zIGFuZCB2ZXJicykgZnJvbSB0aGUgcGxhbi4KVGhlIGFjdGlvbiBjYW4gYmUgb2YgdGhlIGZvbGxvd2luZyBmb3JtOgpbYWN0aW9uX25hbWVdLCBlZy4sIHR1cm4gbGVmdDsKW2FjdGlvbl9uYW1lXSBhcmd1bWVudDEsIGVnLiwgcGljayB1cChhcHBsZSk7ClthY3Rpb25fbmFtZV0gYXJndW1lbnQxIGFyZ3VtZW50MiwgZWcuLCBwdXQoYXBwbGUsIHRhYmxlKQpUYXNrOiBwaWNrIHVwIGEgY3VwIG9uIHRoZSB0YWJsZQpwbGFuczogZ3Jhc3AgdGhlIGhhbmRsZSBvZiB0aGUgY3VwIHdpdGggdGhlIGdyaXBwZXIgYW5kIGxpZnQgaXQgdXAKQWN0aW9uczoKMS4gZ3Jhc3AoaGFuZGxlIG9mIHRoZSBjdXAsIGdyaXBwZXIpCjIuIGxpZnQgdXAoY3VwKQ==">
       ‚¨á
      </a>
     </div>
     <div class="ltx_listingline" id="lstnumberx4">
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.1" style="font-size:70%;">
       You
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.2" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.3" style="font-size:70%;">
       need
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.4" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.5" style="font-size:70%;">
       to
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.6" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.7" style="font-size:70%;">
       generate
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.8" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.9" style="font-size:70%;">
       plans
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.10" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.11" style="font-size:70%;">
       with
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.12" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.13" style="font-size:70%;">
       chain
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.14" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.15" style="font-size:70%;">
       of
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.16" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.17" style="font-size:70%;">
       thought
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.18" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx4.19" style="font-size:70%;">
       for
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.20" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.21" style="font-size:70%;">
       each
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.22" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.23" style="font-size:70%;">
       task
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx4.24" style="font-size:70%;">
       ,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.25" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx4.26" style="font-size:70%;">
       and
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.27" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.28" style="font-size:70%;">
       then
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.29" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.30" style="font-size:70%;">
       extract
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.31" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.32" style="font-size:70%;">
       detailed
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.33" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.34" style="font-size:70%;">
       actions
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.35" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx4.36" style="font-size:70%;">
       (
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.37" style="font-size:70%;">
       collocation
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.38" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.39" style="font-size:70%;">
       of
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.40" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.41" style="font-size:70%;">
       nouns
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.42" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx4.43" style="font-size:70%;">
       and
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.44" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.45" style="font-size:70%;">
       verbs
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx4.46" style="font-size:70%;">
       )
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.47" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx4.48" style="font-size:70%;">
       from
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.49" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.50" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.51" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.52" style="font-size:70%;">
       plan
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx4.53" style="font-size:70%;">
       .
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx5">
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.1" style="font-size:70%;">
       The
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.2" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.3" style="font-size:70%;">
       action
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.4" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.5" style="font-size:70%;">
       can
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.6" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.7" style="font-size:70%;">
       be
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.8" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.9" style="font-size:70%;">
       of
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.10" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.11" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.12" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.13" style="font-size:70%;">
       following
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.14" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.15" style="font-size:70%;">
       form
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx5.16" style="font-size:70%;">
       :
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx6">
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx6.1" style="font-size:70%;">
       [
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.2" style="font-size:70%;">
       action_name
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx6.3" style="font-size:70%;">
       ],
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.4" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.5" style="font-size:70%;">
       eg
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx6.6" style="font-size:70%;">
       .,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.7" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.8" style="font-size:70%;">
       turn
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.9" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.10" style="font-size:70%;">
       left
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx6.11" style="font-size:70%;">
       ;
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx7">
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx7.1" style="font-size:70%;">
       [
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.2" style="font-size:70%;">
       action_name
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx7.3" style="font-size:70%;">
       ]
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.4" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.5" style="font-size:70%;">
       argument1
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx7.6" style="font-size:70%;">
       ,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.7" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.8" style="font-size:70%;">
       eg
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx7.9" style="font-size:70%;">
       .,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.10" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.11" style="font-size:70%;">
       pick
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.12" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.13" style="font-size:70%;">
       up
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx7.14" style="font-size:70%;">
       (
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.15" style="font-size:70%;">
       apple
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx7.16" style="font-size:70%;">
       );
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx8">
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx8.1" style="font-size:70%;">
       [
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.2" style="font-size:70%;">
       action_name
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx8.3" style="font-size:70%;">
       ]
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.4" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.5" style="font-size:70%;">
       argument1
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.6" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.7" style="font-size:70%;">
       argument2
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx8.8" style="font-size:70%;">
       ,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.9" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.10" style="font-size:70%;">
       eg
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx8.11" style="font-size:70%;">
       .,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.12" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.13" style="font-size:70%;">
       put
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx8.14" style="font-size:70%;">
       (
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.15" style="font-size:70%;">
       apple
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx8.16" style="font-size:70%;">
       ,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.17" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.18" style="font-size:70%;">
       table
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx8.19" style="font-size:70%;">
       )
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx9">
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.1" style="font-size:70%;">
       Task
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx9.2" style="font-size:70%;">
       :
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.3" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.4" style="font-size:70%;">
       pick
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.5" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.6" style="font-size:70%;">
       up
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.7" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.8" style="font-size:70%;">
       a
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.9" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.10" style="font-size:70%;">
       cup
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.11" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.12" style="font-size:70%;">
       on
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.13" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.14" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.15" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.16" style="font-size:70%;">
       table
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx10">
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.1" style="font-size:70%;">
       plans
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx10.2" style="font-size:70%;">
       :
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.3" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.4" style="font-size:70%;">
       grasp
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.5" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.6" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.7" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.8" style="font-size:70%;">
       handle
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.9" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.10" style="font-size:70%;">
       of
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.11" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.12" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.13" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.14" style="font-size:70%;">
       cup
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.15" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.16" style="font-size:70%;">
       with
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.17" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.18" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.19" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.20" style="font-size:70%;">
       gripper
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.21" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx10.22" style="font-size:70%;">
       and
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.23" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.24" style="font-size:70%;">
       lift
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.25" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.26" style="font-size:70%;">
       it
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.27" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.28" style="font-size:70%;">
       up
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx11">
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.1" style="font-size:70%;">
       Actions
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx11.2" style="font-size:70%;">
       :
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx12">
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.1" style="font-size:70%;">
       1.
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.2" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.3" style="font-size:70%;">
       grasp
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.4" style="font-size:70%;">
       (
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.5" style="font-size:70%;">
       handle
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.6" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.7" style="font-size:70%;">
       of
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.8" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.9" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.10" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.11" style="font-size:70%;">
       cup
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.12" style="font-size:70%;">
       ,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.13" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.14" style="font-size:70%;">
       gripper
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.15" style="font-size:70%;">
       )
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx13">
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx13.1" style="font-size:70%;">
       2.
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.2" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.3" style="font-size:70%;">
       lift
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.4" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.5" style="font-size:70%;">
       up
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx13.6" style="font-size:70%;">
       (
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.7" style="font-size:70%;">
       cup
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx13.8" style="font-size:70%;">
       )
      </span>
     </div>
    </div>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_float">
      Listing¬†2:
     </span>
     Prompt we used for creating EgoCOT dataset.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS4.p3">
    <p class="ltx_p" id="S3.SS4.p3.9">
     <span class="ltx_text ltx_font_bold" id="S3.SS4.p3.9.1">
      Post-procedure.
     </span>
     To ensure the quality of the generated planning instructions, we perform the second stage of data cleaning. We used the CLIP model
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib56" title="">
       56
      </a>
      ]
     </cite>
     to assess the similarities between the video and text pairs. For each video, we compared it against five potential embodied plans and selected the one with the highest similarity as the corresponding label for the embodied plan.
We then took our data-cleaning process a step further by filtering out any video-caption-planning pairs with similarities lower than the threshold. We eliminated both data with the low similarity between the video and caption and between the video and planning to ensure the highest quality data for our EgoCOT dataset. For each keyframe of the video segment, we use the CLIP model to encode both the text data
     <math alttext="T" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1">
      <semantics id="S3.SS4.p3.1.m1.1a">
       <mi id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">
        T
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b">
        <ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">
         ùëá
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">
        T
       </annotation>
      </semantics>
     </math>
     and the image data
     <math alttext="I" class="ltx_Math" display="inline" id="S3.SS4.p3.2.m2.1">
      <semantics id="S3.SS4.p3.2.m2.1a">
       <mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">
        I
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b">
        <ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">
         ùêº
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">
        I
       </annotation>
      </semantics>
     </math>
     into a shared embedding space. The similarity is calculated using the cosine similarity function as
     <math alttext="S(y_{T},y_{I})=\frac{y_{T}\cdot y_{I}}{\|y_{T}\|\|y_{I}\|}" class="ltx_Math" display="inline" id="S3.SS4.p3.3.m3.4">
      <semantics id="S3.SS4.p3.3.m3.4a">
       <mrow id="S3.SS4.p3.3.m3.4.4" xref="S3.SS4.p3.3.m3.4.4.cmml">
        <mrow id="S3.SS4.p3.3.m3.4.4.2" xref="S3.SS4.p3.3.m3.4.4.2.cmml">
         <mi id="S3.SS4.p3.3.m3.4.4.2.4" xref="S3.SS4.p3.3.m3.4.4.2.4.cmml">
          S
         </mi>
         <mo id="S3.SS4.p3.3.m3.4.4.2.3" lspace="0em" rspace="0em" xref="S3.SS4.p3.3.m3.4.4.2.3.cmml">
          ‚Äã
         </mo>
         <mrow id="S3.SS4.p3.3.m3.4.4.2.2.2" xref="S3.SS4.p3.3.m3.4.4.2.2.3.cmml">
          <mo id="S3.SS4.p3.3.m3.4.4.2.2.2.3" stretchy="false" xref="S3.SS4.p3.3.m3.4.4.2.2.3.cmml">
           (
          </mo>
          <msub id="S3.SS4.p3.3.m3.3.3.1.1.1.1" xref="S3.SS4.p3.3.m3.3.3.1.1.1.1.cmml">
           <mi id="S3.SS4.p3.3.m3.3.3.1.1.1.1.2" xref="S3.SS4.p3.3.m3.3.3.1.1.1.1.2.cmml">
            y
           </mi>
           <mi id="S3.SS4.p3.3.m3.3.3.1.1.1.1.3" xref="S3.SS4.p3.3.m3.3.3.1.1.1.1.3.cmml">
            T
           </mi>
          </msub>
          <mo id="S3.SS4.p3.3.m3.4.4.2.2.2.4" xref="S3.SS4.p3.3.m3.4.4.2.2.3.cmml">
           ,
          </mo>
          <msub id="S3.SS4.p3.3.m3.4.4.2.2.2.2" xref="S3.SS4.p3.3.m3.4.4.2.2.2.2.cmml">
           <mi id="S3.SS4.p3.3.m3.4.4.2.2.2.2.2" xref="S3.SS4.p3.3.m3.4.4.2.2.2.2.2.cmml">
            y
           </mi>
           <mi id="S3.SS4.p3.3.m3.4.4.2.2.2.2.3" xref="S3.SS4.p3.3.m3.4.4.2.2.2.2.3.cmml">
            I
           </mi>
          </msub>
          <mo id="S3.SS4.p3.3.m3.4.4.2.2.2.5" stretchy="false" xref="S3.SS4.p3.3.m3.4.4.2.2.3.cmml">
           )
          </mo>
         </mrow>
        </mrow>
        <mo id="S3.SS4.p3.3.m3.4.4.3" xref="S3.SS4.p3.3.m3.4.4.3.cmml">
         =
        </mo>
        <mfrac id="S3.SS4.p3.3.m3.2.2" xref="S3.SS4.p3.3.m3.2.2.cmml">
         <mrow id="S3.SS4.p3.3.m3.2.2.4" xref="S3.SS4.p3.3.m3.2.2.4.cmml">
          <msub id="S3.SS4.p3.3.m3.2.2.4.2" xref="S3.SS4.p3.3.m3.2.2.4.2.cmml">
           <mi id="S3.SS4.p3.3.m3.2.2.4.2.2" xref="S3.SS4.p3.3.m3.2.2.4.2.2.cmml">
            y
           </mi>
           <mi id="S3.SS4.p3.3.m3.2.2.4.2.3" xref="S3.SS4.p3.3.m3.2.2.4.2.3.cmml">
            T
           </mi>
          </msub>
          <mo id="S3.SS4.p3.3.m3.2.2.4.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p3.3.m3.2.2.4.1.cmml">
           ‚ãÖ
          </mo>
          <msub id="S3.SS4.p3.3.m3.2.2.4.3" xref="S3.SS4.p3.3.m3.2.2.4.3.cmml">
           <mi id="S3.SS4.p3.3.m3.2.2.4.3.2" xref="S3.SS4.p3.3.m3.2.2.4.3.2.cmml">
            y
           </mi>
           <mi id="S3.SS4.p3.3.m3.2.2.4.3.3" xref="S3.SS4.p3.3.m3.2.2.4.3.3.cmml">
            I
           </mi>
          </msub>
         </mrow>
         <mrow id="S3.SS4.p3.3.m3.2.2.2" xref="S3.SS4.p3.3.m3.2.2.2.cmml">
          <mrow id="S3.SS4.p3.3.m3.1.1.1.1.1" xref="S3.SS4.p3.3.m3.1.1.1.1.2.cmml">
           <mo id="S3.SS4.p3.3.m3.1.1.1.1.1.2" stretchy="false" xref="S3.SS4.p3.3.m3.1.1.1.1.2.1.cmml">
            ‚Äñ
           </mo>
           <msub id="S3.SS4.p3.3.m3.1.1.1.1.1.1" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.cmml">
            <mi id="S3.SS4.p3.3.m3.1.1.1.1.1.1.2" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.2.cmml">
             y
            </mi>
            <mi id="S3.SS4.p3.3.m3.1.1.1.1.1.1.3" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.3.cmml">
             T
            </mi>
           </msub>
           <mo id="S3.SS4.p3.3.m3.1.1.1.1.1.3" stretchy="false" xref="S3.SS4.p3.3.m3.1.1.1.1.2.1.cmml">
            ‚Äñ
           </mo>
          </mrow>
          <mo id="S3.SS4.p3.3.m3.2.2.2.3" lspace="0em" rspace="0em" xref="S3.SS4.p3.3.m3.2.2.2.3.cmml">
           ‚Äã
          </mo>
          <mrow id="S3.SS4.p3.3.m3.2.2.2.2.1" xref="S3.SS4.p3.3.m3.2.2.2.2.2.cmml">
           <mo id="S3.SS4.p3.3.m3.2.2.2.2.1.2" stretchy="false" xref="S3.SS4.p3.3.m3.2.2.2.2.2.1.cmml">
            ‚Äñ
           </mo>
           <msub id="S3.SS4.p3.3.m3.2.2.2.2.1.1" xref="S3.SS4.p3.3.m3.2.2.2.2.1.1.cmml">
            <mi id="S3.SS4.p3.3.m3.2.2.2.2.1.1.2" xref="S3.SS4.p3.3.m3.2.2.2.2.1.1.2.cmml">
             y
            </mi>
            <mi id="S3.SS4.p3.3.m3.2.2.2.2.1.1.3" xref="S3.SS4.p3.3.m3.2.2.2.2.1.1.3.cmml">
             I
            </mi>
           </msub>
           <mo id="S3.SS4.p3.3.m3.2.2.2.2.1.3" stretchy="false" xref="S3.SS4.p3.3.m3.2.2.2.2.2.1.cmml">
            ‚Äñ
           </mo>
          </mrow>
         </mrow>
        </mfrac>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.4b">
        <apply id="S3.SS4.p3.3.m3.4.4.cmml" xref="S3.SS4.p3.3.m3.4.4">
         <eq id="S3.SS4.p3.3.m3.4.4.3.cmml" xref="S3.SS4.p3.3.m3.4.4.3">
         </eq>
         <apply id="S3.SS4.p3.3.m3.4.4.2.cmml" xref="S3.SS4.p3.3.m3.4.4.2">
          <times id="S3.SS4.p3.3.m3.4.4.2.3.cmml" xref="S3.SS4.p3.3.m3.4.4.2.3">
          </times>
          <ci id="S3.SS4.p3.3.m3.4.4.2.4.cmml" xref="S3.SS4.p3.3.m3.4.4.2.4">
           ùëÜ
          </ci>
          <interval closure="open" id="S3.SS4.p3.3.m3.4.4.2.2.3.cmml" xref="S3.SS4.p3.3.m3.4.4.2.2.2">
           <apply id="S3.SS4.p3.3.m3.3.3.1.1.1.1.cmml" xref="S3.SS4.p3.3.m3.3.3.1.1.1.1">
            <csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.3.3.1.1.1.1.1.cmml" xref="S3.SS4.p3.3.m3.3.3.1.1.1.1">
             subscript
            </csymbol>
            <ci id="S3.SS4.p3.3.m3.3.3.1.1.1.1.2.cmml" xref="S3.SS4.p3.3.m3.3.3.1.1.1.1.2">
             ùë¶
            </ci>
            <ci id="S3.SS4.p3.3.m3.3.3.1.1.1.1.3.cmml" xref="S3.SS4.p3.3.m3.3.3.1.1.1.1.3">
             ùëá
            </ci>
           </apply>
           <apply id="S3.SS4.p3.3.m3.4.4.2.2.2.2.cmml" xref="S3.SS4.p3.3.m3.4.4.2.2.2.2">
            <csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.4.4.2.2.2.2.1.cmml" xref="S3.SS4.p3.3.m3.4.4.2.2.2.2">
             subscript
            </csymbol>
            <ci id="S3.SS4.p3.3.m3.4.4.2.2.2.2.2.cmml" xref="S3.SS4.p3.3.m3.4.4.2.2.2.2.2">
             ùë¶
            </ci>
            <ci id="S3.SS4.p3.3.m3.4.4.2.2.2.2.3.cmml" xref="S3.SS4.p3.3.m3.4.4.2.2.2.2.3">
             ùêº
            </ci>
           </apply>
          </interval>
         </apply>
         <apply id="S3.SS4.p3.3.m3.2.2.cmml" xref="S3.SS4.p3.3.m3.2.2">
          <divide id="S3.SS4.p3.3.m3.2.2.3.cmml" xref="S3.SS4.p3.3.m3.2.2">
          </divide>
          <apply id="S3.SS4.p3.3.m3.2.2.4.cmml" xref="S3.SS4.p3.3.m3.2.2.4">
           <ci id="S3.SS4.p3.3.m3.2.2.4.1.cmml" xref="S3.SS4.p3.3.m3.2.2.4.1">
            ‚ãÖ
           </ci>
           <apply id="S3.SS4.p3.3.m3.2.2.4.2.cmml" xref="S3.SS4.p3.3.m3.2.2.4.2">
            <csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.2.2.4.2.1.cmml" xref="S3.SS4.p3.3.m3.2.2.4.2">
             subscript
            </csymbol>
            <ci id="S3.SS4.p3.3.m3.2.2.4.2.2.cmml" xref="S3.SS4.p3.3.m3.2.2.4.2.2">
             ùë¶
            </ci>
            <ci id="S3.SS4.p3.3.m3.2.2.4.2.3.cmml" xref="S3.SS4.p3.3.m3.2.2.4.2.3">
             ùëá
            </ci>
           </apply>
           <apply id="S3.SS4.p3.3.m3.2.2.4.3.cmml" xref="S3.SS4.p3.3.m3.2.2.4.3">
            <csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.2.2.4.3.1.cmml" xref="S3.SS4.p3.3.m3.2.2.4.3">
             subscript
            </csymbol>
            <ci id="S3.SS4.p3.3.m3.2.2.4.3.2.cmml" xref="S3.SS4.p3.3.m3.2.2.4.3.2">
             ùë¶
            </ci>
            <ci id="S3.SS4.p3.3.m3.2.2.4.3.3.cmml" xref="S3.SS4.p3.3.m3.2.2.4.3.3">
             ùêº
            </ci>
           </apply>
          </apply>
          <apply id="S3.SS4.p3.3.m3.2.2.2.cmml" xref="S3.SS4.p3.3.m3.2.2.2">
           <times id="S3.SS4.p3.3.m3.2.2.2.3.cmml" xref="S3.SS4.p3.3.m3.2.2.2.3">
           </times>
           <apply id="S3.SS4.p3.3.m3.1.1.1.1.2.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1">
            <csymbol cd="latexml" id="S3.SS4.p3.3.m3.1.1.1.1.2.1.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2">
             norm
            </csymbol>
            <apply id="S3.SS4.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1">
             <csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1">
              subscript
             </csymbol>
             <ci id="S3.SS4.p3.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.2">
              ùë¶
             </ci>
             <ci id="S3.SS4.p3.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.3">
              ùëá
             </ci>
            </apply>
           </apply>
           <apply id="S3.SS4.p3.3.m3.2.2.2.2.2.cmml" xref="S3.SS4.p3.3.m3.2.2.2.2.1">
            <csymbol cd="latexml" id="S3.SS4.p3.3.m3.2.2.2.2.2.1.cmml" xref="S3.SS4.p3.3.m3.2.2.2.2.1.2">
             norm
            </csymbol>
            <apply id="S3.SS4.p3.3.m3.2.2.2.2.1.1.cmml" xref="S3.SS4.p3.3.m3.2.2.2.2.1.1">
             <csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.2.2.2.2.1.1.1.cmml" xref="S3.SS4.p3.3.m3.2.2.2.2.1.1">
              subscript
             </csymbol>
             <ci id="S3.SS4.p3.3.m3.2.2.2.2.1.1.2.cmml" xref="S3.SS4.p3.3.m3.2.2.2.2.1.1.2">
              ùë¶
             </ci>
             <ci id="S3.SS4.p3.3.m3.2.2.2.2.1.1.3.cmml" xref="S3.SS4.p3.3.m3.2.2.2.2.1.1.3">
              ùêº
             </ci>
            </apply>
           </apply>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.4c">
        S(y_{T},y_{I})=\frac{y_{T}\cdot y_{I}}{\|y_{T}\|\|y_{I}\|}
       </annotation>
      </semantics>
     </math>
     , where
     <math alttext="S(y_{T},y_{I})" class="ltx_Math" display="inline" id="S3.SS4.p3.4.m4.2">
      <semantics id="S3.SS4.p3.4.m4.2a">
       <mrow id="S3.SS4.p3.4.m4.2.2" xref="S3.SS4.p3.4.m4.2.2.cmml">
        <mi id="S3.SS4.p3.4.m4.2.2.4" xref="S3.SS4.p3.4.m4.2.2.4.cmml">
         S
        </mi>
        <mo id="S3.SS4.p3.4.m4.2.2.3" lspace="0em" rspace="0em" xref="S3.SS4.p3.4.m4.2.2.3.cmml">
         ‚Äã
        </mo>
        <mrow id="S3.SS4.p3.4.m4.2.2.2.2" xref="S3.SS4.p3.4.m4.2.2.2.3.cmml">
         <mo id="S3.SS4.p3.4.m4.2.2.2.2.3" stretchy="false" xref="S3.SS4.p3.4.m4.2.2.2.3.cmml">
          (
         </mo>
         <msub id="S3.SS4.p3.4.m4.1.1.1.1.1" xref="S3.SS4.p3.4.m4.1.1.1.1.1.cmml">
          <mi id="S3.SS4.p3.4.m4.1.1.1.1.1.2" xref="S3.SS4.p3.4.m4.1.1.1.1.1.2.cmml">
           y
          </mi>
          <mi id="S3.SS4.p3.4.m4.1.1.1.1.1.3" xref="S3.SS4.p3.4.m4.1.1.1.1.1.3.cmml">
           T
          </mi>
         </msub>
         <mo id="S3.SS4.p3.4.m4.2.2.2.2.4" xref="S3.SS4.p3.4.m4.2.2.2.3.cmml">
          ,
         </mo>
         <msub id="S3.SS4.p3.4.m4.2.2.2.2.2" xref="S3.SS4.p3.4.m4.2.2.2.2.2.cmml">
          <mi id="S3.SS4.p3.4.m4.2.2.2.2.2.2" xref="S3.SS4.p3.4.m4.2.2.2.2.2.2.cmml">
           y
          </mi>
          <mi id="S3.SS4.p3.4.m4.2.2.2.2.2.3" xref="S3.SS4.p3.4.m4.2.2.2.2.2.3.cmml">
           I
          </mi>
         </msub>
         <mo id="S3.SS4.p3.4.m4.2.2.2.2.5" stretchy="false" xref="S3.SS4.p3.4.m4.2.2.2.3.cmml">
          )
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.2b">
        <apply id="S3.SS4.p3.4.m4.2.2.cmml" xref="S3.SS4.p3.4.m4.2.2">
         <times id="S3.SS4.p3.4.m4.2.2.3.cmml" xref="S3.SS4.p3.4.m4.2.2.3">
         </times>
         <ci id="S3.SS4.p3.4.m4.2.2.4.cmml" xref="S3.SS4.p3.4.m4.2.2.4">
          ùëÜ
         </ci>
         <interval closure="open" id="S3.SS4.p3.4.m4.2.2.2.3.cmml" xref="S3.SS4.p3.4.m4.2.2.2.2">
          <apply id="S3.SS4.p3.4.m4.1.1.1.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1.1.1.1">
           <csymbol cd="ambiguous" id="S3.SS4.p3.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1.1.1.1">
            subscript
           </csymbol>
           <ci id="S3.SS4.p3.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS4.p3.4.m4.1.1.1.1.1.2">
            ùë¶
           </ci>
           <ci id="S3.SS4.p3.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS4.p3.4.m4.1.1.1.1.1.3">
            ùëá
           </ci>
          </apply>
          <apply id="S3.SS4.p3.4.m4.2.2.2.2.2.cmml" xref="S3.SS4.p3.4.m4.2.2.2.2.2">
           <csymbol cd="ambiguous" id="S3.SS4.p3.4.m4.2.2.2.2.2.1.cmml" xref="S3.SS4.p3.4.m4.2.2.2.2.2">
            subscript
           </csymbol>
           <ci id="S3.SS4.p3.4.m4.2.2.2.2.2.2.cmml" xref="S3.SS4.p3.4.m4.2.2.2.2.2.2">
            ùë¶
           </ci>
           <ci id="S3.SS4.p3.4.m4.2.2.2.2.2.3.cmml" xref="S3.SS4.p3.4.m4.2.2.2.2.2.3">
            ùêº
           </ci>
          </apply>
         </interval>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.2c">
        S(y_{T},y_{I})
       </annotation>
      </semantics>
     </math>
     denotes the similarity between the text and image, and
     <math alttext="y_{T}" class="ltx_Math" display="inline" id="S3.SS4.p3.5.m5.1">
      <semantics id="S3.SS4.p3.5.m5.1a">
       <msub id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">
        <mi id="S3.SS4.p3.5.m5.1.1.2" xref="S3.SS4.p3.5.m5.1.1.2.cmml">
         y
        </mi>
        <mi id="S3.SS4.p3.5.m5.1.1.3" xref="S3.SS4.p3.5.m5.1.1.3.cmml">
         T
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b">
        <apply id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">
         <csymbol cd="ambiguous" id="S3.SS4.p3.5.m5.1.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS4.p3.5.m5.1.1.2.cmml" xref="S3.SS4.p3.5.m5.1.1.2">
          ùë¶
         </ci>
         <ci id="S3.SS4.p3.5.m5.1.1.3.cmml" xref="S3.SS4.p3.5.m5.1.1.3">
          ùëá
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">
        y_{T}
       </annotation>
      </semantics>
     </math>
     and
     <math alttext="y_{I}" class="ltx_Math" display="inline" id="S3.SS4.p3.6.m6.1">
      <semantics id="S3.SS4.p3.6.m6.1a">
       <msub id="S3.SS4.p3.6.m6.1.1" xref="S3.SS4.p3.6.m6.1.1.cmml">
        <mi id="S3.SS4.p3.6.m6.1.1.2" xref="S3.SS4.p3.6.m6.1.1.2.cmml">
         y
        </mi>
        <mi id="S3.SS4.p3.6.m6.1.1.3" xref="S3.SS4.p3.6.m6.1.1.3.cmml">
         I
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m6.1b">
        <apply id="S3.SS4.p3.6.m6.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1">
         <csymbol cd="ambiguous" id="S3.SS4.p3.6.m6.1.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS4.p3.6.m6.1.1.2.cmml" xref="S3.SS4.p3.6.m6.1.1.2">
          ùë¶
         </ci>
         <ci id="S3.SS4.p3.6.m6.1.1.3.cmml" xref="S3.SS4.p3.6.m6.1.1.3">
          ùêº
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p3.6.m6.1c">
        y_{I}
       </annotation>
      </semantics>
     </math>
     are the respective embeddings.
Given that each video contains multiple keyframes, an ensemble of similarity scores is obtained for each video. This ensemble strategy helps to alleviate the problem of variability among individual frames and ensures a more robust and representative measure of overall similarity. The ensemble similarity score between a video
     <math alttext="V" class="ltx_Math" display="inline" id="S3.SS4.p3.7.m7.1">
      <semantics id="S3.SS4.p3.7.m7.1a">
       <mi id="S3.SS4.p3.7.m7.1.1" xref="S3.SS4.p3.7.m7.1.1.cmml">
        V
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p3.7.m7.1b">
        <ci id="S3.SS4.p3.7.m7.1.1.cmml" xref="S3.SS4.p3.7.m7.1.1">
         ùëâ
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p3.7.m7.1c">
        V
       </annotation>
      </semantics>
     </math>
     with
     <math alttext="n" class="ltx_Math" display="inline" id="S3.SS4.p3.8.m8.1">
      <semantics id="S3.SS4.p3.8.m8.1a">
       <mi id="S3.SS4.p3.8.m8.1.1" xref="S3.SS4.p3.8.m8.1.1.cmml">
        n
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p3.8.m8.1b">
        <ci id="S3.SS4.p3.8.m8.1.1.cmml" xref="S3.SS4.p3.8.m8.1.1">
         ùëõ
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p3.8.m8.1c">
        n
       </annotation>
      </semantics>
     </math>
     keyframes and text data
     <math alttext="T" class="ltx_Math" display="inline" id="S3.SS4.p3.9.m9.1">
      <semantics id="S3.SS4.p3.9.m9.1a">
       <mi id="S3.SS4.p3.9.m9.1.1" xref="S3.SS4.p3.9.m9.1.1.cmml">
        T
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p3.9.m9.1b">
        <ci id="S3.SS4.p3.9.m9.1.1.cmml" xref="S3.SS4.p3.9.m9.1.1">
         ùëá
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p3.9.m9.1c">
        T
       </annotation>
      </semantics>
     </math>
     is given by:
    </p>
    <table class="ltx_equation ltx_eqn_table" id="S3.E2">
     <tbody>
      <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
       <td class="ltx_eqn_cell ltx_eqn_center_padleft">
       </td>
       <td class="ltx_eqn_cell ltx_align_center">
        <math alttext="E(V,T)=\frac{1}{n}\sum_{i=1}^{n}S({y_{T}}_{i},{y_{I}}_{i})" class="ltx_Math" display="block" id="S3.E2.m1.4">
         <semantics id="S3.E2.m1.4a">
          <mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">
           <mrow id="S3.E2.m1.4.4.4" xref="S3.E2.m1.4.4.4.cmml">
            <mi id="S3.E2.m1.4.4.4.2" xref="S3.E2.m1.4.4.4.2.cmml">
             E
            </mi>
            <mo id="S3.E2.m1.4.4.4.1" lspace="0em" rspace="0em" xref="S3.E2.m1.4.4.4.1.cmml">
             ‚Äã
            </mo>
            <mrow id="S3.E2.m1.4.4.4.3.2" xref="S3.E2.m1.4.4.4.3.1.cmml">
             <mo id="S3.E2.m1.4.4.4.3.2.1" stretchy="false" xref="S3.E2.m1.4.4.4.3.1.cmml">
              (
             </mo>
             <mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">
              V
             </mi>
             <mo id="S3.E2.m1.4.4.4.3.2.2" xref="S3.E2.m1.4.4.4.3.1.cmml">
              ,
             </mo>
             <mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">
              T
             </mi>
             <mo id="S3.E2.m1.4.4.4.3.2.3" stretchy="false" xref="S3.E2.m1.4.4.4.3.1.cmml">
              )
             </mo>
            </mrow>
           </mrow>
           <mo id="S3.E2.m1.4.4.3" xref="S3.E2.m1.4.4.3.cmml">
            =
           </mo>
           <mrow id="S3.E2.m1.4.4.2" xref="S3.E2.m1.4.4.2.cmml">
            <mfrac id="S3.E2.m1.4.4.2.4" xref="S3.E2.m1.4.4.2.4.cmml">
             <mn id="S3.E2.m1.4.4.2.4.2" xref="S3.E2.m1.4.4.2.4.2.cmml">
              1
             </mn>
             <mi id="S3.E2.m1.4.4.2.4.3" xref="S3.E2.m1.4.4.2.4.3.cmml">
              n
             </mi>
            </mfrac>
            <mo id="S3.E2.m1.4.4.2.3" lspace="0em" rspace="0em" xref="S3.E2.m1.4.4.2.3.cmml">
             ‚Äã
            </mo>
            <mrow id="S3.E2.m1.4.4.2.2" xref="S3.E2.m1.4.4.2.2.cmml">
             <munderover id="S3.E2.m1.4.4.2.2.3" xref="S3.E2.m1.4.4.2.2.3.cmml">
              <mo id="S3.E2.m1.4.4.2.2.3.2.2" movablelimits="false" xref="S3.E2.m1.4.4.2.2.3.2.2.cmml">
               ‚àë
              </mo>
              <mrow id="S3.E2.m1.4.4.2.2.3.2.3" xref="S3.E2.m1.4.4.2.2.3.2.3.cmml">
               <mi id="S3.E2.m1.4.4.2.2.3.2.3.2" xref="S3.E2.m1.4.4.2.2.3.2.3.2.cmml">
                i
               </mi>
               <mo id="S3.E2.m1.4.4.2.2.3.2.3.1" xref="S3.E2.m1.4.4.2.2.3.2.3.1.cmml">
                =
               </mo>
               <mn id="S3.E2.m1.4.4.2.2.3.2.3.3" xref="S3.E2.m1.4.4.2.2.3.2.3.3.cmml">
                1
               </mn>
              </mrow>
              <mi id="S3.E2.m1.4.4.2.2.3.3" xref="S3.E2.m1.4.4.2.2.3.3.cmml">
               n
              </mi>
             </munderover>
             <mrow id="S3.E2.m1.4.4.2.2.2" xref="S3.E2.m1.4.4.2.2.2.cmml">
              <mi id="S3.E2.m1.4.4.2.2.2.4" xref="S3.E2.m1.4.4.2.2.2.4.cmml">
               S
              </mi>
              <mo id="S3.E2.m1.4.4.2.2.2.3" lspace="0em" rspace="0em" xref="S3.E2.m1.4.4.2.2.2.3.cmml">
               ‚Äã
              </mo>
              <mrow id="S3.E2.m1.4.4.2.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.3.cmml">
               <mo id="S3.E2.m1.4.4.2.2.2.2.2.3" stretchy="false" xref="S3.E2.m1.4.4.2.2.2.2.3.cmml">
                (
               </mo>
               <mmultiscripts id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">
                <mi id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.cmml">
                 y
                </mi>
                <mi id="S3.E2.m1.3.3.1.1.1.1.1.1.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.3.cmml">
                 T
                </mi>
                <mrow id="S3.E2.m1.3.3.1.1.1.1.1.1a" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">
                </mrow>
                <mi id="S3.E2.m1.3.3.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.cmml">
                 i
                </mi>
                <mrow id="S3.E2.m1.3.3.1.1.1.1.1.1b" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">
                </mrow>
               </mmultiscripts>
               <mo id="S3.E2.m1.4.4.2.2.2.2.2.4" xref="S3.E2.m1.4.4.2.2.2.2.3.cmml">
                ,
               </mo>
               <mmultiscripts id="S3.E2.m1.4.4.2.2.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.2.2.cmml">
                <mi id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.cmml">
                 y
                </mi>
                <mi id="S3.E2.m1.4.4.2.2.2.2.2.2.2.3" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.3.cmml">
                 I
                </mi>
                <mrow id="S3.E2.m1.4.4.2.2.2.2.2.2a" xref="S3.E2.m1.4.4.2.2.2.2.2.2.cmml">
                </mrow>
                <mi id="S3.E2.m1.4.4.2.2.2.2.2.2.3" xref="S3.E2.m1.4.4.2.2.2.2.2.2.3.cmml">
                 i
                </mi>
                <mrow id="S3.E2.m1.4.4.2.2.2.2.2.2b" xref="S3.E2.m1.4.4.2.2.2.2.2.2.cmml">
                </mrow>
               </mmultiscripts>
               <mo id="S3.E2.m1.4.4.2.2.2.2.2.5" stretchy="false" xref="S3.E2.m1.4.4.2.2.2.2.3.cmml">
                )
               </mo>
              </mrow>
             </mrow>
            </mrow>
           </mrow>
          </mrow>
          <annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b">
           <apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">
            <eq id="S3.E2.m1.4.4.3.cmml" xref="S3.E2.m1.4.4.3">
            </eq>
            <apply id="S3.E2.m1.4.4.4.cmml" xref="S3.E2.m1.4.4.4">
             <times id="S3.E2.m1.4.4.4.1.cmml" xref="S3.E2.m1.4.4.4.1">
             </times>
             <ci id="S3.E2.m1.4.4.4.2.cmml" xref="S3.E2.m1.4.4.4.2">
              ùê∏
             </ci>
             <interval closure="open" id="S3.E2.m1.4.4.4.3.1.cmml" xref="S3.E2.m1.4.4.4.3.2">
              <ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">
               ùëâ
              </ci>
              <ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">
               ùëá
              </ci>
             </interval>
            </apply>
            <apply id="S3.E2.m1.4.4.2.cmml" xref="S3.E2.m1.4.4.2">
             <times id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.3">
             </times>
             <apply id="S3.E2.m1.4.4.2.4.cmml" xref="S3.E2.m1.4.4.2.4">
              <divide id="S3.E2.m1.4.4.2.4.1.cmml" xref="S3.E2.m1.4.4.2.4">
              </divide>
              <cn id="S3.E2.m1.4.4.2.4.2.cmml" type="integer" xref="S3.E2.m1.4.4.2.4.2">
               1
              </cn>
              <ci id="S3.E2.m1.4.4.2.4.3.cmml" xref="S3.E2.m1.4.4.2.4.3">
               ùëõ
              </ci>
             </apply>
             <apply id="S3.E2.m1.4.4.2.2.cmml" xref="S3.E2.m1.4.4.2.2">
              <apply id="S3.E2.m1.4.4.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.3">
               <csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.3.1.cmml" xref="S3.E2.m1.4.4.2.2.3">
                superscript
               </csymbol>
               <apply id="S3.E2.m1.4.4.2.2.3.2.cmml" xref="S3.E2.m1.4.4.2.2.3">
                <csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.3.2.1.cmml" xref="S3.E2.m1.4.4.2.2.3">
                 subscript
                </csymbol>
                <sum id="S3.E2.m1.4.4.2.2.3.2.2.cmml" xref="S3.E2.m1.4.4.2.2.3.2.2">
                </sum>
                <apply id="S3.E2.m1.4.4.2.2.3.2.3.cmml" xref="S3.E2.m1.4.4.2.2.3.2.3">
                 <eq id="S3.E2.m1.4.4.2.2.3.2.3.1.cmml" xref="S3.E2.m1.4.4.2.2.3.2.3.1">
                 </eq>
                 <ci id="S3.E2.m1.4.4.2.2.3.2.3.2.cmml" xref="S3.E2.m1.4.4.2.2.3.2.3.2">
                  ùëñ
                 </ci>
                 <cn id="S3.E2.m1.4.4.2.2.3.2.3.3.cmml" type="integer" xref="S3.E2.m1.4.4.2.2.3.2.3.3">
                  1
                 </cn>
                </apply>
               </apply>
               <ci id="S3.E2.m1.4.4.2.2.3.3.cmml" xref="S3.E2.m1.4.4.2.2.3.3">
                ùëõ
               </ci>
              </apply>
              <apply id="S3.E2.m1.4.4.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2">
               <times id="S3.E2.m1.4.4.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.3">
               </times>
               <ci id="S3.E2.m1.4.4.2.2.2.4.cmml" xref="S3.E2.m1.4.4.2.2.2.4">
                ùëÜ
               </ci>
               <interval closure="open" id="S3.E2.m1.4.4.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2">
                <apply id="S3.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1">
                 <csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1">
                  subscript
                 </csymbol>
                 <apply id="S3.E2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1">
                  <csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1">
                   subscript
                  </csymbol>
                  <ci id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2">
                   ùë¶
                  </ci>
                  <ci id="S3.E2.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.3">
                   ùëá
                  </ci>
                 </apply>
                 <ci id="S3.E2.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3">
                  ùëñ
                 </ci>
                </apply>
                <apply id="S3.E2.m1.4.4.2.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2">
                 <csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2">
                  subscript
                 </csymbol>
                 <apply id="S3.E2.m1.4.4.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2">
                  <csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2">
                   subscript
                  </csymbol>
                  <ci id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2">
                   ùë¶
                  </ci>
                  <ci id="S3.E2.m1.4.4.2.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.3">
                   ùêº
                  </ci>
                 </apply>
                 <ci id="S3.E2.m1.4.4.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.3">
                  ùëñ
                 </ci>
                </apply>
               </interval>
              </apply>
             </apply>
            </apply>
           </apply>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.E2.m1.4c">
           E(V,T)=\frac{1}{n}\sum_{i=1}^{n}S({y_{T}}_{i},{y_{I}}_{i})
          </annotation>
         </semantics>
        </math>
       </td>
       <td class="ltx_eqn_cell ltx_eqn_center_padright">
       </td>
       <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
        <span class="ltx_tag ltx_tag_equation ltx_align_right">
         (2)
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <p class="ltx_p" id="S3.SS4.p3.13">
     where
     <math alttext="E(V,T)" class="ltx_Math" display="inline" id="S3.SS4.p3.10.m1.2">
      <semantics id="S3.SS4.p3.10.m1.2a">
       <mrow id="S3.SS4.p3.10.m1.2.3" xref="S3.SS4.p3.10.m1.2.3.cmml">
        <mi id="S3.SS4.p3.10.m1.2.3.2" xref="S3.SS4.p3.10.m1.2.3.2.cmml">
         E
        </mi>
        <mo id="S3.SS4.p3.10.m1.2.3.1" lspace="0em" rspace="0em" xref="S3.SS4.p3.10.m1.2.3.1.cmml">
         ‚Äã
        </mo>
        <mrow id="S3.SS4.p3.10.m1.2.3.3.2" xref="S3.SS4.p3.10.m1.2.3.3.1.cmml">
         <mo id="S3.SS4.p3.10.m1.2.3.3.2.1" stretchy="false" xref="S3.SS4.p3.10.m1.2.3.3.1.cmml">
          (
         </mo>
         <mi id="S3.SS4.p3.10.m1.1.1" xref="S3.SS4.p3.10.m1.1.1.cmml">
          V
         </mi>
         <mo id="S3.SS4.p3.10.m1.2.3.3.2.2" xref="S3.SS4.p3.10.m1.2.3.3.1.cmml">
          ,
         </mo>
         <mi id="S3.SS4.p3.10.m1.2.2" xref="S3.SS4.p3.10.m1.2.2.cmml">
          T
         </mi>
         <mo id="S3.SS4.p3.10.m1.2.3.3.2.3" stretchy="false" xref="S3.SS4.p3.10.m1.2.3.3.1.cmml">
          )
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p3.10.m1.2b">
        <apply id="S3.SS4.p3.10.m1.2.3.cmml" xref="S3.SS4.p3.10.m1.2.3">
         <times id="S3.SS4.p3.10.m1.2.3.1.cmml" xref="S3.SS4.p3.10.m1.2.3.1">
         </times>
         <ci id="S3.SS4.p3.10.m1.2.3.2.cmml" xref="S3.SS4.p3.10.m1.2.3.2">
          ùê∏
         </ci>
         <interval closure="open" id="S3.SS4.p3.10.m1.2.3.3.1.cmml" xref="S3.SS4.p3.10.m1.2.3.3.2">
          <ci id="S3.SS4.p3.10.m1.1.1.cmml" xref="S3.SS4.p3.10.m1.1.1">
           ùëâ
          </ci>
          <ci id="S3.SS4.p3.10.m1.2.2.cmml" xref="S3.SS4.p3.10.m1.2.2">
           ùëá
          </ci>
         </interval>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p3.10.m1.2c">
        E(V,T)
       </annotation>
      </semantics>
     </math>
     is the ensemble similarity score,
     <math alttext="S({y_{T}}_{i},{y_{I}}_{i})" class="ltx_Math" display="inline" id="S3.SS4.p3.11.m2.2">
      <semantics id="S3.SS4.p3.11.m2.2a">
       <mrow id="S3.SS4.p3.11.m2.2.2" xref="S3.SS4.p3.11.m2.2.2.cmml">
        <mi id="S3.SS4.p3.11.m2.2.2.4" xref="S3.SS4.p3.11.m2.2.2.4.cmml">
         S
        </mi>
        <mo id="S3.SS4.p3.11.m2.2.2.3" lspace="0em" rspace="0em" xref="S3.SS4.p3.11.m2.2.2.3.cmml">
         ‚Äã
        </mo>
        <mrow id="S3.SS4.p3.11.m2.2.2.2.2" xref="S3.SS4.p3.11.m2.2.2.2.3.cmml">
         <mo id="S3.SS4.p3.11.m2.2.2.2.2.3" stretchy="false" xref="S3.SS4.p3.11.m2.2.2.2.3.cmml">
          (
         </mo>
         <mmultiscripts id="S3.SS4.p3.11.m2.1.1.1.1.1" xref="S3.SS4.p3.11.m2.1.1.1.1.1.cmml">
          <mi id="S3.SS4.p3.11.m2.1.1.1.1.1.2.2" xref="S3.SS4.p3.11.m2.1.1.1.1.1.2.2.cmml">
           y
          </mi>
          <mi id="S3.SS4.p3.11.m2.1.1.1.1.1.2.3" xref="S3.SS4.p3.11.m2.1.1.1.1.1.2.3.cmml">
           T
          </mi>
          <mrow id="S3.SS4.p3.11.m2.1.1.1.1.1a" xref="S3.SS4.p3.11.m2.1.1.1.1.1.cmml">
          </mrow>
          <mi id="S3.SS4.p3.11.m2.1.1.1.1.1.3" xref="S3.SS4.p3.11.m2.1.1.1.1.1.3.cmml">
           i
          </mi>
          <mrow id="S3.SS4.p3.11.m2.1.1.1.1.1b" xref="S3.SS4.p3.11.m2.1.1.1.1.1.cmml">
          </mrow>
         </mmultiscripts>
         <mo id="S3.SS4.p3.11.m2.2.2.2.2.4" xref="S3.SS4.p3.11.m2.2.2.2.3.cmml">
          ,
         </mo>
         <mmultiscripts id="S3.SS4.p3.11.m2.2.2.2.2.2" xref="S3.SS4.p3.11.m2.2.2.2.2.2.cmml">
          <mi id="S3.SS4.p3.11.m2.2.2.2.2.2.2.2" xref="S3.SS4.p3.11.m2.2.2.2.2.2.2.2.cmml">
           y
          </mi>
          <mi id="S3.SS4.p3.11.m2.2.2.2.2.2.2.3" xref="S3.SS4.p3.11.m2.2.2.2.2.2.2.3.cmml">
           I
          </mi>
          <mrow id="S3.SS4.p3.11.m2.2.2.2.2.2a" xref="S3.SS4.p3.11.m2.2.2.2.2.2.cmml">
          </mrow>
          <mi id="S3.SS4.p3.11.m2.2.2.2.2.2.3" xref="S3.SS4.p3.11.m2.2.2.2.2.2.3.cmml">
           i
          </mi>
          <mrow id="S3.SS4.p3.11.m2.2.2.2.2.2b" xref="S3.SS4.p3.11.m2.2.2.2.2.2.cmml">
          </mrow>
         </mmultiscripts>
         <mo id="S3.SS4.p3.11.m2.2.2.2.2.5" stretchy="false" xref="S3.SS4.p3.11.m2.2.2.2.3.cmml">
          )
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p3.11.m2.2b">
        <apply id="S3.SS4.p3.11.m2.2.2.cmml" xref="S3.SS4.p3.11.m2.2.2">
         <times id="S3.SS4.p3.11.m2.2.2.3.cmml" xref="S3.SS4.p3.11.m2.2.2.3">
         </times>
         <ci id="S3.SS4.p3.11.m2.2.2.4.cmml" xref="S3.SS4.p3.11.m2.2.2.4">
          ùëÜ
         </ci>
         <interval closure="open" id="S3.SS4.p3.11.m2.2.2.2.3.cmml" xref="S3.SS4.p3.11.m2.2.2.2.2">
          <apply id="S3.SS4.p3.11.m2.1.1.1.1.1.cmml" xref="S3.SS4.p3.11.m2.1.1.1.1.1">
           <csymbol cd="ambiguous" id="S3.SS4.p3.11.m2.1.1.1.1.1.1.cmml" xref="S3.SS4.p3.11.m2.1.1.1.1.1">
            subscript
           </csymbol>
           <apply id="S3.SS4.p3.11.m2.1.1.1.1.1.2.cmml" xref="S3.SS4.p3.11.m2.1.1.1.1.1">
            <csymbol cd="ambiguous" id="S3.SS4.p3.11.m2.1.1.1.1.1.2.1.cmml" xref="S3.SS4.p3.11.m2.1.1.1.1.1">
             subscript
            </csymbol>
            <ci id="S3.SS4.p3.11.m2.1.1.1.1.1.2.2.cmml" xref="S3.SS4.p3.11.m2.1.1.1.1.1.2.2">
             ùë¶
            </ci>
            <ci id="S3.SS4.p3.11.m2.1.1.1.1.1.2.3.cmml" xref="S3.SS4.p3.11.m2.1.1.1.1.1.2.3">
             ùëá
            </ci>
           </apply>
           <ci id="S3.SS4.p3.11.m2.1.1.1.1.1.3.cmml" xref="S3.SS4.p3.11.m2.1.1.1.1.1.3">
            ùëñ
           </ci>
          </apply>
          <apply id="S3.SS4.p3.11.m2.2.2.2.2.2.cmml" xref="S3.SS4.p3.11.m2.2.2.2.2.2">
           <csymbol cd="ambiguous" id="S3.SS4.p3.11.m2.2.2.2.2.2.1.cmml" xref="S3.SS4.p3.11.m2.2.2.2.2.2">
            subscript
           </csymbol>
           <apply id="S3.SS4.p3.11.m2.2.2.2.2.2.2.cmml" xref="S3.SS4.p3.11.m2.2.2.2.2.2">
            <csymbol cd="ambiguous" id="S3.SS4.p3.11.m2.2.2.2.2.2.2.1.cmml" xref="S3.SS4.p3.11.m2.2.2.2.2.2">
             subscript
            </csymbol>
            <ci id="S3.SS4.p3.11.m2.2.2.2.2.2.2.2.cmml" xref="S3.SS4.p3.11.m2.2.2.2.2.2.2.2">
             ùë¶
            </ci>
            <ci id="S3.SS4.p3.11.m2.2.2.2.2.2.2.3.cmml" xref="S3.SS4.p3.11.m2.2.2.2.2.2.2.3">
             ùêº
            </ci>
           </apply>
           <ci id="S3.SS4.p3.11.m2.2.2.2.2.2.3.cmml" xref="S3.SS4.p3.11.m2.2.2.2.2.2.3">
            ùëñ
           </ci>
          </apply>
         </interval>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p3.11.m2.2c">
        S({y_{T}}_{i},{y_{I}}_{i})
       </annotation>
      </semantics>
     </math>
     is the similarity score for the
     <math alttext="i" class="ltx_Math" display="inline" id="S3.SS4.p3.12.m3.1">
      <semantics id="S3.SS4.p3.12.m3.1a">
       <mi id="S3.SS4.p3.12.m3.1.1" xref="S3.SS4.p3.12.m3.1.1.cmml">
        i
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p3.12.m3.1b">
        <ci id="S3.SS4.p3.12.m3.1.1.cmml" xref="S3.SS4.p3.12.m3.1.1">
         ùëñ
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p3.12.m3.1c">
        i
       </annotation>
      </semantics>
     </math>
     -th keyframe, and
     <math alttext="n" class="ltx_Math" display="inline" id="S3.SS4.p3.13.m4.1">
      <semantics id="S3.SS4.p3.13.m4.1a">
       <mi id="S3.SS4.p3.13.m4.1.1" xref="S3.SS4.p3.13.m4.1.1.cmml">
        n
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p3.13.m4.1b">
        <ci id="S3.SS4.p3.13.m4.1.1.cmml" xref="S3.SS4.p3.13.m4.1.1">
         ùëõ
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p3.13.m4.1c">
        n
       </annotation>
      </semantics>
     </math>
     is the total number of keyframes.
We also created the EgoVQA dataset specifically for egocentric human-object interaction video question-answering tasks to enrich the training data. For each caption in the Ego4D dataset, we used ChatGPT to generate five QA pairs. To ensure relevance, we guided ChatGPT to focus on core key verbs and nouns by designing prompts as shown in Listing
     <a class="ltx_ref" href="#LST3" title="Listing 3 ‚Ä£ 3.4 Creating EgoCOT and EgoVQA Dataset ‚Ä£ 3 Method ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     . The sampling schema when crafting EgoVQA is the same to that as EgoCOT.
    </p>
   </div>
   <figure class="ltx_float ltx_lstlisting" id="LST3">
    <div class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="LST3.1">
     <div class="ltx_listing_data">
      <a download="" href="data:text/plain;base64,UGxlYXNlIGFzayBzb21lIHF1ZXN0aW9ucyBhY2Nyb2RpbmcgdG8gdGhlIHZlcmJzIGFuZCBub3VucyBpbiB0aGUgc2VudGVuY2UuCkZvciBleGFtcGxlLCBpbiB0aGlzIHNlbnRlbmNlICJhIG1hbiBpcyBwaWNraW5nIHVwIGEgY3VwIiwgdGhlIHZlcmIgaXMgcGlja2luZyB1cCBhbmQgdGhlIG5vdW4gaXMgY3VwLCB0aGVyZWZvciBxdWVzdGlvbnMgY2FuIGJlICJ3aGF0IGlzIHRoZSBvYmplY3QgdGhlIG1hbiBpcyBwaWNraW5nIHVwPyIgb3IgIndoYXQgb3BlcmF0aW9uIGlzIHBlcmZvcm1lZCBvbiB0aGUgY3VwPyIuClRoZW4gWW91IG5lZWQgdG8gZ2l2ZSB0aGUgYW5zd2VyLgoKaW5wdXQ6IGEgbWFuIGlzIHBpY2tpbmcgdXAgYSBjdXAKcXVlc3Rpb246IFdoYXQgaXMgdGhlIG9iamVjdCB0aGUgbWFuIGlzIHBpY2tpbmcgdXAKYW5zd2VyOiBUaGUgY3Vw">
       ‚¨á
      </a>
     </div>
     <div class="ltx_listingline" id="lstnumberx14">
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.1" style="font-size:70%;">
       Please
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.2" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.3" style="font-size:70%;">
       ask
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.4" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.5" style="font-size:70%;">
       some
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.6" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.7" style="font-size:70%;">
       questions
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.8" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.9" style="font-size:70%;">
       accroding
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.10" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.11" style="font-size:70%;">
       to
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.12" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.13" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.14" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.15" style="font-size:70%;">
       verbs
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.16" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx14.17" style="font-size:70%;">
       and
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.18" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.19" style="font-size:70%;">
       nouns
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.20" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx14.21" style="font-size:70%;">
       in
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.22" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.23" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.24" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.25" style="font-size:70%;">
       sentence
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx14.26" style="font-size:70%;">
       .
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx15">
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.1" style="font-size:70%;">
       For
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.2" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.3" style="font-size:70%;">
       example
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx15.4" style="font-size:70%;">
       ,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.5" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx15.6" style="font-size:70%;">
       in
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.7" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.8" style="font-size:70%;">
       this
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.9" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.10" style="font-size:70%;">
       sentence
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.11" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx15.12" style="font-size:70%;">
       "a
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.12.1">
       </span>
       man
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.12.2">
       </span>
       is
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.12.3">
       </span>
       picking
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.12.4">
       </span>
       up
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.12.5">
       </span>
       a
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.12.6">
       </span>
       cup"
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx15.13" style="font-size:70%;">
       ,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.14" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.15" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.16" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.17" style="font-size:70%;">
       verb
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.18" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx15.19" style="font-size:70%;">
       is
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.20" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.21" style="font-size:70%;">
       picking
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.22" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.23" style="font-size:70%;">
       up
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.24" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx15.25" style="font-size:70%;">
       and
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.26" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.27" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.28" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.29" style="font-size:70%;">
       noun
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.30" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx15.31" style="font-size:70%;">
       is
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.32" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.33" style="font-size:70%;">
       cup
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx15.34" style="font-size:70%;">
       ,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.35" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.36" style="font-size:70%;">
       therefor
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.37" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.38" style="font-size:70%;">
       questions
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.39" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.40" style="font-size:70%;">
       can
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.41" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.42" style="font-size:70%;">
       be
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.43" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx15.44" style="font-size:70%;">
       "what
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.44.1">
       </span>
       is
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.44.2">
       </span>
       the
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.44.3">
       </span>
       object
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.44.4">
       </span>
       the
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.44.5">
       </span>
       man
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.44.6">
       </span>
       is
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.44.7">
       </span>
       picking
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.44.8">
       </span>
       up?"
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.45" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx15.46" style="font-size:70%;">
       or
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.47" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx15.48" style="font-size:70%;">
       "what
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.48.1">
       </span>
       operation
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.48.2">
       </span>
       is
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.48.3">
       </span>
       performed
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.48.4">
       </span>
       on
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.48.5">
       </span>
       the
       <span class="ltx_text ltx_lst_space" id="lstnumberx15.48.6">
       </span>
       cup?"
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx15.49" style="font-size:70%;">
       .
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx16">
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.1" style="font-size:70%;">
       Then
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.2" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.3" style="font-size:70%;">
       You
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.4" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.5" style="font-size:70%;">
       need
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.6" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.7" style="font-size:70%;">
       to
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.8" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.9" style="font-size:70%;">
       give
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.10" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.11" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.12" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.13" style="font-size:70%;">
       answer
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx16.14" style="font-size:70%;">
       .
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx17">
     </div>
     <div class="ltx_listingline" id="lstnumberx18">
      <span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter ltx_font_bold" id="lstnumberx18.1" style="font-size:70%;">
       input
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx18.2" style="font-size:70%;">
       :
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.3" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.4" style="font-size:70%;">
       a
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.5" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.6" style="font-size:70%;">
       man
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.7" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx18.8" style="font-size:70%;">
       is
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.9" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.10" style="font-size:70%;">
       picking
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.11" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.12" style="font-size:70%;">
       up
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.13" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.14" style="font-size:70%;">
       a
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.15" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.16" style="font-size:70%;">
       cup
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx19">
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.1" style="font-size:70%;">
       question
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx19.2" style="font-size:70%;">
       :
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.3" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.4" style="font-size:70%;">
       What
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.5" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx19.6" style="font-size:70%;">
       is
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.7" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.8" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.9" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter ltx_font_bold" id="lstnumberx19.10" style="font-size:70%;">
       object
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.11" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.12" style="font-size:70%;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.13" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.14" style="font-size:70%;">
       man
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.15" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx19.16" style="font-size:70%;">
       is
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.17" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.18" style="font-size:70%;">
       picking
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.19" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.20" style="font-size:70%;">
       up
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx20">
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.1" style="font-size:70%;">
       answer
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx20.2" style="font-size:70%;">
       :
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.3" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.4" style="font-size:70%;">
       The
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.5" style="font-size:70%;">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.6" style="font-size:70%;">
       cup
      </span>
     </div>
    </div>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_float">
      Listing¬†3:
     </span>
     Prompt used for creating EgoVQA dataset.
    </figcaption>
   </figure>
   <figure class="ltx_table" id="S3.T1">
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.5">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S3.T1.5.5">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.5.5.6">
        <span class="ltx_text" id="S3.T1.5.5.6.1" style="font-size:90%;">
         Model
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1">
        <span class="ltx_text" id="S3.T1.1.1.1.1" style="font-size:90%;">
         Object(
        </span>
        <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1">
         <semantics id="S3.T1.1.1.1.m1.1a">
          <mo id="S3.T1.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S3.T1.1.1.1.m1.1.1.cmml">
           ‚Üë
          </mo>
          <annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b">
           <ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">
            ‚Üë
           </ci>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">
           \uparrow
          </annotation>
         </semantics>
        </math>
        <span class="ltx_text" id="S3.T1.1.1.1.2" style="font-size:90%;">
         )
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.2.2">
        <span class="ltx_text" id="S3.T1.2.2.2.1" style="font-size:90%;">
         Spatial(
        </span>
        <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.2.2.2.m1.1">
         <semantics id="S3.T1.2.2.2.m1.1a">
          <mo id="S3.T1.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S3.T1.2.2.2.m1.1.1.cmml">
           ‚Üë
          </mo>
          <annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.1b">
           <ci id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1">
            ‚Üë
           </ci>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.1c">
           \uparrow
          </annotation>
         </semantics>
        </math>
        <span class="ltx_text" id="S3.T1.2.2.2.2" style="font-size:90%;">
         )
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.3.3.3">
        <span class="ltx_text" id="S3.T1.3.3.3.1" style="font-size:90%;">
         Redundancy(
        </span>
        <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.3.3.3.m1.1">
         <semantics id="S3.T1.3.3.3.m1.1a">
          <mo id="S3.T1.3.3.3.m1.1.1" mathsize="90%" stretchy="false" xref="S3.T1.3.3.3.m1.1.1.cmml">
           ‚Üì
          </mo>
          <annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.m1.1b">
           <ci id="S3.T1.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.m1.1.1">
            ‚Üì
           </ci>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T1.3.3.3.m1.1c">
           \downarrow
          </annotation>
         </semantics>
        </math>
        <span class="ltx_text" id="S3.T1.3.3.3.2" style="font-size:90%;">
         )
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.4.4.4">
        <span class="ltx_text" id="S3.T1.4.4.4.1" style="font-size:90%;">
         Plan Reasonable(
        </span>
        <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.4.4.4.m1.1">
         <semantics id="S3.T1.4.4.4.m1.1a">
          <mo id="S3.T1.4.4.4.m1.1.1" mathsize="90%" stretchy="false" xref="S3.T1.4.4.4.m1.1.1.cmml">
           ‚Üë
          </mo>
          <annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.m1.1b">
           <ci id="S3.T1.4.4.4.m1.1.1.cmml" xref="S3.T1.4.4.4.m1.1.1">
            ‚Üë
           </ci>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T1.4.4.4.m1.1c">
           \uparrow
          </annotation>
         </semantics>
        </math>
        <span class="ltx_text" id="S3.T1.4.4.4.2" style="font-size:90%;">
         )
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.5.5.5">
        <span class="ltx_text" id="S3.T1.5.5.5.1" style="font-size:90%;">
         Plan Executable(
        </span>
        <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.5.5.5.m1.1">
         <semantics id="S3.T1.5.5.5.m1.1a">
          <mo id="S3.T1.5.5.5.m1.1.1" mathsize="90%" stretchy="false" xref="S3.T1.5.5.5.m1.1.1.cmml">
           ‚Üë
          </mo>
          <annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.m1.1b">
           <ci id="S3.T1.5.5.5.m1.1.1.cmml" xref="S3.T1.5.5.5.m1.1.1">
            ‚Üë
           </ci>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T1.5.5.5.m1.1c">
           \uparrow
          </annotation>
         </semantics>
        </math>
        <span class="ltx_text" id="S3.T1.5.5.5.2" style="font-size:90%;">
         )
        </span>
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S3.T1.5.6.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.5.6.1.1">
        <span class="ltx_text" id="S3.T1.5.6.1.1.1" style="font-size:90%;">
         Minigpt4
        </span>
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.6.1.2">
        <span class="ltx_text" id="S3.T1.5.6.1.2.1" style="font-size:90%;">
         5.6
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.6.1.3">
        <span class="ltx_text" id="S3.T1.5.6.1.3.1" style="font-size:90%;">
         4.8
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.6.1.4">
        <span class="ltx_text" id="S3.T1.5.6.1.4.1" style="font-size:90%;">
         4.4
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.6.1.5">
        <span class="ltx_text" id="S3.T1.5.6.1.5.1" style="font-size:90%;">
         4.5
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.6.1.6">
        <span class="ltx_text" id="S3.T1.5.6.1.6.1" style="font-size:90%;">
         4.8
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.5.7.2">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.5.7.2.1">
        <span class="ltx_text" id="S3.T1.5.7.2.1.1" style="font-size:90%;">
         LLaVA-7B
        </span>
       </th>
       <td class="ltx_td ltx_align_center" id="S3.T1.5.7.2.2">
        <span class="ltx_text" id="S3.T1.5.7.2.2.1" style="font-size:90%;">
         7.3
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.5.7.2.3">
        <span class="ltx_text" id="S3.T1.5.7.2.3.1" style="font-size:90%;">
         7.4
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.5.7.2.4">
        <span class="ltx_text" id="S3.T1.5.7.2.4.1" style="font-size:90%;">
         3.9
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.5.7.2.5">
        <span class="ltx_text" id="S3.T1.5.7.2.5.1" style="font-size:90%;">
         7.5
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.5.7.2.6">
        <span class="ltx_text" id="S3.T1.5.7.2.6.1" style="font-size:90%;">
         6.6
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.5.8.3">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.5.8.3.1">
        <span class="ltx_text" id="S3.T1.5.8.3.1.1" style="font-size:90%;">
         LLaVA-13B
        </span>
       </th>
       <td class="ltx_td ltx_align_center" id="S3.T1.5.8.3.2">
        <span class="ltx_text ltx_font_bold" id="S3.T1.5.8.3.2.1" style="font-size:90%;">
         8.5
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.5.8.3.3">
        <span class="ltx_text" id="S3.T1.5.8.3.3.1" style="font-size:90%;">
         8.6
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.5.8.3.4">
        <span class="ltx_text" id="S3.T1.5.8.3.4.1" style="font-size:90%;">
         3.4
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.5.8.3.5">
        <span class="ltx_text" id="S3.T1.5.8.3.5.1" style="font-size:90%;">
         8.4
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.5.8.3.6">
        <span class="ltx_text" id="S3.T1.5.8.3.6.1" style="font-size:90%;">
         7.6
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.5.9.4">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.5.9.4.1" style="background-color:#E6E6E6;">
        <span class="ltx_text" id="S3.T1.5.9.4.1.1" style="font-size:90%;background-color:#E6E6E6;">
         EmbodiedGPT
        </span>
       </th>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.9.4.2" style="background-color:#E6E6E6;">
        <span class="ltx_text" id="S3.T1.5.9.4.2.1" style="font-size:90%;background-color:#E6E6E6;">
         8.4
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.9.4.3" style="background-color:#E6E6E6;">
        <span class="ltx_text ltx_font_bold" id="S3.T1.5.9.4.3.1" style="font-size:90%;background-color:#E6E6E6;">
         8.8
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.9.4.4" style="background-color:#E6E6E6;">
        <span class="ltx_text ltx_font_bold" id="S3.T1.5.9.4.4.1" style="font-size:90%;background-color:#E6E6E6;">
         2.6
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.9.4.5" style="background-color:#E6E6E6;">
        <span class="ltx_text ltx_font_bold" id="S3.T1.5.9.4.5.1" style="font-size:90%;background-color:#E6E6E6;">
         8.8
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.9.4.6" style="background-color:#E6E6E6;">
        <span class="ltx_text ltx_font_bold" id="S3.T1.5.9.4.6.1" style="font-size:90%;background-color:#E6E6E6;">
         8.4
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <figcaption class="ltx_caption ltx_centering" style="font-size:90%;">
     <span class="ltx_tag ltx_tag_table">
      Table 1:
     </span>
     Generate Quality Evaluation on image input tasks.
    </figcaption>
   </figure>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Experiments
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    In this section, we present a comprehensive evaluation of multi-modal foundation models and EmbodiedGPT, across various tasks including visual captioning, embodied planning, and control.
   </p>
  </div>
  <div class="ltx_para" id="S4.p2">
   <p class="ltx_p" id="S4.p2.1">
    <span class="ltx_text ltx_font_bold" id="S4.p2.1.1">
     Evaluation on image input tasks.
    </span>
    In order to evaluate the quality of generated captions and planning with the given image, we conducted a user study with 30 participants. The study included 10 cases of image caption tasks from MS-COCO dataset
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib44" title="">
      44
     </a>
     ]
    </cite>
    , 5 embodied planning scenarios in different embodied AI simulators, and 5 real-world scenes with accompanying embodied planning tasks. Participants were asked to rate the generated captions from different end-to-end models on five dimensions using a scoring system ranging from 1 to 10: object recognition accuracy, spatial relationship understanding, level of redundancy in the answer, and reasonability of the planning and the executability of the planning. The average scores among all the participants for different models are shown in Table
    <a class="ltx_ref" href="#S3.T1" title="Table 1 ‚Ä£ 3.4 Creating EgoCOT and EgoVQA Dataset ‚Ä£ 3 Method ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    .
The results demonstrate that EmbodiedGPT achieves a comparable level of object recognition and spatial relationship understanding as the LLaVA-13B model, despite having only 7B parameters in the language model. Furthermore, EmbodiedGPT generates less redundant content in relation to the given embodied AI task, and produces the most reasonable and executable planning outputs.
We also compared the performance of EmbodiedGPT with Visual ChatGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib33" title="">
      33
     </a>
     ]
    </cite>
    , which adopts a hierarchical approach by combining several pre-trained vision models and language models to answer questions. In the Virtual-Home
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib57" title="">
      57
     </a>
     ]
    </cite>
    benchmark, Visual ChatGPT uses a visual caption model to generate dense captions that are subsequently passed into ChatGPT for deriving a solution. As shown in Figure
    <a class="ltx_ref" href="#S4.F3" title="Figure 3 ‚Ä£ 4 Experiments ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    , Visual ChatGPT failed to find a coat hanger due to its limitations of relying solely on the caption model for extracting visual information, resulting in poor performance when compared to the end-to-end model like EmbodiedGPT. These findings highlight the advantages of adopting a unified, end-to-end model over hierarchical approaches that rely on multiple stages.
   </p>
  </div>
  <figure class="ltx_figure" id="S4.F3">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="101" id="S4.F3.g1" src="/html/2305.15021/assets/x3.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">
      Figure 3
     </span>
     :
    </span>
    <span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">
     Comparison between EmbodiedGPT and VisualGPT in the question-answering task.
    </span>
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="S4.F4">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="239" id="S4.F4.g1" src="/html/2305.15021/assets/x4.png" width="415"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">
      Figure 4
     </span>
     :
    </span>
    <span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">
     Example of video input embodied AI tasks on Meta-World benchmark. EmbodiedGPT accurately analyzes embodied control tasks in demonstration videos and provides precise planning.
    </span>
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="S4.F5">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_1">
     <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf1">
      <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="78" id="S4.F5.sf1.g1" src="/html/2305.15021/assets/x5.png" width="456"/>
      <figcaption class="ltx_caption ltx_centering">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="S4.F5.sf1.3.1.1" style="font-size:90%;">
         (a)
        </span>
       </span>
       <span class="ltx_text" id="S4.F5.sf1.4.2" style="font-size:90%;">
        Performance comparison in
        <span class="ltx_text ltx_font_italic" id="S4.F5.sf1.4.2.1">
         Franka Kitchen
        </span>
        with only 10 demos.
       </span>
      </figcaption>
     </figure>
    </div>
    <div class="ltx_flex_break">
    </div>
    <div class="ltx_flex_cell ltx_flex_size_1">
     <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf2">
      <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="75" id="S4.F5.sf2.g1" src="/html/2305.15021/assets/x6.png" width="456"/>
      <figcaption class="ltx_caption ltx_centering">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="S4.F5.sf2.3.1.1" style="font-size:90%;">
         (b)
        </span>
       </span>
       <span class="ltx_text" id="S4.F5.sf2.4.2" style="font-size:90%;">
        Performance comparison in
        <span class="ltx_text ltx_font_italic" id="S4.F5.sf2.4.2.1">
         Meta-World
        </span>
        with only 10 demos.
       </span>
      </figcaption>
     </figure>
    </div>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">
      Figure 5
     </span>
     :
    </span>
    <span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">
     Performance of EmbodiedGPT in low-level control tasks with 10 demonstration demos.
    </span>
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="S4.F6">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_1">
     <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf1">
      <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="80" id="S4.F6.sf1.g1" src="/html/2305.15021/assets/x7.png" width="457"/>
      <figcaption class="ltx_caption ltx_centering">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="S4.F6.sf1.3.1.1" style="font-size:90%;">
         (a)
        </span>
       </span>
       <span class="ltx_text" id="S4.F6.sf1.4.2" style="font-size:90%;">
        Performance comparison in
        <span class="ltx_text ltx_font_italic" id="S4.F6.sf1.4.2.1">
         Franka Kitchen
        </span>
        with 25 demos.
       </span>
      </figcaption>
     </figure>
    </div>
    <div class="ltx_flex_break">
    </div>
    <div class="ltx_flex_cell ltx_flex_size_1">
     <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf2">
      <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="79" id="S4.F6.sf2.g1" src="/html/2305.15021/assets/x8.png" width="456"/>
      <figcaption class="ltx_caption ltx_centering">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="S4.F6.sf2.3.1.1" style="font-size:90%;">
         (b)
        </span>
       </span>
       <span class="ltx_text" id="S4.F6.sf2.4.2" style="font-size:90%;">
        Performance comparison in
        <span class="ltx_text ltx_font_italic" id="S4.F6.sf2.4.2.1">
         Meta-World
        </span>
        with 25 demos.
       </span>
      </figcaption>
     </figure>
    </div>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">
      Figure 6
     </span>
     :
    </span>
    <span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">
     Performance of EmbodiedGPT in low-level control tasks with 25 demonstration demos.
    </span>
   </figcaption>
  </figure>
  <figure class="ltx_table" id="S4.T2">
   <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.12">
    <thead class="ltx_thead">
     <tr class="ltx_tr" id="S4.T2.12.13.1">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.12.13.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.12.13.1.1.1" style="font-size:90%;">
        Model
       </span>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.12.13.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.12.13.1.2.1" style="font-size:90%;">
        Franka(10 demos)
       </span>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.12.13.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.12.13.1.3.1" style="font-size:90%;">
        Franka(25 demos)
       </span>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.12.13.1.4" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.12.13.1.4.1" style="font-size:90%;">
        Meta-World(10 demos)
       </span>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.12.13.1.5" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.12.13.1.5.1" style="font-size:90%;">
        Meta-World(25 demos)
       </span>
      </th>
     </tr>
     <tr class="ltx_tr" id="S4.T2.4.4">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.4.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.4.4.5.1" style="font-size:90%;">
        EmbodiedGPT
       </span>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1" style="font-size:90%;">
        50.8
       </span>
       <span class="ltx_text" id="S4.T2.1.1.1.2" style="font-size:90%;">
        %
       </span>
       <math alttext="\pm 2.8" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1">
        <semantics id="S4.T2.1.1.1.m1.1a">
         <mrow id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">
          <mo id="S4.T2.1.1.1.m1.1.1a" mathsize="90%" xref="S4.T2.1.1.1.m1.1.1.cmml">
           ¬±
          </mo>
          <mn id="S4.T2.1.1.1.m1.1.1.2" mathsize="90%" xref="S4.T2.1.1.1.m1.1.1.2.cmml">
           2.8
          </mn>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b">
          <apply id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">
           <csymbol cd="latexml" id="S4.T2.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">
            plus-or-minus
           </csymbol>
           <cn id="S4.T2.1.1.1.m1.1.1.2.cmml" type="float" xref="S4.T2.1.1.1.m1.1.1.2">
            2.8
           </cn>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">
          \pm 2.8
         </annotation>
        </semantics>
       </math>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.1" style="font-size:90%;">
        58.5
       </span>
       <span class="ltx_text" id="S4.T2.2.2.2.2" style="font-size:90%;">
        %
       </span>
       <math alttext="\pm 2.7" class="ltx_Math" display="inline" id="S4.T2.2.2.2.m1.1">
        <semantics id="S4.T2.2.2.2.m1.1a">
         <mrow id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml">
          <mo id="S4.T2.2.2.2.m1.1.1a" mathsize="90%" xref="S4.T2.2.2.2.m1.1.1.cmml">
           ¬±
          </mo>
          <mn id="S4.T2.2.2.2.m1.1.1.2" mathsize="90%" xref="S4.T2.2.2.2.m1.1.1.2.cmml">
           2.7
          </mn>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b">
          <apply id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">
           <csymbol cd="latexml" id="S4.T2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">
            plus-or-minus
           </csymbol>
           <cn id="S4.T2.2.2.2.m1.1.1.2.cmml" type="float" xref="S4.T2.2.2.2.m1.1.1.2">
            2.7
           </cn>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">
          \pm 2.7
         </annotation>
        </semantics>
       </math>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.1" style="font-size:90%;">
        76.4
       </span>
       <span class="ltx_text" id="S4.T2.3.3.3.2" style="font-size:90%;">
        %
       </span>
       <math alttext="\pm 2.2" class="ltx_Math" display="inline" id="S4.T2.3.3.3.m1.1">
        <semantics id="S4.T2.3.3.3.m1.1a">
         <mrow id="S4.T2.3.3.3.m1.1.1" xref="S4.T2.3.3.3.m1.1.1.cmml">
          <mo id="S4.T2.3.3.3.m1.1.1a" mathsize="90%" xref="S4.T2.3.3.3.m1.1.1.cmml">
           ¬±
          </mo>
          <mn id="S4.T2.3.3.3.m1.1.1.2" mathsize="90%" xref="S4.T2.3.3.3.m1.1.1.2.cmml">
           2.2
          </mn>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b">
          <apply id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">
           <csymbol cd="latexml" id="S4.T2.3.3.3.m1.1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">
            plus-or-minus
           </csymbol>
           <cn id="S4.T2.3.3.3.m1.1.1.2.cmml" type="float" xref="S4.T2.3.3.3.m1.1.1.2">
            2.2
           </cn>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">
          \pm 2.2
         </annotation>
        </semantics>
       </math>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text ltx_font_bold" id="S4.T2.4.4.4.1" style="font-size:90%;">
        81.2
       </span>
       <span class="ltx_text" id="S4.T2.4.4.4.2" style="font-size:90%;">
        %
       </span>
       <math alttext="\pm 2.0" class="ltx_Math" display="inline" id="S4.T2.4.4.4.m1.1">
        <semantics id="S4.T2.4.4.4.m1.1a">
         <mrow id="S4.T2.4.4.4.m1.1.1" xref="S4.T2.4.4.4.m1.1.1.cmml">
          <mo id="S4.T2.4.4.4.m1.1.1a" mathsize="90%" xref="S4.T2.4.4.4.m1.1.1.cmml">
           ¬±
          </mo>
          <mn id="S4.T2.4.4.4.m1.1.1.2" mathsize="90%" xref="S4.T2.4.4.4.m1.1.1.2.cmml">
           2.0
          </mn>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.m1.1b">
          <apply id="S4.T2.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.m1.1.1">
           <csymbol cd="latexml" id="S4.T2.4.4.4.m1.1.1.1.cmml" xref="S4.T2.4.4.4.m1.1.1">
            plus-or-minus
           </csymbol>
           <cn id="S4.T2.4.4.4.m1.1.1.2.cmml" type="float" xref="S4.T2.4.4.4.m1.1.1.2">
            2.0
           </cn>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T2.4.4.4.m1.1c">
          \pm 2.0
         </annotation>
        </semantics>
       </math>
      </th>
     </tr>
    </thead>
    <tbody class="ltx_tbody">
     <tr class="ltx_tr" id="S4.T2.8.8">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.8.8.5" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.8.8.5.1" style="font-size:90%;">
        - Close-loop
       </span>
      </th>
      <td class="ltx_td ltx_align_center" id="S4.T2.5.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.5.5.1.1" style="font-size:90%;">
        38.6%
       </span>
       <math alttext="\pm 2.9" class="ltx_Math" display="inline" id="S4.T2.5.5.1.m1.1">
        <semantics id="S4.T2.5.5.1.m1.1a">
         <mrow id="S4.T2.5.5.1.m1.1.1" xref="S4.T2.5.5.1.m1.1.1.cmml">
          <mo id="S4.T2.5.5.1.m1.1.1a" mathsize="90%" xref="S4.T2.5.5.1.m1.1.1.cmml">
           ¬±
          </mo>
          <mn id="S4.T2.5.5.1.m1.1.1.2" mathsize="90%" xref="S4.T2.5.5.1.m1.1.1.2.cmml">
           2.9
          </mn>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S4.T2.5.5.1.m1.1b">
          <apply id="S4.T2.5.5.1.m1.1.1.cmml" xref="S4.T2.5.5.1.m1.1.1">
           <csymbol cd="latexml" id="S4.T2.5.5.1.m1.1.1.1.cmml" xref="S4.T2.5.5.1.m1.1.1">
            plus-or-minus
           </csymbol>
           <cn id="S4.T2.5.5.1.m1.1.1.2.cmml" type="float" xref="S4.T2.5.5.1.m1.1.1.2">
            2.9
           </cn>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T2.5.5.1.m1.1c">
          \pm 2.9
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T2.6.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.6.6.2.1" style="font-size:90%;">
        47.3%
       </span>
       <math alttext="\pm 2.5" class="ltx_Math" display="inline" id="S4.T2.6.6.2.m1.1">
        <semantics id="S4.T2.6.6.2.m1.1a">
         <mrow id="S4.T2.6.6.2.m1.1.1" xref="S4.T2.6.6.2.m1.1.1.cmml">
          <mo id="S4.T2.6.6.2.m1.1.1a" mathsize="90%" xref="S4.T2.6.6.2.m1.1.1.cmml">
           ¬±
          </mo>
          <mn id="S4.T2.6.6.2.m1.1.1.2" mathsize="90%" xref="S4.T2.6.6.2.m1.1.1.2.cmml">
           2.5
          </mn>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S4.T2.6.6.2.m1.1b">
          <apply id="S4.T2.6.6.2.m1.1.1.cmml" xref="S4.T2.6.6.2.m1.1.1">
           <csymbol cd="latexml" id="S4.T2.6.6.2.m1.1.1.1.cmml" xref="S4.T2.6.6.2.m1.1.1">
            plus-or-minus
           </csymbol>
           <cn id="S4.T2.6.6.2.m1.1.1.2.cmml" type="float" xref="S4.T2.6.6.2.m1.1.1.2">
            2.5
           </cn>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T2.6.6.2.m1.1c">
          \pm 2.5
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T2.7.7.3" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.7.7.3.1" style="font-size:90%;">
        62.7%
       </span>
       <math alttext="\pm 2.2" class="ltx_Math" display="inline" id="S4.T2.7.7.3.m1.1">
        <semantics id="S4.T2.7.7.3.m1.1a">
         <mrow id="S4.T2.7.7.3.m1.1.1" xref="S4.T2.7.7.3.m1.1.1.cmml">
          <mo id="S4.T2.7.7.3.m1.1.1a" mathsize="90%" xref="S4.T2.7.7.3.m1.1.1.cmml">
           ¬±
          </mo>
          <mn id="S4.T2.7.7.3.m1.1.1.2" mathsize="90%" xref="S4.T2.7.7.3.m1.1.1.2.cmml">
           2.2
          </mn>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S4.T2.7.7.3.m1.1b">
          <apply id="S4.T2.7.7.3.m1.1.1.cmml" xref="S4.T2.7.7.3.m1.1.1">
           <csymbol cd="latexml" id="S4.T2.7.7.3.m1.1.1.1.cmml" xref="S4.T2.7.7.3.m1.1.1">
            plus-or-minus
           </csymbol>
           <cn id="S4.T2.7.7.3.m1.1.1.2.cmml" type="float" xref="S4.T2.7.7.3.m1.1.1.2">
            2.2
           </cn>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T2.7.7.3.m1.1c">
          \pm 2.2
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T2.8.8.4" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.8.8.4.1" style="font-size:90%;">
        64.9%
       </span>
       <math alttext="\pm 2.0" class="ltx_Math" display="inline" id="S4.T2.8.8.4.m1.1">
        <semantics id="S4.T2.8.8.4.m1.1a">
         <mrow id="S4.T2.8.8.4.m1.1.1" xref="S4.T2.8.8.4.m1.1.1.cmml">
          <mo id="S4.T2.8.8.4.m1.1.1a" mathsize="90%" xref="S4.T2.8.8.4.m1.1.1.cmml">
           ¬±
          </mo>
          <mn id="S4.T2.8.8.4.m1.1.1.2" mathsize="90%" xref="S4.T2.8.8.4.m1.1.1.2.cmml">
           2.0
          </mn>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S4.T2.8.8.4.m1.1b">
          <apply id="S4.T2.8.8.4.m1.1.1.cmml" xref="S4.T2.8.8.4.m1.1.1">
           <csymbol cd="latexml" id="S4.T2.8.8.4.m1.1.1.1.cmml" xref="S4.T2.8.8.4.m1.1.1">
            plus-or-minus
           </csymbol>
           <cn id="S4.T2.8.8.4.m1.1.1.2.cmml" type="float" xref="S4.T2.8.8.4.m1.1.1.2">
            2.0
           </cn>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T2.8.8.4.m1.1c">
          \pm 2.0
         </annotation>
        </semantics>
       </math>
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T2.12.12">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.12.12.5" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.12.12.5.1" style="font-size:90%;">
        - COT
       </span>
      </th>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.9.9.1" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.9.9.1.1" style="font-size:90%;">
        26.2%
       </span>
       <math alttext="\pm 3.2" class="ltx_Math" display="inline" id="S4.T2.9.9.1.m1.1">
        <semantics id="S4.T2.9.9.1.m1.1a">
         <mrow id="S4.T2.9.9.1.m1.1.1" xref="S4.T2.9.9.1.m1.1.1.cmml">
          <mo id="S4.T2.9.9.1.m1.1.1a" mathsize="90%" xref="S4.T2.9.9.1.m1.1.1.cmml">
           ¬±
          </mo>
          <mn id="S4.T2.9.9.1.m1.1.1.2" mathsize="90%" xref="S4.T2.9.9.1.m1.1.1.2.cmml">
           3.2
          </mn>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S4.T2.9.9.1.m1.1b">
          <apply id="S4.T2.9.9.1.m1.1.1.cmml" xref="S4.T2.9.9.1.m1.1.1">
           <csymbol cd="latexml" id="S4.T2.9.9.1.m1.1.1.1.cmml" xref="S4.T2.9.9.1.m1.1.1">
            plus-or-minus
           </csymbol>
           <cn id="S4.T2.9.9.1.m1.1.1.2.cmml" type="float" xref="S4.T2.9.9.1.m1.1.1.2">
            3.2
           </cn>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T2.9.9.1.m1.1c">
          \pm 3.2
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.10.10.2" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.10.10.2.1" style="font-size:90%;">
        36.4%
       </span>
       <math alttext="\pm 2.7" class="ltx_Math" display="inline" id="S4.T2.10.10.2.m1.1">
        <semantics id="S4.T2.10.10.2.m1.1a">
         <mrow id="S4.T2.10.10.2.m1.1.1" xref="S4.T2.10.10.2.m1.1.1.cmml">
          <mo id="S4.T2.10.10.2.m1.1.1a" mathsize="90%" xref="S4.T2.10.10.2.m1.1.1.cmml">
           ¬±
          </mo>
          <mn id="S4.T2.10.10.2.m1.1.1.2" mathsize="90%" xref="S4.T2.10.10.2.m1.1.1.2.cmml">
           2.7
          </mn>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S4.T2.10.10.2.m1.1b">
          <apply id="S4.T2.10.10.2.m1.1.1.cmml" xref="S4.T2.10.10.2.m1.1.1">
           <csymbol cd="latexml" id="S4.T2.10.10.2.m1.1.1.1.cmml" xref="S4.T2.10.10.2.m1.1.1">
            plus-or-minus
           </csymbol>
           <cn id="S4.T2.10.10.2.m1.1.1.2.cmml" type="float" xref="S4.T2.10.10.2.m1.1.1.2">
            2.7
           </cn>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T2.10.10.2.m1.1c">
          \pm 2.7
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.11.11.3" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.11.11.3.1" style="font-size:90%;">
        55.2%
       </span>
       <math alttext="\pm 2.4" class="ltx_Math" display="inline" id="S4.T2.11.11.3.m1.1">
        <semantics id="S4.T2.11.11.3.m1.1a">
         <mrow id="S4.T2.11.11.3.m1.1.1" xref="S4.T2.11.11.3.m1.1.1.cmml">
          <mo id="S4.T2.11.11.3.m1.1.1a" mathsize="90%" xref="S4.T2.11.11.3.m1.1.1.cmml">
           ¬±
          </mo>
          <mn id="S4.T2.11.11.3.m1.1.1.2" mathsize="90%" xref="S4.T2.11.11.3.m1.1.1.2.cmml">
           2.4
          </mn>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S4.T2.11.11.3.m1.1b">
          <apply id="S4.T2.11.11.3.m1.1.1.cmml" xref="S4.T2.11.11.3.m1.1.1">
           <csymbol cd="latexml" id="S4.T2.11.11.3.m1.1.1.1.cmml" xref="S4.T2.11.11.3.m1.1.1">
            plus-or-minus
           </csymbol>
           <cn id="S4.T2.11.11.3.m1.1.1.2.cmml" type="float" xref="S4.T2.11.11.3.m1.1.1.2">
            2.4
           </cn>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T2.11.11.3.m1.1c">
          \pm 2.4
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.12.12.4" style="padding-left:3.0pt;padding-right:3.0pt;">
       <span class="ltx_text" id="S4.T2.12.12.4.1" style="font-size:90%;">
        58.7%
       </span>
       <math alttext="\pm 2.0" class="ltx_Math" display="inline" id="S4.T2.12.12.4.m1.1">
        <semantics id="S4.T2.12.12.4.m1.1a">
         <mrow id="S4.T2.12.12.4.m1.1.1" xref="S4.T2.12.12.4.m1.1.1.cmml">
          <mo id="S4.T2.12.12.4.m1.1.1a" mathsize="90%" xref="S4.T2.12.12.4.m1.1.1.cmml">
           ¬±
          </mo>
          <mn id="S4.T2.12.12.4.m1.1.1.2" mathsize="90%" xref="S4.T2.12.12.4.m1.1.1.2.cmml">
           2.0
          </mn>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S4.T2.12.12.4.m1.1b">
          <apply id="S4.T2.12.12.4.m1.1.1.cmml" xref="S4.T2.12.12.4.m1.1.1">
           <csymbol cd="latexml" id="S4.T2.12.12.4.m1.1.1.1.cmml" xref="S4.T2.12.12.4.m1.1.1">
            plus-or-minus
           </csymbol>
           <cn id="S4.T2.12.12.4.m1.1.1.2.cmml" type="float" xref="S4.T2.12.12.4.m1.1.1.2">
            2.0
           </cn>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T2.12.12.4.m1.1c">
          \pm 2.0
         </annotation>
        </semantics>
       </math>
      </td>
     </tr>
    </tbody>
   </table>
   <figcaption class="ltx_caption ltx_centering" style="font-size:90%;">
    <span class="ltx_tag ltx_tag_table">
     Table 2:
    </span>
    Ablation on the closed-loop spans from planning to low-level control, and "chain-of-thought" (COT) training with 25 and 10 demonstrations("-" symbol indicates "removing"). We report the average success rate over 5 tasks and 2 camera views per benchmark.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S4.p3">
   <p class="ltx_p" id="S4.p3.1">
    <span class="ltx_text ltx_font_bold" id="S4.p3.1.1">
     Evaluation on video input embodied AI tasks.
    </span>
    We evaluate the recognition ability of videos and planning abilities of our model for embodied control tasks on standard embodied AI benchmarks, Franka Kitchen
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib14" title="">
      14
     </a>
     ]
    </cite>
    and Meta-World
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib15" title="">
      15
     </a>
     ]
    </cite>
    .
Meta-World provides a challenging set of tasks that require complex object manipulation skills, including assembling a ring on a peg, picking and placing a block between bins, pushing a button, opening a drawer, and hammering a nail. Franka Kitchen benchmark focuses on tasks like sliding open the right door, opening the cabinet, turning on the light, turning the stovetop knob, and opening the microwave. As shown in Figure
    <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‚Ä£ 4 Experiments ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    ,
given a demonstration video, EmbodiedGPT can accurately interpret the embodied control task and provide step-by-step planning. The output planning is fed into the Embodied-former module of EmbodiedGPT to query highly relevant features for use as inputs in the policy network and the low-level actions are generated by the policy network to interact with the environment (see more visualizations in Appendix B).
   </p>
  </div>
  <div class="ltx_para" id="S4.p4">
   <p class="ltx_p" id="S4.p4.1">
    <span class="ltx_text ltx_font_bold" id="S4.p4.1.1">
     Evaluation on embodied control tasks.
    </span>
    For embodied control tasks, we compare our model with R3M
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ]
    </cite>
    , which is the state-of-the-art method in these two benchmarks, and an ablation version called ‚ÄôBLIP-2[Ego4D]‚Äô, which has the same structure and same amount of parameters as EmbodiedGPT, and is only fine-tuned on the video caption task using the Ego4D dataset without incorporating EgoCOT.
In all experiments, the policy network is learned using few-shot learning on a small amount of demonstration data. There are two settings, one of which utilizes 10 demonstrations, and the other utilizes 25 demonstrations. We report the success rate in 100 random evaluations with only visual observations in 5 tasks per benchmark over 5 seeds and 2 different camera views for each setting, respectively. As shown in Figure
    <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‚Ä£ 4 Experiments ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
     <span class="ltx_text ltx_ref_tag">
      5
     </span>
    </a>
    and Figure
    <a class="ltx_ref" href="#S4.F6" title="Figure 6 ‚Ä£ 4 Experiments ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
     <span class="ltx_text ltx_ref_tag">
      6
     </span>
    </a>
    , EmbodiedGPT outperforms the baseline methods, demonstrating the effectiveness of learning with EgoCOT.
   </p>
  </div>
  <div class="ltx_para" id="S4.p5">
   <p class="ltx_p" id="S4.p5.1">
    <span class="ltx_text ltx_font_bold" id="S4.p5.1.1">
     Ablation study.
    </span>
    We perform ablation studies to analyze the effectiveness of the "Chain-of-Thought" training mode and the importance of a closed-loop design for embodied control.
The results, as shown in Table
    <a class="ltx_ref" href="#S4.T2" title="Table 2 ‚Ä£ 4 Experiments ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    , demonstrate a significant improvement in success rate when using the EgoCOT approach compared to training solely with the EGO4D caption task. Moreover, the closed-loop design is necessary as the generated plans contained specific and relevant sub-goal information, which proved crucial for control tasks.
   </p>
  </div>
  <div class="ltx_para" id="S4.p6">
   <p class="ltx_p" id="S4.p6.1">
    In summary, EmbodiedGPT exhibits a strong ability to generate reasonable planning, accurately extract task-relevant features from visual inputs, as well as execute low-level actions to interact with the environment. The ablation experiments demonstrate that both the training paradigm based on EgoCOT and the closed-loop design from embodied planning to low-level control significantly contribute to the performance improvement of EmbodiedGPT.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    In this paper, we present EmbodiedGPT, an end-to-end multi-modal foundational model for embodied AI that enables agents to perform step-by-step planning and execute low-level commands. To achieve this, we create a large-scale embodied planning dataset called EgoCOT and develop an efficient training approach that utilizes prefix tuning to generate high-quality plans with a "chain-of-thought". Furthermore, our embodied control paradigm seamlessly coordinates high-level planning and low-level control. Extensive experiments demonstrate the effectiveness of EmbodiedGPT on various embodied tasks, achieving state-of-the-art or comparable performance. We believe that EmbodiedGPT represents a significant step towards developing more intelligent embodied AI agents.
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_bold" id="S5.p1.1.1">
     Future works and limitations:
    </span>
    EmbodiedGPT freezes the parameters of the vision and language model due to limited computational resources. Joint training with all modules and exploring other modalities, such as speech, could be future works. We do not foresee obvious undesirable ethical or social impacts at this moment.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_tag_bibitem">
     [1]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
      OpenAI.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">
      Gpt-4 technical report, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_tag_bibitem">
     [2]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
      Danny Driess, Fei Xia, Mehdi S.¬†M. Sajjadi, Corey Lynch, Aakanksha Chowdhery,
Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong
Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine,
Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng,
Igor Mordatch, and Pete Florence.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">
      Palm-e: An embodied multimodal language model.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib2.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib2.4.2" style="font-size:90%;">
      arXiv preprint arXiv:2303.03378
     </span>
     <span class="ltx_text" id="bib.bib2.5.3" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_tag_bibitem">
     [3]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
      Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas
Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John
Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">
      Openclip, July 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_tag_bibitem">
     [4]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
      Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun
Chen, Li¬†Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">
      Vima: General robot manipulation with multimodal prompts.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib4.3.1" style="font-size:90%;">
      arXiv preprint arXiv: Arxiv-2210.03094
     </span>
     <span class="ltx_text" id="bib.bib4.4.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_tag_bibitem">
     [5]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
      Yizhou Zhao, Qiaozi Gao, Liang Qiu, Govind Thattai, and Gaurav¬†S Sukhatme.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">
      Opend: A benchmark for language-driven door and drawer opening.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib5.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2212.05211
     </span>
     <span class="ltx_text" id="bib.bib5.4.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_tag_bibitem">
     [6]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
      Mohit Shridhar, Lucas Manuelli, and Dieter Fox.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">
      Cliport: What and where pathways for robotic manipulation.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib6.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib6.4.2" style="font-size:90%;">
      Conference on Robot Learning
     </span>
     <span class="ltx_text" id="bib.bib6.5.3" style="font-size:90%;">
      , pages 894‚Äì906. PMLR, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_tag_bibitem">
     [7]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
      Kaizhi Zheng, Xiaotong Chen, Odest¬†Chadwicke Jenkins, and Xin Wang.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">
      Vlmbench: A compositional benchmark for vision-and-language
manipulation.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib7.3.1" style="font-size:90%;">
      Advances in Neural Information Processing Systems
     </span>
     <span class="ltx_text" id="bib.bib7.4.2" style="font-size:90%;">
      , 35:665‚Äì678,
2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_tag_bibitem">
     [8]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
      Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu,
Hongsheng Li, Peng Gao, and Yu¬†Qiao.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">
      Llama-adapter: Efficient fine-tuning of language models with
zero-init attention.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib8.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2303.16199
     </span>
     <span class="ltx_text" id="bib.bib8.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_tag_bibitem">
     [9]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
      Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei
Zhang, Pan Lu, Conghui He, Xiangyu Yue, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">
      Llama-adapter v2: Parameter-efficient visual instruction model.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib9.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2304.15010
     </span>
     <span class="ltx_text" id="bib.bib9.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_tag_bibitem">
     [10]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
      Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee,
Lidong Bing, and Soujanya Poria.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">
      Llm-adapters: An adapter family for parameter-efficient fine-tuning
of large language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib10.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2304.01933
     </span>
     <span class="ltx_text" id="bib.bib10.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_tag_bibitem">
     [11]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
      Edward¬†J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu¬†Wang, and Weizhu Chen.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">
      Lora: Low-rank adaptation of large language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib11.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2106.09685
     </span>
     <span class="ltx_text" id="bib.bib11.4.2" style="font-size:90%;">
      , 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_tag_bibitem">
     [12]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
      Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">
      R3m: A universal visual representation for robot manipulation.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib12.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2203.12601
     </span>
     <span class="ltx_text" id="bib.bib12.4.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_tag_bibitem">
     [13]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
      Junnan Li, Dongxu Li, Silvio Savarese, and Steven C.¬†H. Hoi.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">
      BLIP-2: bootstrapping language-image pre-training with frozen image
encoders and large language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib13.3.1" style="font-size:90%;">
      CoRR
     </span>
     <span class="ltx_text" id="bib.bib13.4.2" style="font-size:90%;">
      , abs/2301.12597, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_tag_bibitem">
     [14]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
      Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">
      Relay policy learning: Solving long-horizon tasks via imitation and
reinforcement learning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib14.3.1" style="font-size:90%;">
      arXiv preprint arXiv:1910.11956
     </span>
     <span class="ltx_text" id="bib.bib14.4.2" style="font-size:90%;">
      , 2019.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_tag_bibitem">
     [15]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
      Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea
Finn, and Sergey Levine.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">
      Meta-world: A benchmark and evaluation for multi-task and meta
reinforcement learning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib15.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib15.4.2" style="font-size:90%;">
      Conference on robot learning
     </span>
     <span class="ltx_text" id="bib.bib15.5.3" style="font-size:90%;">
      , pages 1094‚Äì1100. PMLR, 2020.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_tag_bibitem">
     [16]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
      Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino
Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu,
et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">
      Ego4d: Around the world in 3,000 hours of egocentric video.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib16.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib16.4.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition
     </span>
     <span class="ltx_text" id="bib.bib16.5.3" style="font-size:90%;">
      , pages 18995‚Äì19012, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_tag_bibitem">
     [17]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
      Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">
      Blip-2: Bootstrapping language-image pre-training with frozen image
encoders and large language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib17.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2301.12597
     </span>
     <span class="ltx_text" id="bib.bib17.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_tag_bibitem">
     [18]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
      Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed¬†El Kholy, Faisal Ahmed, Zhe Gan,
Yu¬†Cheng, and Jingjing Liu.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">
      UNITER: universal image-text representation learning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib18.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib18.4.2" style="font-size:90%;">
      ECCV
     </span>
     <span class="ltx_text" id="bib.bib18.5.3" style="font-size:90%;">
      , volume 12375, pages 104‚Äì120, 2020.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_tag_bibitem">
     [19]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
      Xiujun Li, Xi¬†Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
Wang, Houdong Hu, Li¬†Dong, Furu Wei, Yejin Choi, and Jianfeng Gao.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">
      Oscar: Object-semantics aligned pre-training for vision-language
tasks.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib19.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib19.4.2" style="font-size:90%;">
      ECCV
     </span>
     <span class="ltx_text" id="bib.bib19.5.3" style="font-size:90%;">
      , pages 121‚Äì137, 2020.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_tag_bibitem">
     [20]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
      Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
Yejin Choi, and Jianfeng Gao.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">
      Vinvl: Making visual representations matter in vision-language
models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib20.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2101.00529
     </span>
     <span class="ltx_text" id="bib.bib20.4.2" style="font-size:90%;">
      , 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_tag_bibitem">
     [21]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
      Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers,
Alexander Kolesnikov, and Lucas Beyer.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">
      Lit: Zero-shot transfer with locked-image text tuning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib21.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib21.4.2" style="font-size:90%;">
      CVPR
     </span>
     <span class="ltx_text" id="bib.bib21.5.3" style="font-size:90%;">
      , pages 18102‚Äì18112, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_tag_bibitem">
     [22]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
      Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S.¬†M.¬†Ali Eslami, Oriol Vinyals,
and Felix Hill.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">
      Multimodal few-shot learning with frozen language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib22.3.1" style="font-size:90%;">
      In Marc‚ÄôAurelio Ranzato, Alina Beygelzimer, Yann¬†N. Dauphin, Percy
Liang, and Jennifer¬†Wortman Vaughan, editors,
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib22.4.2" style="font-size:90%;">
      NeurIPS
     </span>
     <span class="ltx_text" id="bib.bib22.5.3" style="font-size:90%;">
      , pages 200‚Äì212,
2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_tag_bibitem">
     [23]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
      Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">
      Visualgpt: Data-efficient adaptation of pretrained language models
for image captioning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib23.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib23.4.2" style="font-size:90%;">
      CVPR
     </span>
     <span class="ltx_text" id="bib.bib23.5.3" style="font-size:90%;">
      , pages 18009‚Äì18019, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_tag_bibitem">
     [24]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
      Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds,
Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina
Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew
Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo
Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">
      Flamingo: a visual language model for few-shot learning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib24.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2204.14198
     </span>
     <span class="ltx_text" id="bib.bib24.4.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_tag_bibitem">
     [25]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
      Fabian Caba¬†Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos¬†Niebles.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">
      Activitynet: A large-scale video benchmark for human activity
understanding.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib25.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib25.4.2" style="font-size:90%;">
      CVPR
     </span>
     <span class="ltx_text" id="bib.bib25.5.3" style="font-size:90%;">
      , pages 961‚Äì970, 2015.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_tag_bibitem">
     [26]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
      Yazan Abu¬†Farha, Alexander Richard, and Juergen Gall.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">
      When will you do what?-anticipating temporal occurrences of
activities.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib26.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib26.4.2" style="font-size:90%;">
      CVPR
     </span>
     <span class="ltx_text" id="bib.bib26.5.3" style="font-size:90%;">
      , pages 5343‚Äì5352, 2018.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_tag_bibitem">
     [27]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
      Benita Wong, Joya Chen, You Wu, Stan¬†Weixian Lei, Dongxing Mao, Difei Gao, and
Mike¬†Zheng Shou.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">
      Assistq: Affordance-centric question-driven task completion for
egocentric assistant.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib27.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib27.4.2" style="font-size:90%;">
      ECCV
     </span>
     <span class="ltx_text" id="bib.bib27.5.3" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_tag_bibitem">
     [28]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
      Dima Damen, Hazel Doughty, Giovanni¬†Maria Farinella, Antonino Furnari,
Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett,
Will Price, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">
      Rescaling egocentric vision: Collection, pipeline and challenges for
epic-kitchens-100.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib28.3.1" style="font-size:90%;">
      IJCV
     </span>
     <span class="ltx_text" id="bib.bib28.4.2" style="font-size:90%;">
      , 130(1):33‚Äì55, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_tag_bibitem">
     [29]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
      Gunnar¬†A Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek
Alahari.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">
      Charades-ego: A large-scale dataset of paired third and first person
videos.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib29.3.1" style="font-size:90%;">
      arXiv preprint arXiv:1804.09626
     </span>
     <span class="ltx_text" id="bib.bib29.4.2" style="font-size:90%;">
      , 2018.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_tag_bibitem">
     [30]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
      Yin Li, Zhefan Ye, and James¬†M Rehg.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">
      Delving into egocentric actions.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib30.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib30.4.2" style="font-size:90%;">
      CVPR
     </span>
     <span class="ltx_text" id="bib.bib30.5.3" style="font-size:90%;">
      , pages 287‚Äì295, 2015.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_tag_bibitem">
     [31]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
      Yecheng¬†Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash
Kumar, and Amy Zhang.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">
      Vip: Towards universal visual reward and representation via
value-implicit pre-training.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib31.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2210.00030
     </span>
     <span class="ltx_text" id="bib.bib31.4.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_tag_bibitem">
     [32]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
      Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared¬†D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">
      Language models are few-shot learners.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib32.3.1" style="font-size:90%;">
      In H.¬†Larochelle, M.¬†Ranzato, R.¬†Hadsell, M.F. Balcan, and H.¬†Lin,
editors,
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib32.4.2" style="font-size:90%;">
      Advances in Neural Information Processing Systems
     </span>
     <span class="ltx_text" id="bib.bib32.5.3" style="font-size:90%;">
      , volume¬†33,
pages 1877‚Äì1901. Curran Associates, Inc., 2020.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_tag_bibitem">
     [33]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
      Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan
Duan.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">
      Visual chatgpt: Talking, drawing and editing with visual foundation
models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib33.3.1" style="font-size:90%;">
      CoRR
     </span>
     <span class="ltx_text" id="bib.bib33.4.2" style="font-size:90%;">
      , abs/2303.04671, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_tag_bibitem">
     [34]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
      Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal
Ahmed, Zicheng Liu, Ce¬†Liu, Michael Zeng, and Lijuan Wang.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">
      Mm-react: Prompting chatgpt for multimodal reasoning and action.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib34.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2303.11381
     </span>
     <span class="ltx_text" id="bib.bib34.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_tag_bibitem">
     [35]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
      Yongliang Shen, Kaitao Song, Xu¬†Tan, Dongsheng Li, Weiming Lu, and Yueting
Zhuang.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">
      Hugginggpt: Solving AI tasks with chatgpt and its friends in
huggingface.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib35.3.1" style="font-size:90%;">
      CoRR
     </span>
     <span class="ltx_text" id="bib.bib35.4.2" style="font-size:90%;">
      , abs/2303.17580, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_tag_bibitem">
     [36]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
      Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">
      Minigpt-4: Enhancing vision-language understanding with advanced
large language models, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_tag_bibitem">
     [37]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
      Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong¬†Jae Lee.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">
      Visual instruction tuning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib37.3.1" style="font-size:90%;">
      CoRR
     </span>
     <span class="ltx_text" id="bib.bib37.4.2" style="font-size:90%;">
      , abs/2304.08485, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_tag_bibitem">
     [38]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
      KunChang Li, Yinan He, Yi¬†Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang,
Limin Wang, and Yu¬†Qiao.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">
      Videochat: Chat-centric video understanding.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib38.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2305.06355
     </span>
     <span class="ltx_text" id="bib.bib38.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_tag_bibitem">
     [39]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
      Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang
Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">
      mplug-owl: Modularization empowers large language models with
multimodality.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib39.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2304.14178
     </span>
     <span class="ltx_text" id="bib.bib39.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_tag_bibitem">
     [40]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
      Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu,
and Bo¬†Xu.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib40.2.1" style="font-size:90%;">
      X-llm: Bootstrapping advanced large language models by treating
multi-modalities as foreign languages.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib40.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2305.04160
     </span>
     <span class="ltx_text" id="bib.bib40.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_tag_bibitem">
     [41]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib41.1.1" style="font-size:90%;">
      Danny Driess, Fei Xia, Mehdi S.¬†M. Sajjadi, Corey Lynch, Aakanksha Chowdhery,
Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong
Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine,
Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng,
Igor Mordatch, and Pete Florence.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib41.2.1" style="font-size:90%;">
      Palm-e: An embodied multimodal language model.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib41.3.1" style="font-size:90%;">
      CoRR
     </span>
     <span class="ltx_text" id="bib.bib41.4.2" style="font-size:90%;">
      , abs/2303.03378, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_tag_bibitem">
     [42]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib42.1.1" style="font-size:90%;">
      Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron
David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog,
et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib42.2.1" style="font-size:90%;">
      Do as i can, not as i say: Grounding language in robotic affordances.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib42.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2204.01691
     </span>
     <span class="ltx_text" id="bib.bib42.4.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_tag_bibitem">
     [43]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib43.1.1" style="font-size:90%;">
      Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric
Hambro, Faisal Azhar, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib43.2.1" style="font-size:90%;">
      Llama: Open and efficient foundation language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib43.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2302.13971
     </span>
     <span class="ltx_text" id="bib.bib43.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_tag_bibitem">
     [44]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib44.1.1" style="font-size:90%;">
      Tsung-Yi Lin, Michael Maire, Serge¬†J. Belongie, James Hays, Pietro Perona,
Deva Ramanan, Piotr Doll√°r, and C.¬†Lawrence Zitnick.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib44.2.1" style="font-size:90%;">
      Microsoft COCO: common objects in context.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib44.3.1" style="font-size:90%;">
      In David¬†J. Fleet, Tom√°s Pajdla, Bernt Schiele, and Tinne
Tuytelaars, editors,
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib44.4.2" style="font-size:90%;">
      ECCV
     </span>
     <span class="ltx_text" id="bib.bib44.5.3" style="font-size:90%;">
      , volume 8693, pages 740‚Äì755, 2014.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_tag_bibitem">
     [45]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib45.1.1" style="font-size:90%;">
      Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib45.2.1" style="font-size:90%;">
      Conceptual captions: A cleaned, hypernymed, image alt-text dataset
for automatic image captioning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib45.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib45.4.2" style="font-size:90%;">
      Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)
     </span>
     <span class="ltx_text" id="bib.bib45.5.3" style="font-size:90%;">
      , pages 2556‚Äì2565, 2018.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_tag_bibitem">
     [46]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib46.1.1" style="font-size:90%;">
      Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong¬†Jae Lee.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib46.2.1" style="font-size:90%;">
      Visual instruction tuning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib46.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2304.08485
     </span>
     <span class="ltx_text" id="bib.bib46.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_tag_bibitem">
     [47]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib47.1.1" style="font-size:90%;">
      Rahul¬†Dev Singh, Ajay Mittal, and Rajesh¬†K Bhatia.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib47.2.1" style="font-size:90%;">
      3d convolutional neural network for object recognition: a review.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib47.3.1" style="font-size:90%;">
      Multimedia Tools and Applications
     </span>
     <span class="ltx_text" id="bib.bib47.4.2" style="font-size:90%;">
      , 78:15951‚Äì15995, 2019.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_tag_bibitem">
     [48]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib48.1.1" style="font-size:90%;">
      Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib48.2.1" style="font-size:90%;">
      Deep residual learning for image recognition.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib48.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib48.4.2" style="font-size:90%;">
      Proceedings of the IEEE conference on computer vision and
pattern recognition
     </span>
     <span class="ltx_text" id="bib.bib48.5.3" style="font-size:90%;">
      , pages 770‚Äì778, 2016.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_tag_bibitem">
     [49]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib49.1.1" style="font-size:90%;">
      Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li¬†Fei-Fei.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib49.2.1" style="font-size:90%;">
      Imagenet: A large-scale hierarchical image database.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib49.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib49.4.2" style="font-size:90%;">
      2009 IEEE conference on computer vision and pattern
recognition
     </span>
     <span class="ltx_text" id="bib.bib49.5.3" style="font-size:90%;">
      , pages 248‚Äì255. Ieee, 2009.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_tag_bibitem">
     [50]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib50.1.1" style="font-size:90%;">
      Martin Riedmiller and A¬†Lernen.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib50.2.1" style="font-size:90%;">
      Multi layer perceptron.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib50.3.1" style="font-size:90%;">
      Machine Learning Lab Special Lecture, University of Freiburg
     </span>
     <span class="ltx_text" id="bib.bib50.4.2" style="font-size:90%;">
      ,
pages 7‚Äì24, 2014.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib51">
    <span class="ltx_tag ltx_tag_bibitem">
     [51]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib51.1.1" style="font-size:90%;">
      Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun
Huang, Xinlong Wang, and Yue Cao.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib51.2.1" style="font-size:90%;">
      Eva: Exploring the limits of masked visual representation learning at
scale.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib51.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2211.07636
     </span>
     <span class="ltx_text" id="bib.bib51.4.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib52">
    <span class="ltx_tag ltx_tag_bibitem">
     [52]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib52.1.1" style="font-size:90%;">
      Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib52.2.1" style="font-size:90%;">
      Instruction tuning with gpt-4, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib53">
    <span class="ltx_tag ltx_tag_bibitem">
     [53]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib53.1.1" style="font-size:90%;">
      Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib53.2.1" style="font-size:90%;">
      An image is worth 16x16 words: Transformers for image recognition at
scale.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib53.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib53.4.2" style="font-size:90%;">
      ICLR
     </span>
     <span class="ltx_text" id="bib.bib53.5.3" style="font-size:90%;">
      , 2020.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib54">
    <span class="ltx_tag ltx_tag_bibitem">
     [54]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib54.1.1" style="font-size:90%;">
      Kevin¬†Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric¬†Z.
XU, Difei Gao, Rong-Cheng Tu, Wenzhe Zhao, Weijie Kong, Chengfei Cai, WANG
HongFa, Dima Damen, Bernard Ghanem, Wei Liu, and Mike¬†Zheng Shou.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib54.2.1" style="font-size:90%;">
      Egocentric video-language pretraining.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib54.3.1" style="font-size:90%;">
      In S.¬†Koyejo, S.¬†Mohamed, A.¬†Agarwal, D.¬†Belgrave, K.¬†Cho, and A.¬†Oh,
editors,
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib54.4.2" style="font-size:90%;">
      Advances in Neural Information Processing Systems
     </span>
     <span class="ltx_text" id="bib.bib54.5.3" style="font-size:90%;">
      , volume¬†35,
pages 7575‚Äì7586. Curran Associates, Inc., 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib55">
    <span class="ltx_tag ltx_tag_bibitem">
     [55]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib55.1.1" style="font-size:90%;">
      OpenAI.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib55.2.1" style="font-size:90%;">
      Chatgpt (mar 14 version) [large language model], 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib56">
    <span class="ltx_tag ltx_tag_bibitem">
     [56]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib56.1.1" style="font-size:90%;">
      Alec Radford, Jong¬†Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib56.2.1" style="font-size:90%;">
      Learning transferable visual models from natural language
supervision.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib56.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2103.00020
     </span>
     <span class="ltx_text" id="bib.bib56.4.2" style="font-size:90%;">
      , 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib57">
    <span class="ltx_tag ltx_tag_bibitem">
     [57]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib57.1.1" style="font-size:90%;">
      Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and
Antonio Torralba.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib57.2.1" style="font-size:90%;">
      Virtualhome: Simulating household activities via programs.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib57.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib57.4.2" style="font-size:90%;">
      Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition
     </span>
     <span class="ltx_text" id="bib.bib57.5.3" style="font-size:90%;">
      , pages 8494‚Äì8502, 2018.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib58">
    <span class="ltx_tag ltx_tag_bibitem">
     [58]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib58.1.1" style="font-size:90%;">
      Ilya Loshchilov and Frank Hutter.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib58.2.1" style="font-size:90%;">
      Decoupled weight decay regularization.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib58.3.1" style="font-size:90%;">
      arXiv preprint arXiv:1711.05101
     </span>
     <span class="ltx_text" id="bib.bib58.4.2" style="font-size:90%;">
      , 2017.
     </span>
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Implementation details
  </h2>
  <section class="ltx_subsection" id="A1.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.1
    </span>
    Hyper-parameters
   </h3>
   <div class="ltx_para" id="A1.SS1.p1">
    <p class="ltx_p" id="A1.SS1.p1.5">
     We use the same set of training hyper-parameters for all models during vision-language pre-training. We employ the AdamW optimizer
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib58" title="">
       58
      </a>
      ]
     </cite>
     with
     <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="A1.SS1.p1.1.m1.1">
      <semantics id="A1.SS1.p1.1.m1.1a">
       <mrow id="A1.SS1.p1.1.m1.1.1" xref="A1.SS1.p1.1.m1.1.1.cmml">
        <msub id="A1.SS1.p1.1.m1.1.1.2" xref="A1.SS1.p1.1.m1.1.1.2.cmml">
         <mi id="A1.SS1.p1.1.m1.1.1.2.2" xref="A1.SS1.p1.1.m1.1.1.2.2.cmml">
          Œ≤
         </mi>
         <mn id="A1.SS1.p1.1.m1.1.1.2.3" xref="A1.SS1.p1.1.m1.1.1.2.3.cmml">
          1
         </mn>
        </msub>
        <mo id="A1.SS1.p1.1.m1.1.1.1" xref="A1.SS1.p1.1.m1.1.1.1.cmml">
         =
        </mo>
        <mn id="A1.SS1.p1.1.m1.1.1.3" xref="A1.SS1.p1.1.m1.1.1.3.cmml">
         0.9
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A1.SS1.p1.1.m1.1b">
        <apply id="A1.SS1.p1.1.m1.1.1.cmml" xref="A1.SS1.p1.1.m1.1.1">
         <eq id="A1.SS1.p1.1.m1.1.1.1.cmml" xref="A1.SS1.p1.1.m1.1.1.1">
         </eq>
         <apply id="A1.SS1.p1.1.m1.1.1.2.cmml" xref="A1.SS1.p1.1.m1.1.1.2">
          <csymbol cd="ambiguous" id="A1.SS1.p1.1.m1.1.1.2.1.cmml" xref="A1.SS1.p1.1.m1.1.1.2">
           subscript
          </csymbol>
          <ci id="A1.SS1.p1.1.m1.1.1.2.2.cmml" xref="A1.SS1.p1.1.m1.1.1.2.2">
           ùõΩ
          </ci>
          <cn id="A1.SS1.p1.1.m1.1.1.2.3.cmml" type="integer" xref="A1.SS1.p1.1.m1.1.1.2.3">
           1
          </cn>
         </apply>
         <cn id="A1.SS1.p1.1.m1.1.1.3.cmml" type="float" xref="A1.SS1.p1.1.m1.1.1.3">
          0.9
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS1.p1.1.m1.1c">
        \beta_{1}=0.9
       </annotation>
      </semantics>
     </math>
     ,
     <math alttext="\beta_{2}=0.98" class="ltx_Math" display="inline" id="A1.SS1.p1.2.m2.1">
      <semantics id="A1.SS1.p1.2.m2.1a">
       <mrow id="A1.SS1.p1.2.m2.1.1" xref="A1.SS1.p1.2.m2.1.1.cmml">
        <msub id="A1.SS1.p1.2.m2.1.1.2" xref="A1.SS1.p1.2.m2.1.1.2.cmml">
         <mi id="A1.SS1.p1.2.m2.1.1.2.2" xref="A1.SS1.p1.2.m2.1.1.2.2.cmml">
          Œ≤
         </mi>
         <mn id="A1.SS1.p1.2.m2.1.1.2.3" xref="A1.SS1.p1.2.m2.1.1.2.3.cmml">
          2
         </mn>
        </msub>
        <mo id="A1.SS1.p1.2.m2.1.1.1" xref="A1.SS1.p1.2.m2.1.1.1.cmml">
         =
        </mo>
        <mn id="A1.SS1.p1.2.m2.1.1.3" xref="A1.SS1.p1.2.m2.1.1.3.cmml">
         0.98
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A1.SS1.p1.2.m2.1b">
        <apply id="A1.SS1.p1.2.m2.1.1.cmml" xref="A1.SS1.p1.2.m2.1.1">
         <eq id="A1.SS1.p1.2.m2.1.1.1.cmml" xref="A1.SS1.p1.2.m2.1.1.1">
         </eq>
         <apply id="A1.SS1.p1.2.m2.1.1.2.cmml" xref="A1.SS1.p1.2.m2.1.1.2">
          <csymbol cd="ambiguous" id="A1.SS1.p1.2.m2.1.1.2.1.cmml" xref="A1.SS1.p1.2.m2.1.1.2">
           subscript
          </csymbol>
          <ci id="A1.SS1.p1.2.m2.1.1.2.2.cmml" xref="A1.SS1.p1.2.m2.1.1.2.2">
           ùõΩ
          </ci>
          <cn id="A1.SS1.p1.2.m2.1.1.2.3.cmml" type="integer" xref="A1.SS1.p1.2.m2.1.1.2.3">
           2
          </cn>
         </apply>
         <cn id="A1.SS1.p1.2.m2.1.1.3.cmml" type="float" xref="A1.SS1.p1.2.m2.1.1.3">
          0.98
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS1.p1.2.m2.1c">
        \beta_{2}=0.98
       </annotation>
      </semantics>
     </math>
     , and a weight decay of 0.05. We also utilize a cosine learning rate decay with a peak learning rate of
     <math alttext="2\times 10^{-5}" class="ltx_Math" display="inline" id="A1.SS1.p1.3.m3.1">
      <semantics id="A1.SS1.p1.3.m3.1a">
       <mrow id="A1.SS1.p1.3.m3.1.1" xref="A1.SS1.p1.3.m3.1.1.cmml">
        <mn id="A1.SS1.p1.3.m3.1.1.2" xref="A1.SS1.p1.3.m3.1.1.2.cmml">
         2
        </mn>
        <mo id="A1.SS1.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS1.p1.3.m3.1.1.1.cmml">
         √ó
        </mo>
        <msup id="A1.SS1.p1.3.m3.1.1.3" xref="A1.SS1.p1.3.m3.1.1.3.cmml">
         <mn id="A1.SS1.p1.3.m3.1.1.3.2" xref="A1.SS1.p1.3.m3.1.1.3.2.cmml">
          10
         </mn>
         <mrow id="A1.SS1.p1.3.m3.1.1.3.3" xref="A1.SS1.p1.3.m3.1.1.3.3.cmml">
          <mo id="A1.SS1.p1.3.m3.1.1.3.3a" xref="A1.SS1.p1.3.m3.1.1.3.3.cmml">
           ‚àí
          </mo>
          <mn id="A1.SS1.p1.3.m3.1.1.3.3.2" xref="A1.SS1.p1.3.m3.1.1.3.3.2.cmml">
           5
          </mn>
         </mrow>
        </msup>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A1.SS1.p1.3.m3.1b">
        <apply id="A1.SS1.p1.3.m3.1.1.cmml" xref="A1.SS1.p1.3.m3.1.1">
         <times id="A1.SS1.p1.3.m3.1.1.1.cmml" xref="A1.SS1.p1.3.m3.1.1.1">
         </times>
         <cn id="A1.SS1.p1.3.m3.1.1.2.cmml" type="integer" xref="A1.SS1.p1.3.m3.1.1.2">
          2
         </cn>
         <apply id="A1.SS1.p1.3.m3.1.1.3.cmml" xref="A1.SS1.p1.3.m3.1.1.3">
          <csymbol cd="ambiguous" id="A1.SS1.p1.3.m3.1.1.3.1.cmml" xref="A1.SS1.p1.3.m3.1.1.3">
           superscript
          </csymbol>
          <cn id="A1.SS1.p1.3.m3.1.1.3.2.cmml" type="integer" xref="A1.SS1.p1.3.m3.1.1.3.2">
           10
          </cn>
          <apply id="A1.SS1.p1.3.m3.1.1.3.3.cmml" xref="A1.SS1.p1.3.m3.1.1.3.3">
           <minus id="A1.SS1.p1.3.m3.1.1.3.3.1.cmml" xref="A1.SS1.p1.3.m3.1.1.3.3">
           </minus>
           <cn id="A1.SS1.p1.3.m3.1.1.3.3.2.cmml" type="integer" xref="A1.SS1.p1.3.m3.1.1.3.3.2">
            5
           </cn>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS1.p1.3.m3.1c">
        2\times 10^{-5}
       </annotation>
      </semantics>
     </math>
     and a linear warm-up with warm-up ratio
     <math alttext="5\times 10^{-2}" class="ltx_Math" display="inline" id="A1.SS1.p1.4.m4.1">
      <semantics id="A1.SS1.p1.4.m4.1a">
       <mrow id="A1.SS1.p1.4.m4.1.1" xref="A1.SS1.p1.4.m4.1.1.cmml">
        <mn id="A1.SS1.p1.4.m4.1.1.2" xref="A1.SS1.p1.4.m4.1.1.2.cmml">
         5
        </mn>
        <mo id="A1.SS1.p1.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS1.p1.4.m4.1.1.1.cmml">
         √ó
        </mo>
        <msup id="A1.SS1.p1.4.m4.1.1.3" xref="A1.SS1.p1.4.m4.1.1.3.cmml">
         <mn id="A1.SS1.p1.4.m4.1.1.3.2" xref="A1.SS1.p1.4.m4.1.1.3.2.cmml">
          10
         </mn>
         <mrow id="A1.SS1.p1.4.m4.1.1.3.3" xref="A1.SS1.p1.4.m4.1.1.3.3.cmml">
          <mo id="A1.SS1.p1.4.m4.1.1.3.3a" xref="A1.SS1.p1.4.m4.1.1.3.3.cmml">
           ‚àí
          </mo>
          <mn id="A1.SS1.p1.4.m4.1.1.3.3.2" xref="A1.SS1.p1.4.m4.1.1.3.3.2.cmml">
           2
          </mn>
         </mrow>
        </msup>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A1.SS1.p1.4.m4.1b">
        <apply id="A1.SS1.p1.4.m4.1.1.cmml" xref="A1.SS1.p1.4.m4.1.1">
         <times id="A1.SS1.p1.4.m4.1.1.1.cmml" xref="A1.SS1.p1.4.m4.1.1.1">
         </times>
         <cn id="A1.SS1.p1.4.m4.1.1.2.cmml" type="integer" xref="A1.SS1.p1.4.m4.1.1.2">
          5
         </cn>
         <apply id="A1.SS1.p1.4.m4.1.1.3.cmml" xref="A1.SS1.p1.4.m4.1.1.3">
          <csymbol cd="ambiguous" id="A1.SS1.p1.4.m4.1.1.3.1.cmml" xref="A1.SS1.p1.4.m4.1.1.3">
           superscript
          </csymbol>
          <cn id="A1.SS1.p1.4.m4.1.1.3.2.cmml" type="integer" xref="A1.SS1.p1.4.m4.1.1.3.2">
           10
          </cn>
          <apply id="A1.SS1.p1.4.m4.1.1.3.3.cmml" xref="A1.SS1.p1.4.m4.1.1.3.3">
           <minus id="A1.SS1.p1.4.m4.1.1.3.3.1.cmml" xref="A1.SS1.p1.4.m4.1.1.3.3">
           </minus>
           <cn id="A1.SS1.p1.4.m4.1.1.3.3.2.cmml" type="integer" xref="A1.SS1.p1.4.m4.1.1.3.3.2">
            2
           </cn>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS1.p1.4.m4.1c">
        5\times 10^{-2}
       </annotation>
      </semantics>
     </math>
     . Our training data consists of images of size 224
     <math alttext="\times" class="ltx_Math" display="inline" id="A1.SS1.p1.5.m5.1">
      <semantics id="A1.SS1.p1.5.m5.1a">
       <mo id="A1.SS1.p1.5.m5.1.1" xref="A1.SS1.p1.5.m5.1.1.cmml">
        √ó
       </mo>
       <annotation-xml encoding="MathML-Content" id="A1.SS1.p1.5.m5.1b">
        <times id="A1.SS1.p1.5.m5.1.1.cmml" xref="A1.SS1.p1.5.m5.1.1">
        </times>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS1.p1.5.m5.1c">
        \times
       </annotation>
      </semantics>
     </math>
     224 that are augmented with random resized cropping and horizontal flipping. The maximize sequence length is set as 256.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A1.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.2
    </span>
    Downstream policy learning
   </h3>
   <div class="ltx_para" id="A1.SS2.p1">
    <p class="ltx_p" id="A1.SS2.p1.1">
     We adopt imitation learning as the method of policy learning in low level control tasks, which leverages demonstration data provided by an expert to learn the desired behavior. This technique has found applications in various domains, such as robotics, autonomous driving, and game playing. We provide each task 25 demonstrations, which are trajectories of observations and actions performed by an expert in the given task, and test the performance with 25 demonstrations and only 10 demonstrations respectively. The goal of imitation learning is to learn a policy, denoted as
     <math alttext="\pi" class="ltx_Math" display="inline" id="A1.SS2.p1.1.m1.1">
      <semantics id="A1.SS2.p1.1.m1.1a">
       <mi id="A1.SS2.p1.1.m1.1.1" xref="A1.SS2.p1.1.m1.1.1.cmml">
        œÄ
       </mi>
       <annotation-xml encoding="MathML-Content" id="A1.SS2.p1.1.m1.1b">
        <ci id="A1.SS2.p1.1.m1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1">
         ùúã
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS2.p1.1.m1.1c">
        \pi
       </annotation>
      </semantics>
     </math>
     , that maps the agent‚Äôs observations to appropriate actions. The learned policy should be able to imitate the expert‚Äôs behavior accurately. Speciffically, we use behavioral cloning to learn the downstream policy, which trains a supervised learning model to predict actions given states based on the expert demonstrations, and the loss function is shown as Equation
     <a class="ltx_ref" href="#A1.E3" title="In A.2 Downstream policy learning ‚Ä£ Appendix A Implementation details ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
    </p>
   </div>
   <div class="ltx_para" id="A1.SS2.p2">
    <table class="ltx_equation ltx_eqn_table" id="A1.E3">
     <tbody>
      <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
       <td class="ltx_eqn_cell ltx_eqn_center_padleft">
       </td>
       <td class="ltx_eqn_cell ltx_align_center">
        <math alttext="L(\theta)=\sum[\pi_{\theta}(a|s)\log P^{*}(a|s)]" class="ltx_Math" display="block" id="A1.E3.m1.2">
         <semantics id="A1.E3.m1.2a">
          <mrow id="A1.E3.m1.2.2" xref="A1.E3.m1.2.2.cmml">
           <mrow id="A1.E3.m1.2.2.3" xref="A1.E3.m1.2.2.3.cmml">
            <mi id="A1.E3.m1.2.2.3.2" xref="A1.E3.m1.2.2.3.2.cmml">
             L
            </mi>
            <mo id="A1.E3.m1.2.2.3.1" lspace="0em" rspace="0em" xref="A1.E3.m1.2.2.3.1.cmml">
             ‚Äã
            </mo>
            <mrow id="A1.E3.m1.2.2.3.3.2" xref="A1.E3.m1.2.2.3.cmml">
             <mo id="A1.E3.m1.2.2.3.3.2.1" stretchy="false" xref="A1.E3.m1.2.2.3.cmml">
              (
             </mo>
             <mi id="A1.E3.m1.1.1" xref="A1.E3.m1.1.1.cmml">
              Œ∏
             </mi>
             <mo id="A1.E3.m1.2.2.3.3.2.2" stretchy="false" xref="A1.E3.m1.2.2.3.cmml">
              )
             </mo>
            </mrow>
           </mrow>
           <mo id="A1.E3.m1.2.2.2" rspace="0.111em" xref="A1.E3.m1.2.2.2.cmml">
            =
           </mo>
           <mrow id="A1.E3.m1.2.2.1" xref="A1.E3.m1.2.2.1.cmml">
            <mo id="A1.E3.m1.2.2.1.2" movablelimits="false" rspace="0em" xref="A1.E3.m1.2.2.1.2.cmml">
             ‚àë
            </mo>
            <mrow id="A1.E3.m1.2.2.1.1.1" xref="A1.E3.m1.2.2.1.1.2.cmml">
             <mo id="A1.E3.m1.2.2.1.1.1.2" stretchy="false" xref="A1.E3.m1.2.2.1.1.2.1.cmml">
              [
             </mo>
             <mrow id="A1.E3.m1.2.2.1.1.1.1" xref="A1.E3.m1.2.2.1.1.1.1.cmml">
              <msub id="A1.E3.m1.2.2.1.1.1.1.4" xref="A1.E3.m1.2.2.1.1.1.1.4.cmml">
               <mi id="A1.E3.m1.2.2.1.1.1.1.4.2" xref="A1.E3.m1.2.2.1.1.1.1.4.2.cmml">
                œÄ
               </mi>
               <mi id="A1.E3.m1.2.2.1.1.1.1.4.3" xref="A1.E3.m1.2.2.1.1.1.1.4.3.cmml">
                Œ∏
               </mi>
              </msub>
              <mo id="A1.E3.m1.2.2.1.1.1.1.3" lspace="0em" rspace="0em" xref="A1.E3.m1.2.2.1.1.1.1.3.cmml">
               ‚Äã
              </mo>
              <mrow id="A1.E3.m1.2.2.1.1.1.1.1.1" xref="A1.E3.m1.2.2.1.1.1.1.1.1.1.cmml">
               <mo id="A1.E3.m1.2.2.1.1.1.1.1.1.2" stretchy="false" xref="A1.E3.m1.2.2.1.1.1.1.1.1.1.cmml">
                (
               </mo>
               <mrow id="A1.E3.m1.2.2.1.1.1.1.1.1.1" xref="A1.E3.m1.2.2.1.1.1.1.1.1.1.cmml">
                <mi id="A1.E3.m1.2.2.1.1.1.1.1.1.1.2" xref="A1.E3.m1.2.2.1.1.1.1.1.1.1.2.cmml">
                 a
                </mi>
                <mo fence="false" id="A1.E3.m1.2.2.1.1.1.1.1.1.1.1" xref="A1.E3.m1.2.2.1.1.1.1.1.1.1.1.cmml">
                 |
                </mo>
                <mi id="A1.E3.m1.2.2.1.1.1.1.1.1.1.3" xref="A1.E3.m1.2.2.1.1.1.1.1.1.1.3.cmml">
                 s
                </mi>
               </mrow>
               <mo id="A1.E3.m1.2.2.1.1.1.1.1.1.3" stretchy="false" xref="A1.E3.m1.2.2.1.1.1.1.1.1.1.cmml">
                )
               </mo>
              </mrow>
              <mo id="A1.E3.m1.2.2.1.1.1.1.3a" lspace="0.167em" rspace="0em" xref="A1.E3.m1.2.2.1.1.1.1.3.cmml">
               ‚Äã
              </mo>
              <mrow id="A1.E3.m1.2.2.1.1.1.1.5" xref="A1.E3.m1.2.2.1.1.1.1.5.cmml">
               <mi id="A1.E3.m1.2.2.1.1.1.1.5.1" xref="A1.E3.m1.2.2.1.1.1.1.5.1.cmml">
                log
               </mi>
               <mo id="A1.E3.m1.2.2.1.1.1.1.5a" lspace="0.167em" xref="A1.E3.m1.2.2.1.1.1.1.5.cmml">
                ‚Å°
               </mo>
               <msup id="A1.E3.m1.2.2.1.1.1.1.5.2" xref="A1.E3.m1.2.2.1.1.1.1.5.2.cmml">
                <mi id="A1.E3.m1.2.2.1.1.1.1.5.2.2" xref="A1.E3.m1.2.2.1.1.1.1.5.2.2.cmml">
                 P
                </mi>
                <mo id="A1.E3.m1.2.2.1.1.1.1.5.2.3" xref="A1.E3.m1.2.2.1.1.1.1.5.2.3.cmml">
                 ‚àó
                </mo>
               </msup>
              </mrow>
              <mo id="A1.E3.m1.2.2.1.1.1.1.3b" lspace="0em" rspace="0em" xref="A1.E3.m1.2.2.1.1.1.1.3.cmml">
               ‚Äã
              </mo>
              <mrow id="A1.E3.m1.2.2.1.1.1.1.2.1" xref="A1.E3.m1.2.2.1.1.1.1.2.1.1.cmml">
               <mo id="A1.E3.m1.2.2.1.1.1.1.2.1.2" stretchy="false" xref="A1.E3.m1.2.2.1.1.1.1.2.1.1.cmml">
                (
               </mo>
               <mrow id="A1.E3.m1.2.2.1.1.1.1.2.1.1" xref="A1.E3.m1.2.2.1.1.1.1.2.1.1.cmml">
                <mi id="A1.E3.m1.2.2.1.1.1.1.2.1.1.2" xref="A1.E3.m1.2.2.1.1.1.1.2.1.1.2.cmml">
                 a
                </mi>
                <mo fence="false" id="A1.E3.m1.2.2.1.1.1.1.2.1.1.1" xref="A1.E3.m1.2.2.1.1.1.1.2.1.1.1.cmml">
                 |
                </mo>
                <mi id="A1.E3.m1.2.2.1.1.1.1.2.1.1.3" xref="A1.E3.m1.2.2.1.1.1.1.2.1.1.3.cmml">
                 s
                </mi>
               </mrow>
               <mo id="A1.E3.m1.2.2.1.1.1.1.2.1.3" stretchy="false" xref="A1.E3.m1.2.2.1.1.1.1.2.1.1.cmml">
                )
               </mo>
              </mrow>
             </mrow>
             <mo id="A1.E3.m1.2.2.1.1.1.3" stretchy="false" xref="A1.E3.m1.2.2.1.1.2.1.cmml">
              ]
             </mo>
            </mrow>
           </mrow>
          </mrow>
          <annotation-xml encoding="MathML-Content" id="A1.E3.m1.2b">
           <apply id="A1.E3.m1.2.2.cmml" xref="A1.E3.m1.2.2">
            <eq id="A1.E3.m1.2.2.2.cmml" xref="A1.E3.m1.2.2.2">
            </eq>
            <apply id="A1.E3.m1.2.2.3.cmml" xref="A1.E3.m1.2.2.3">
             <times id="A1.E3.m1.2.2.3.1.cmml" xref="A1.E3.m1.2.2.3.1">
             </times>
             <ci id="A1.E3.m1.2.2.3.2.cmml" xref="A1.E3.m1.2.2.3.2">
              ùêø
             </ci>
             <ci id="A1.E3.m1.1.1.cmml" xref="A1.E3.m1.1.1">
              ùúÉ
             </ci>
            </apply>
            <apply id="A1.E3.m1.2.2.1.cmml" xref="A1.E3.m1.2.2.1">
             <sum id="A1.E3.m1.2.2.1.2.cmml" xref="A1.E3.m1.2.2.1.2">
             </sum>
             <apply id="A1.E3.m1.2.2.1.1.2.cmml" xref="A1.E3.m1.2.2.1.1.1">
              <csymbol cd="latexml" id="A1.E3.m1.2.2.1.1.2.1.cmml" xref="A1.E3.m1.2.2.1.1.1.2">
               delimited-[]
              </csymbol>
              <apply id="A1.E3.m1.2.2.1.1.1.1.cmml" xref="A1.E3.m1.2.2.1.1.1.1">
               <times id="A1.E3.m1.2.2.1.1.1.1.3.cmml" xref="A1.E3.m1.2.2.1.1.1.1.3">
               </times>
               <apply id="A1.E3.m1.2.2.1.1.1.1.4.cmml" xref="A1.E3.m1.2.2.1.1.1.1.4">
                <csymbol cd="ambiguous" id="A1.E3.m1.2.2.1.1.1.1.4.1.cmml" xref="A1.E3.m1.2.2.1.1.1.1.4">
                 subscript
                </csymbol>
                <ci id="A1.E3.m1.2.2.1.1.1.1.4.2.cmml" xref="A1.E3.m1.2.2.1.1.1.1.4.2">
                 ùúã
                </ci>
                <ci id="A1.E3.m1.2.2.1.1.1.1.4.3.cmml" xref="A1.E3.m1.2.2.1.1.1.1.4.3">
                 ùúÉ
                </ci>
               </apply>
               <apply id="A1.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="A1.E3.m1.2.2.1.1.1.1.1.1">
                <csymbol cd="latexml" id="A1.E3.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="A1.E3.m1.2.2.1.1.1.1.1.1.1.1">
                 conditional
                </csymbol>
                <ci id="A1.E3.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="A1.E3.m1.2.2.1.1.1.1.1.1.1.2">
                 ùëé
                </ci>
                <ci id="A1.E3.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="A1.E3.m1.2.2.1.1.1.1.1.1.1.3">
                 ùë†
                </ci>
               </apply>
               <apply id="A1.E3.m1.2.2.1.1.1.1.5.cmml" xref="A1.E3.m1.2.2.1.1.1.1.5">
                <log id="A1.E3.m1.2.2.1.1.1.1.5.1.cmml" xref="A1.E3.m1.2.2.1.1.1.1.5.1">
                </log>
                <apply id="A1.E3.m1.2.2.1.1.1.1.5.2.cmml" xref="A1.E3.m1.2.2.1.1.1.1.5.2">
                 <csymbol cd="ambiguous" id="A1.E3.m1.2.2.1.1.1.1.5.2.1.cmml" xref="A1.E3.m1.2.2.1.1.1.1.5.2">
                  superscript
                 </csymbol>
                 <ci id="A1.E3.m1.2.2.1.1.1.1.5.2.2.cmml" xref="A1.E3.m1.2.2.1.1.1.1.5.2.2">
                  ùëÉ
                 </ci>
                 <times id="A1.E3.m1.2.2.1.1.1.1.5.2.3.cmml" xref="A1.E3.m1.2.2.1.1.1.1.5.2.3">
                 </times>
                </apply>
               </apply>
               <apply id="A1.E3.m1.2.2.1.1.1.1.2.1.1.cmml" xref="A1.E3.m1.2.2.1.1.1.1.2.1">
                <csymbol cd="latexml" id="A1.E3.m1.2.2.1.1.1.1.2.1.1.1.cmml" xref="A1.E3.m1.2.2.1.1.1.1.2.1.1.1">
                 conditional
                </csymbol>
                <ci id="A1.E3.m1.2.2.1.1.1.1.2.1.1.2.cmml" xref="A1.E3.m1.2.2.1.1.1.1.2.1.1.2">
                 ùëé
                </ci>
                <ci id="A1.E3.m1.2.2.1.1.1.1.2.1.1.3.cmml" xref="A1.E3.m1.2.2.1.1.1.1.2.1.1.3">
                 ùë†
                </ci>
               </apply>
              </apply>
             </apply>
            </apply>
           </apply>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="A1.E3.m1.2c">
           L(\theta)=\sum[\pi_{\theta}(a|s)\log P^{*}(a|s)]
          </annotation>
         </semantics>
        </math>
       </td>
       <td class="ltx_eqn_cell ltx_eqn_center_padright">
       </td>
       <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
        <span class="ltx_tag ltx_tag_equation ltx_align_right">
         (3)
        </span>
       </td>
      </tr>
     </tbody>
    </table>
   </div>
   <div class="ltx_para" id="A1.SS2.p3">
    <p class="ltx_p" id="A1.SS2.p3.4">
     Here,
     <math alttext="\theta" class="ltx_Math" display="inline" id="A1.SS2.p3.1.m1.1">
      <semantics id="A1.SS2.p3.1.m1.1a">
       <mi id="A1.SS2.p3.1.m1.1.1" xref="A1.SS2.p3.1.m1.1.1.cmml">
        Œ∏
       </mi>
       <annotation-xml encoding="MathML-Content" id="A1.SS2.p3.1.m1.1b">
        <ci id="A1.SS2.p3.1.m1.1.1.cmml" xref="A1.SS2.p3.1.m1.1.1">
         ùúÉ
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS2.p3.1.m1.1c">
        \theta
       </annotation>
      </semantics>
     </math>
     represents the parameters of the policy model,
     <math alttext="\pi_{\theta}(a|s)" class="ltx_Math" display="inline" id="A1.SS2.p3.2.m2.1">
      <semantics id="A1.SS2.p3.2.m2.1a">
       <mrow id="A1.SS2.p3.2.m2.1.1" xref="A1.SS2.p3.2.m2.1.1.cmml">
        <msub id="A1.SS2.p3.2.m2.1.1.3" xref="A1.SS2.p3.2.m2.1.1.3.cmml">
         <mi id="A1.SS2.p3.2.m2.1.1.3.2" xref="A1.SS2.p3.2.m2.1.1.3.2.cmml">
          œÄ
         </mi>
         <mi id="A1.SS2.p3.2.m2.1.1.3.3" xref="A1.SS2.p3.2.m2.1.1.3.3.cmml">
          Œ∏
         </mi>
        </msub>
        <mo id="A1.SS2.p3.2.m2.1.1.2" lspace="0em" rspace="0em" xref="A1.SS2.p3.2.m2.1.1.2.cmml">
         ‚Äã
        </mo>
        <mrow id="A1.SS2.p3.2.m2.1.1.1.1" xref="A1.SS2.p3.2.m2.1.1.1.1.1.cmml">
         <mo id="A1.SS2.p3.2.m2.1.1.1.1.2" stretchy="false" xref="A1.SS2.p3.2.m2.1.1.1.1.1.cmml">
          (
         </mo>
         <mrow id="A1.SS2.p3.2.m2.1.1.1.1.1" xref="A1.SS2.p3.2.m2.1.1.1.1.1.cmml">
          <mi id="A1.SS2.p3.2.m2.1.1.1.1.1.2" xref="A1.SS2.p3.2.m2.1.1.1.1.1.2.cmml">
           a
          </mi>
          <mo fence="false" id="A1.SS2.p3.2.m2.1.1.1.1.1.1" xref="A1.SS2.p3.2.m2.1.1.1.1.1.1.cmml">
           |
          </mo>
          <mi id="A1.SS2.p3.2.m2.1.1.1.1.1.3" xref="A1.SS2.p3.2.m2.1.1.1.1.1.3.cmml">
           s
          </mi>
         </mrow>
         <mo id="A1.SS2.p3.2.m2.1.1.1.1.3" stretchy="false" xref="A1.SS2.p3.2.m2.1.1.1.1.1.cmml">
          )
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A1.SS2.p3.2.m2.1b">
        <apply id="A1.SS2.p3.2.m2.1.1.cmml" xref="A1.SS2.p3.2.m2.1.1">
         <times id="A1.SS2.p3.2.m2.1.1.2.cmml" xref="A1.SS2.p3.2.m2.1.1.2">
         </times>
         <apply id="A1.SS2.p3.2.m2.1.1.3.cmml" xref="A1.SS2.p3.2.m2.1.1.3">
          <csymbol cd="ambiguous" id="A1.SS2.p3.2.m2.1.1.3.1.cmml" xref="A1.SS2.p3.2.m2.1.1.3">
           subscript
          </csymbol>
          <ci id="A1.SS2.p3.2.m2.1.1.3.2.cmml" xref="A1.SS2.p3.2.m2.1.1.3.2">
           ùúã
          </ci>
          <ci id="A1.SS2.p3.2.m2.1.1.3.3.cmml" xref="A1.SS2.p3.2.m2.1.1.3.3">
           ùúÉ
          </ci>
         </apply>
         <apply id="A1.SS2.p3.2.m2.1.1.1.1.1.cmml" xref="A1.SS2.p3.2.m2.1.1.1.1">
          <csymbol cd="latexml" id="A1.SS2.p3.2.m2.1.1.1.1.1.1.cmml" xref="A1.SS2.p3.2.m2.1.1.1.1.1.1">
           conditional
          </csymbol>
          <ci id="A1.SS2.p3.2.m2.1.1.1.1.1.2.cmml" xref="A1.SS2.p3.2.m2.1.1.1.1.1.2">
           ùëé
          </ci>
          <ci id="A1.SS2.p3.2.m2.1.1.1.1.1.3.cmml" xref="A1.SS2.p3.2.m2.1.1.1.1.1.3">
           ùë†
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS2.p3.2.m2.1c">
        \pi_{\theta}(a|s)
       </annotation>
      </semantics>
     </math>
     denotes the predicted action probability distribution given a state
     <math alttext="s" class="ltx_Math" display="inline" id="A1.SS2.p3.3.m3.1">
      <semantics id="A1.SS2.p3.3.m3.1a">
       <mi id="A1.SS2.p3.3.m3.1.1" xref="A1.SS2.p3.3.m3.1.1.cmml">
        s
       </mi>
       <annotation-xml encoding="MathML-Content" id="A1.SS2.p3.3.m3.1b">
        <ci id="A1.SS2.p3.3.m3.1.1.cmml" xref="A1.SS2.p3.3.m3.1.1">
         ùë†
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS2.p3.3.m3.1c">
        s
       </annotation>
      </semantics>
     </math>
     , and
     <math alttext="P^{*}(a|s)" class="ltx_Math" display="inline" id="A1.SS2.p3.4.m4.1">
      <semantics id="A1.SS2.p3.4.m4.1a">
       <mrow id="A1.SS2.p3.4.m4.1.1" xref="A1.SS2.p3.4.m4.1.1.cmml">
        <msup id="A1.SS2.p3.4.m4.1.1.3" xref="A1.SS2.p3.4.m4.1.1.3.cmml">
         <mi id="A1.SS2.p3.4.m4.1.1.3.2" xref="A1.SS2.p3.4.m4.1.1.3.2.cmml">
          P
         </mi>
         <mo id="A1.SS2.p3.4.m4.1.1.3.3" xref="A1.SS2.p3.4.m4.1.1.3.3.cmml">
          ‚àó
         </mo>
        </msup>
        <mo id="A1.SS2.p3.4.m4.1.1.2" lspace="0em" rspace="0em" xref="A1.SS2.p3.4.m4.1.1.2.cmml">
         ‚Äã
        </mo>
        <mrow id="A1.SS2.p3.4.m4.1.1.1.1" xref="A1.SS2.p3.4.m4.1.1.1.1.1.cmml">
         <mo id="A1.SS2.p3.4.m4.1.1.1.1.2" stretchy="false" xref="A1.SS2.p3.4.m4.1.1.1.1.1.cmml">
          (
         </mo>
         <mrow id="A1.SS2.p3.4.m4.1.1.1.1.1" xref="A1.SS2.p3.4.m4.1.1.1.1.1.cmml">
          <mi id="A1.SS2.p3.4.m4.1.1.1.1.1.2" xref="A1.SS2.p3.4.m4.1.1.1.1.1.2.cmml">
           a
          </mi>
          <mo fence="false" id="A1.SS2.p3.4.m4.1.1.1.1.1.1" xref="A1.SS2.p3.4.m4.1.1.1.1.1.1.cmml">
           |
          </mo>
          <mi id="A1.SS2.p3.4.m4.1.1.1.1.1.3" xref="A1.SS2.p3.4.m4.1.1.1.1.1.3.cmml">
           s
          </mi>
         </mrow>
         <mo id="A1.SS2.p3.4.m4.1.1.1.1.3" stretchy="false" xref="A1.SS2.p3.4.m4.1.1.1.1.1.cmml">
          )
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A1.SS2.p3.4.m4.1b">
        <apply id="A1.SS2.p3.4.m4.1.1.cmml" xref="A1.SS2.p3.4.m4.1.1">
         <times id="A1.SS2.p3.4.m4.1.1.2.cmml" xref="A1.SS2.p3.4.m4.1.1.2">
         </times>
         <apply id="A1.SS2.p3.4.m4.1.1.3.cmml" xref="A1.SS2.p3.4.m4.1.1.3">
          <csymbol cd="ambiguous" id="A1.SS2.p3.4.m4.1.1.3.1.cmml" xref="A1.SS2.p3.4.m4.1.1.3">
           superscript
          </csymbol>
          <ci id="A1.SS2.p3.4.m4.1.1.3.2.cmml" xref="A1.SS2.p3.4.m4.1.1.3.2">
           ùëÉ
          </ci>
          <times id="A1.SS2.p3.4.m4.1.1.3.3.cmml" xref="A1.SS2.p3.4.m4.1.1.3.3">
          </times>
         </apply>
         <apply id="A1.SS2.p3.4.m4.1.1.1.1.1.cmml" xref="A1.SS2.p3.4.m4.1.1.1.1">
          <csymbol cd="latexml" id="A1.SS2.p3.4.m4.1.1.1.1.1.1.cmml" xref="A1.SS2.p3.4.m4.1.1.1.1.1.1">
           conditional
          </csymbol>
          <ci id="A1.SS2.p3.4.m4.1.1.1.1.1.2.cmml" xref="A1.SS2.p3.4.m4.1.1.1.1.1.2">
           ùëé
          </ci>
          <ci id="A1.SS2.p3.4.m4.1.1.1.1.1.3.cmml" xref="A1.SS2.p3.4.m4.1.1.1.1.1.3">
           ùë†
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS2.p3.4.m4.1c">
        P^{*}(a|s)
       </annotation>
      </semantics>
     </math>
     represents the ground truth action probability distribution derived from the expert demonstrations.
    </p>
   </div>
   <div class="ltx_para" id="A1.SS2.p4">
    <p class="ltx_p" id="A1.SS2.p4.1">
     For Franka-kitchen tasks, the length of demonstration is 50, which contains 50 state-action pairs. For Meta-World tasks, the length of demonstration is 500, which contains 500 state-action pairs.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   More demos of EmbodiedGPT
  </h2>
  <section class="ltx_subsection" id="A2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.1
    </span>
    Visual Captioning
   </h3>
   <div class="ltx_para" id="A2.SS1.p1">
    <p class="ltx_p" id="A2.SS1.p1.1">
     We assessed EmbodiedGPT on numerous visual captioning tasks spanning a range of embodied AI benchmarks. As shown in Figure
     <a class="ltx_ref" href="#A2.F9" title="Figure 9 ‚Ä£ B.2 Embodied Planning with image input ‚Ä£ Appendix B More demos of EmbodiedGPT ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
      <span class="ltx_text ltx_ref_tag">
       9
      </span>
     </a>
     , the model displayed an exceptional ability to accurately describe objects, characters, and spatial relationships relevant to embodied AI tasks. Furthermore, EmbodiedGPT exhibited robust zero-shot learning capabilities, evidenced by its strong performance across multiple benchmarks without the need for task-specific fine-tuning.
    </p>
   </div>
   <figure class="ltx_figure" id="A2.F7">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_1">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F7.sf1">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="140" id="A2.F7.sf1.g1" src="/html/2305.15021/assets/x9.png" width="461"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="A2.F7.sf1.2.1.1" style="font-size:90%;">
          (a)
         </span>
        </span>
        <span class="ltx_text" id="A2.F7.sf1.3.2" style="font-size:90%;">
         Image caption in COCO dataset.
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_break">
     </div>
     <div class="ltx_flex_cell ltx_flex_size_1">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F7.sf2">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="150" id="A2.F7.sf2.g1" src="/html/2305.15021/assets/x10.png" width="461"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="A2.F7.sf2.2.1.1" style="font-size:90%;">
          (b)
         </span>
        </span>
        <span class="ltx_text" id="A2.F7.sf2.3.2" style="font-size:90%;">
         Embodied image caption in Franka Kitchen benchmark.
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_break">
     </div>
     <div class="ltx_flex_cell ltx_flex_size_1">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F7.sf3">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="143" id="A2.F7.sf3.g1" src="/html/2305.15021/assets/x11.png" width="461"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="A2.F7.sf3.2.1.1" style="font-size:90%;">
          (c)
         </span>
        </span>
        <span class="ltx_text" id="A2.F7.sf3.3.2" style="font-size:90%;">
         Embodied image caption in Habitat2.0 benchmark.
        </span>
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="A2.F7.2.1.1" style="font-size:90%;">
       Figure 7
      </span>
      :
     </span>
     <span class="ltx_text" id="A2.F7.3.2" style="font-size:90%;">
      Generation results in image caption tasks.
     </span>
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="A2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.2
    </span>
    Embodied Planning with image input
   </h3>
   <div class="ltx_para" id="A2.SS2.p1">
    <p class="ltx_p" id="A2.SS2.p1.1">
     <span class="ltx_text ltx_font_bold" id="A2.SS2.p1.1.1">
      Embodied Planning for Concrete Tasks (image input)
     </span>
     :
In the context of concrete task planning, such as making a cup of coffee, EmbodiedGPT effectively utilized visual information to pinpoint the required objects and their positional relationships within the environment. The model produced coherent, multi-step plans, taking into consideration both the task requirements and environmental constraints. This capability demonstrates the model‚Äôs potential to facilitate real-world applications and tackle complex planning challenges within the realm of embodied AI.
    </p>
   </div>
   <figure class="ltx_figure" id="A2.F8">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="191" id="A2.F8.g1" src="/html/2305.15021/assets/x12.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="A2.F8.2.1.1" style="font-size:90%;">
       Figure 8
      </span>
      :
     </span>
     <span class="ltx_text" id="A2.F8.3.2" style="font-size:90%;">
      Embodied planning of in real-world scenarios.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="A2.SS2.p2">
    <p class="ltx_p" id="A2.SS2.p2.1">
     <span class="ltx_text ltx_font_bold" id="A2.SS2.p2.1.1">
      Embodied Planning for Abstract Tasks
     </span>
     :
For abstract task scenarios, EmbodiedGPT adeptly combined visual observations with abstract concepts to generate concrete sub-task descriptions. For instance, when given the abstract prompt of feeling hot, the model identified pertinent objects in the environment (e.g., a fan) and suggested a practical solution (e.g., turning on the fan). Subsequently, the model generated a detailed plan to accomplish the identified sub-tasks, highlighting its adaptability across diverse problem-solving contexts.
    </p>
   </div>
   <figure class="ltx_figure" id="A2.F9">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_1">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F9.sf1">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="212" id="A2.F9.sf1.g1" src="/html/2305.15021/assets/x13.png" width="461"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="A2.F9.sf1.2.1.1" style="font-size:90%;">
          (a)
         </span>
        </span>
        <span class="ltx_text" id="A2.F9.sf1.3.2" style="font-size:90%;">
         Embodied planning for abstract tasks [meeting].
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_break">
     </div>
     <div class="ltx_flex_cell ltx_flex_size_1">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F9.sf2">
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="168" id="A2.F9.sf2.g1" src="/html/2305.15021/assets/x14.png" width="461"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="A2.F9.sf2.2.1.1" style="font-size:90%;">
          (b)
         </span>
        </span>
        <span class="ltx_text" id="A2.F9.sf2.3.2" style="font-size:90%;">
         Embodied planning for abstract tasks[feel hot].
        </span>
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="A2.F9.2.1.1" style="font-size:90%;">
       Figure 9
      </span>
      :
     </span>
     <span class="ltx_text" id="A2.F9.3.2" style="font-size:90%;">
      Embodied planning for abstract tasks.
     </span>
    </figcaption>
   </figure>
  </section>
 </section>
 <section class="ltx_appendix" id="A3">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix C
   </span>
   Evaluation metric and scoring criteria for user study
  </h2>
  <div class="ltx_para" id="A3.p1">
   <p class="ltx_p" id="A3.p1.1">
    We show the Table
    <a class="ltx_ref" href="#A3.T3" title="Table 3 ‚Ä£ Appendix C Evaluation metric and scoring criteria for user study ‚Ä£ EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    to outline the scoring criteria for a user study incorporating the above five evaluation metrics:
   </p>
  </div>
  <figure class="ltx_table" id="A3.T3">
   <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T3.2">
    <thead class="ltx_thead">
     <tr class="ltx_tr" id="A3.T3.2.1.1">
      <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A3.T3.2.1.1.1">
       <span class="ltx_inline-block ltx_align_top" id="A3.T3.2.1.1.1.1">
        <span class="ltx_p" id="A3.T3.2.1.1.1.1.1" style="width:85.4pt;">
         <span class="ltx_text ltx_font_bold" id="A3.T3.2.1.1.1.1.1.1">
          Evaluation Metric
         </span>
        </span>
       </span>
      </th>
      <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="A3.T3.2.1.1.2">
       <span class="ltx_inline-block ltx_align_top" id="A3.T3.2.1.1.2.1">
        <span class="ltx_p" id="A3.T3.2.1.1.2.1.1" style="width:284.5pt;">
         <span class="ltx_text ltx_font_bold" id="A3.T3.2.1.1.2.1.1.1">
          Explanation
         </span>
        </span>
       </span>
      </th>
     </tr>
    </thead>
    <tbody class="ltx_tbody">
     <tr class="ltx_tr" id="A3.T3.2.2.1">
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A3.T3.2.2.1.1">
       <span class="ltx_inline-block ltx_align_top" id="A3.T3.2.2.1.1.1">
        <span class="ltx_p" id="A3.T3.2.2.1.1.1.1" style="width:85.4pt;">
         Object Recognition Accuracy
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.T3.2.2.1.2">
       <span class="ltx_inline-block ltx_align_top" id="A3.T3.2.2.1.2.1">
        <span class="ltx_p" id="A3.T3.2.2.1.2.1.1" style="width:284.5pt;">
         This metric measures the ability of a system to accurately identify objects from images or videos. A higher accuracy indicates that the system can correctly recognize the objects present in the given visual data.
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T3.2.3.2">
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A3.T3.2.3.2.1">
       <span class="ltx_inline-block ltx_align_top" id="A3.T3.2.3.2.1.1">
        <span class="ltx_p" id="A3.T3.2.3.2.1.1.1" style="width:85.4pt;">
         Spatial Relationship Understanding
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.T3.2.3.2.2">
       <span class="ltx_inline-block ltx_align_top" id="A3.T3.2.3.2.2.1">
        <span class="ltx_p" id="A3.T3.2.3.2.2.1.1" style="width:284.5pt;">
         Spatial relationship understanding refers to the system‚Äôs capability to accurately discern the spatial relationships between objects in a scene. It evaluates whether the system can determine the relative positions, orientations, distances, and other spatial attributes of objects with precision.
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T3.2.4.3">
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A3.T3.2.4.3.1">
       <span class="ltx_inline-block ltx_align_top" id="A3.T3.2.4.3.1.1">
        <span class="ltx_p" id="A3.T3.2.4.3.1.1.1" style="width:85.4pt;">
         Level of Redundancy in the Answer
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.T3.2.4.3.2">
       <span class="ltx_inline-block ltx_align_top" id="A3.T3.2.4.3.2.1">
        <span class="ltx_p" id="A3.T3.2.4.3.2.1.1" style="width:284.5pt;">
         The level of redundancy in the answer assesses the amount of unnecessary or repetitive information present in the system‚Äôs response. Lower redundancy indicates that the system provides concise and non-repetitive answers, which is generally preferred as it reduces verbosity and improves clarity.
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T3.2.5.4">
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A3.T3.2.5.4.1">
       <span class="ltx_inline-block ltx_align_top" id="A3.T3.2.5.4.1.1">
        <span class="ltx_p" id="A3.T3.2.5.4.1.1.1" style="width:85.4pt;">
         Reasonability of the Planning
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.T3.2.5.4.2">
       <span class="ltx_inline-block ltx_align_top" id="A3.T3.2.5.4.2.1">
        <span class="ltx_p" id="A3.T3.2.5.4.2.1.1" style="width:284.5pt;">
         The reasonability of the planning metric gauges the logical coherence and appropriateness of the system‚Äôs planning process. It examines whether the system‚Äôs generated plans are sensible and align with the given goals or objectives.
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T3.2.6.5">
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t" id="A3.T3.2.6.5.1">
       <span class="ltx_inline-block ltx_align_top" id="A3.T3.2.6.5.1.1">
        <span class="ltx_p" id="A3.T3.2.6.5.1.1.1" style="width:85.4pt;">
         Executability of the Planning
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="A3.T3.2.6.5.2">
       <span class="ltx_inline-block ltx_align_top" id="A3.T3.2.6.5.2.1">
        <span class="ltx_p" id="A3.T3.2.6.5.2.1.1" style="width:284.5pt;">
         This metric evaluates the feasibility and practicality of the system‚Äôs generated plans. It assesses whether the plans can be executed successfully in the real world.
        </span>
       </span>
      </td>
     </tr>
    </tbody>
   </table>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_table">
     <span class="ltx_text" id="A3.T3.3.1.1" style="font-size:90%;">
      Table 3
     </span>
     :
    </span>
    <span class="ltx_text" id="A3.T3.4.2" style="font-size:90%;">
     Explanation of Evaluation Metric
    </span>
   </figcaption>
  </figure>
 </section>
 <section class="ltx_appendix" id="A4">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix D
   </span>
   Insight about the prompt designing for multi-modal large model
  </h2>
  <div class="ltx_para" id="A4.p1">
   <p class="ltx_p" id="A4.p1.1">
    Through extensive experiments evaluating multi-modal large models in question answering tasks, we have identified a limitation in their ability to effectively utilize visual information. These models tend to rely heavily on textual inputs and may not fully exploit the visual cues present in the question-answering process. In order to address this limitation and enhance their performance, we aimed to investigate the impact of incorporating additional prompts that guide the model‚Äôs attention towards the visual content.
   </p>
  </div>
  <div class="ltx_para" id="A4.p2">
   <p class="ltx_p" id="A4.p2.1">
    To achieve this goal, we propose a straightforward yet highly effective approach: incorporating additional prompting into the model‚Äôs input. This involves introducing specific prompts such as "in the scene shown in this image/video" or allowing the model to describe the image/video as part of a multi-turn dialogue. By including these prompts, we aim to explicitly direct the model‚Äôs focus towards the visual information available and encourage it to utilize this information when generating answers.
Our experiments have yielded promising results. The introduction of additional prompts has significantly improved the model‚Äôs ability to leverage visual information and provide accurate answers based on the visual content. By explicitly referencing the scene depicted in the image or video, the model‚Äôs attention is directed towards the relevant visual features, leading to a more comprehensive integration of visual and textual information. Consequently, the model‚Äôs reasoning ability is enhanced, resulting in more precise and contextually grounded answers.
   </p>
  </div>
  <figure class="ltx_figure" id="A4.F10">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_1">
     <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A4.F10.sf1">
      <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="86" id="A4.F10.sf1.g1" src="/html/2305.15021/assets/x15.png" width="461"/>
      <figcaption class="ltx_caption ltx_centering">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="A4.F10.sf1.2.1.1" style="font-size:90%;">
         (a)
        </span>
       </span>
       <span class="ltx_text" id="A4.F10.sf1.3.2" style="font-size:90%;">
        Ask EmbodiedGPT write the plan directly.
       </span>
      </figcaption>
     </figure>
    </div>
    <div class="ltx_flex_break">
    </div>
    <div class="ltx_flex_cell ltx_flex_size_1">
     <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A4.F10.sf2">
      <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="103" id="A4.F10.sf2.g1" src="/html/2305.15021/assets/x16.png" width="461"/>
      <figcaption class="ltx_caption ltx_centering">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="A4.F10.sf2.3.1.1" style="font-size:90%;">
         (b)
        </span>
       </span>
       <span class="ltx_text" id="A4.F10.sf2.4.2" style="font-size:90%;">
        Ask EmbodiedGPT write the plan directly with
        <span class="ltx_text ltx_font_bold ltx_font_italic" id="A4.F10.sf2.4.2.1">
         image-related chain-of-thought
        </span>
        .
       </span>
      </figcaption>
     </figure>
    </div>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="A4.F10.2.1.1" style="font-size:90%;">
      Figure 10
     </span>
     :
    </span>
    <span class="ltx_text" id="A4.F10.3.2" style="font-size:90%;">
     Performance Comparison with different types prompt.
    </span>
   </figcaption>
  </figure>
 </section>
</article>

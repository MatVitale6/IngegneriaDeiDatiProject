<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  Wireless Multi-Agent Generative AI: From Connected Intelligence to Collective Intelligence
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Hang Zou, Qiyang Zhao, Lina Bariah, Mehdi Bennis, and Mérouane Debbah
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id1.id1">
   The convergence of generative large language models (LLMs), edge networks, and multi-agent systems represents a groundbreaking synergy that holds immense promise for future wireless generations, harnessing the power of collective intelligence and paving the way for self-governed networks where intelligent decision-making happens right at the edge. This article puts the stepping-stone for incorporating multi-agent generative artificial intelligence (AI) in wireless networks, and sets the scene for realizing on-device LLMs, where multi-agent LLMs are collaboratively planning and solving tasks to achieve a number of network goals. We further investigate the profound limitations of cloud-based LLMs, and explore multi-agent LLMs from a game theoretic perspective, where agents collaboratively solve tasks in competitive environments. Moreover, we establish the underpinnings for the architecture design of wireless multi-agent generative AI systems at the network level and the agent level, and we identify the wireless technologies that are envisioned to play a key role in enabling on-device LLM. To demonstrate the promising potentials of wireless multi-agent generative AI networks, we highlight the benefits that can be achieved when implementing wireless generative agents in intent-based networking, and we provide a case study to showcase how on-device LLMs can contribute to solving network intents in a collaborative fashion. We finally shed lights on potential challenges and sketch a research roadmap towards realizing the vision of wireless collective intelligence.
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    I
   </span>
   <span class="ltx_text ltx_font_smallcaps" id="S1.1.1">
    Introduction
   </span>
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Generative
    <span class="ltx_glossaryref" title="artificial intelligence">
     <span class="ltx_text ltx_glossary_long">
      artificial intelligence
     </span>
    </span>
    (
    <abbr class="ltx_glossaryref" title="artificial intelligence">
     <span class="ltx_text ltx_glossary_short">
      AI
     </span>
    </abbr>
    ) has evolved as a powerful branch of
    <abbr class="ltx_glossaryref" title="artificial intelligence">
     <span class="ltx_text ltx_glossary_short">
      AI
     </span>
    </abbr>
    , endowing machines with algorithms that enable them to create original content, such as images, text, and human-like conversations. In particular,
    <span class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_long-plural">
      Large Language Models
     </span>
    </span>
    , a particular model of generative
    <abbr class="ltx_glossaryref" title="artificial intelligence">
     <span class="ltx_text ltx_glossary_short">
      AI
     </span>
    </abbr>
    that are trained on a massive unlabeled textual dataset, have shown impressive capabilities in many applications, such as question answering, language understanding, reading comprehension, code generation, mathematical and common sense reasoning
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    . As a promising example of LLMs, the recent release of ChatGPT, based on the
    <span class="ltx_glossaryref" title="generative pre-trained transformers">
     <span class="ltx_text ltx_glossary_long">
      generative pre-trained transformers
     </span>
    </span>
    (
    <abbr class="ltx_glossaryref" title="generative pre-trained transformers">
     <span class="ltx_text ltx_glossary_short">
      GPT
     </span>
    </abbr>
    ) with 175B parameters (GPT-3) and 1 trillion parameters (GPT-4), has showcased remarkable capabilities of large-scale LLMs in content generation tasks. ChatGPT plugins allows
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    to access up-to-date information, execute tasks, while interacting with the real-world.
More recently, the open-source Falcon
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib2" title="">
      2
     </a>
     ]
    </cite>
    has outperformed
    <abbr class="ltx_glossaryref" title="generative pre-trained transformers">
     <span class="ltx_text ltx_glossary_short">
      GPT
     </span>
    </abbr>
    -3 with only 40B parameters, 75% training cost, and 20% inference compute budget; Similar results were achieved by LLaMA with 65B parameters
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib3" title="">
      3
     </a>
     ]
    </cite>
    . On the other hand, PaLM-2 model has excelled at advanced reasoning tasks including code generation and maths
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib4" title="">
      4
     </a>
     ]
    </cite>
    . Such outstanding results of various LLMs has paved the way for integrating
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    in different domains, such as robotics, telecom, and healthcare
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    .
The maturity of LLMs constitutes a stepping stone towards realizing the vision of
    <span class="ltx_glossaryref" title="artificial general intelligence">
     <span class="ltx_text ltx_glossary_long">
      artificial general intelligence
     </span>
    </span>
    (
    <abbr class="ltx_glossaryref" title="artificial general intelligence">
     <span class="ltx_text ltx_glossary_short">
      AGI
     </span>
    </abbr>
    ), which is a broader concept than
    <abbr class="ltx_glossaryref" title="artificial intelligence">
     <span class="ltx_text ltx_glossary_short">
      AI
     </span>
    </abbr>
    , that encompasses highly autonomous systems of machines that possess general intelligence and cognition capabilities that are comparable to humans. Within this context,
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    will play an essential role in
    <abbr class="ltx_glossaryref" title="artificial general intelligence">
     <span class="ltx_text ltx_glossary_short">
      AGI
     </span>
    </abbr>
    -based systems through their abilities to perform complex tasks on multi-modal data across many domains, with only a few examples
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ]
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    The remarkable capabilities of LLMs are owed to the Transformer architecture, rooted in the self-attention mechanism
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib6" title="">
      6
     </a>
     ]
    </cite>
    . Leveraging multi-head attention, Transformers capture long-range dependencies in parallel by attending a word to all previous words, or to all other words in the text.
With pre-training on huge unlabeled corpus, generative models can learn universal knowledge representation, and can be further tuned to narrow their scopes into a particular domain. Tuning generative LLMs on specific tasks or domains can be done via 1) fine-tuning by adapting pre-trained weights to task-dependent loss or domain-specific dataset; 2) prompt-tuning by applying task description or instruction with a few shot examples to help
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    to understand the task; 3) instruct-tuning by
    <span class="ltx_glossaryref" title="reinforcement learning">
     <span class="ltx_text ltx_glossary_long">
      reinforcement learning
     </span>
    </span>
    (
    <abbr class="ltx_glossaryref" title="reinforcement learning">
     <span class="ltx_text ltx_glossary_short">
      RL
     </span>
    </abbr>
    ) to ground
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short">
      LLM
     </span>
    </abbr>
    into real-world context.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    Accordingly, LLMs running on wireless devices should understand domain specific knowledge and interact effectively with the environment to perform specific tasks. Grounding LLMs in a real-world context with modular and causal knowledge is crucial to produce scalable and lightweight on-device LLMs. Such knowledge should be transferred in wireless networks and encoded into on-device LLMs, to perform logical decisions
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ]
    </cite>
    . This includes planning, that breaks down high-level goals into low-level tasks and reasoning, that deconstructs complex problems into priors and beliefs. Grounding, planning, and reasoning allow on-device
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    to act as autonomous agents.
Wireless networks represent a promising field for the deployment of generative multi-agent system, where multiple on-device
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    plan and solve tasks in a collaborative manner. Specifically, multi-agent planning is essential to decompose a complex task that requires multi-modality sensors and multi-task executors across different wireless devices. Moreover, since on-device LLM encodes specific knowledge due to resource constraints, collective intelligence from multiple
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    is essential to effectively complete complex tasks. To achieve this, generative multi-agent games can be applied with
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    to solve advanced multi-task reasoning problems
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib8" title="">
      8
     </a>
     ]
    </cite>
    . Furthermore,
    <abbr class="ltx_glossaryref" title="Large Language Model">
     <span class="ltx_text ltx_glossary_short-plural">
      LLMs
     </span>
    </abbr>
    can be used with multi-agent
    <abbr class="ltx_glossaryref" title="reinforcement learning">
     <span class="ltx_text ltx_glossary_short">
      RL
     </span>
    </abbr>
    to learn optimal collaborative goal-oriented behaviours.
   </p>
  </div>
  <section class="ltx_subsection" id="S1.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     <span class="ltx_text" id="S1.SS1.4.1.1">
      I-A
     </span>
    </span>
    <span class="ltx_text ltx_font_italic" id="S1.SS1.5.2">
     Related work
    </span>
   </h3>
   <div class="ltx_para" id="S1.SS1.p1">
    <p class="ltx_p" id="S1.SS1.p1.1">
     The release of ChatGPT has led to a fast growing research in autonomous and generative agents. Several recently emerged frameworks expand on Chat
     <abbr class="ltx_glossaryref" title="generative pre-trained transformers">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     from autonomous task planning and reasoning, to multi-agent generative systems. As an example, BabyAGI
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib9" title="">
       9
      </a>
      ]
     </cite>
     is a task-driven autonomous agent built on
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     , that can generate, execute and prioritize tasks in real-time.
Similarly, Auto-GPT
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib10" title="">
       10
      </a>
      ]
     </cite>
     is an AI agent that attempts to achieve a goal specified in natural language.
It chains together
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     ”thoughts” in an infinite loop of reasoning, and planning the next action.
AgentGPT is a user interface powered by the idea of BabyAGI and AutoGPT.
Another line of work represented by HuggingGPT in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib11" title="">
       11
      </a>
      ]
     </cite>
     developed a collaborative system utilizing multiple AI models to complete a task. In HuggingGPT,
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     use language as an interface to connect numerous AI models in Hugging Face for solving complicated tasks.
In this framework,
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     act as a controller to manage and organize cooperation of expert AI models, which pushes the boundary of
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     to applications requiring domain specific knowledge.
    </p>
   </div>
   <div class="ltx_para" id="S1.SS1.p2">
    <p class="ltx_p" id="S1.SS1.p2.1">
     Furthermore, several frameworks on multi-agent LLM have been developed. A role-playing communicative agent framework called CAMEL has been developed in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib12" title="">
       12
      </a>
      ]
     </cite>
     , in which an agent receives an idea to implement from a human, then creates an AI assistant and an AI user agent.
The AI user assigns tasks for AI assistant to execute, and plans new tasks according to the results. The two agents collaboratively communicate by chatting with each other to solve the specified task. Moreover, a generative agent framework in a “west world” simulation has been developed in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib13" title="">
       13
      </a>
      ]
     </cite>
     . It is a sandbox of multiple AI agents that can interact with each other and simulate human behavior. The agent can observe the environment, plan a sequence of actions to execute, create high-level reflections of observation in a memory stream and formulate long-term plans.
The agents communicate in full natural language using
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     , to collaboratively complete a planned task.
    </p>
   </div>
   <div class="ltx_para" id="S1.SS1.p3">
    <p class="ltx_p" id="S1.SS1.p3.1">
     The
     <span class="ltx_glossaryref" title="state-of-the art">
      <span class="ltx_text ltx_glossary_long">
       state-of-the art
      </span>
     </span>
     (
     <abbr class="ltx_glossaryref" title="state-of-the art">
      <span class="ltx_text ltx_glossary_short">
       SOTA
      </span>
     </abbr>
     ) of generative agents explores multiple
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     collaboratively planing and solving tasks. However, most works are executed in the cloud or in simulation environments, where the cost of communication, computing, and storage is ignored. In wireless generative agent networks,
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     models and their inter-agent communication requires a new system design and optimization, which is the goal of this paper.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S1.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     <span class="ltx_text" id="S1.SS2.4.1.1">
      I-B
     </span>
    </span>
    <span class="ltx_text ltx_font_italic" id="S1.SS2.5.2">
     Contributions
    </span>
   </h3>
   <div class="ltx_para" id="S1.SS2.p1">
    <p class="ltx_p" id="S1.SS2.p1.1">
     Motivated by the above discussion, in this article we establish the initial foundation for integrating the generative agents paradigm in wireless networks. In particular, we lay the first building block toward realizing collective intelligence in 6G, where we articulate the collaborative role of multiple on-device LLMs in solving Telecom tasks, and we put a future-oriented perspective on the architecture of multi-agent LLM-enabled wireless network. We further provide insights into the technologies that have manifested themselves as enablers for multi-agent LLMs in wireless networks, focusing on: 1) multi-agent LLM planning and reasoning to break down high-level goals into low-level tasks; 2) multi-agent LLM games and
     <abbr class="ltx_glossaryref" title="reinforcement learning">
      <span class="ltx_text ltx_glossary_short">
       RL
      </span>
     </abbr>
     to learn the optimal collaborative behaviours from competing actors to achieve a goal; 3) semantic communication that transfers knowledge in the network for system 2-type
     <span class="ltx_note ltx_role_footnote" id="footnote1">
      <sup class="ltx_note_mark">
       1
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         1
        </sup>
        <span class="ltx_tag ltx_tag_note">
         1
        </span>
        System 2-type model refers to human-like cognition, based on reasoning and logical deduction, for solving complex problems and making calculated decisions.
       </span>
      </span>
     </span>
     on-device LLMs. With the aim to demonstrate its promising potential, we shed lights on the anticipated applications of generative agents in wireless networks, and we present a case study for showcasing a scenario where reasoning in multi-agent LLM is required to achieve a network-level energy saving goal and guaranteeing users’ transmission rates. Finally, we explore future research directions for the successful realization of collective intelligence through on-device LLM.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    II
   </span>
   <span class="ltx_text ltx_font_smallcaps" id="S2.1.1">
    From Cloud LLM to Massive On-Device LLMs
   </span>
  </h2>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     <span class="ltx_text" id="S2.SS1.4.1.1">
      II-A
     </span>
    </span>
    <span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">
     Recent advances in cloud-based
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     s
    </span>
   </h3>
   <div class="ltx_para" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     From an architectural perspective, the evolution of
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     can be classified as encoder-only (BERT-like), decoder-only (GPT-like), and encoder-decoder (T5). The decoder-only
     <abbr class="ltx_glossaryref" title="generative pre-trained transformers">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     merely utilizes the
     <span class="ltx_glossaryref" title="auto-regressive">
      <span class="ltx_text ltx_glossary_long">
       auto-regressive
      </span>
     </span>
     (
     <abbr class="ltx_glossaryref" title="auto-regressive">
      <span class="ltx_text ltx_glossary_short">
       AR
      </span>
     </abbr>
     ) transformer with multiple stacked transformer decoder layers, equipped with masked self-attention. This mechanism enables
     <abbr class="ltx_glossaryref" title="generative pre-trained transformers">
      <span class="ltx_text ltx_glossary_short">
       GPT
      </span>
     </abbr>
     -like
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     to attend all previous tokens to predict the subsequent one. GPT-4 (with plugs-in and web browsing) was reported to successfully solve the top 10% of various academic and professional exams with improved reasoning capability. On the other hand, a number of lightweight
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     s have been released, such as LLaMA and Falcon. These models have fewer parameters (Falcon-1B) and a smaller number of pre-trained tokens (1T), yielding the same performance as larger models
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib2" title="">
       2
      </a>
      ]
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p2">
    <p class="ltx_p" id="S2.SS1.p2.1">
     The encoder-only BERT is pre-trained on
     <span class="ltx_glossaryref" title="masked language modeling">
      <span class="ltx_text ltx_glossary_long">
       masked language modeling
      </span>
     </span>
     (
     <abbr class="ltx_glossaryref" title="masked language modeling">
      <span class="ltx_text ltx_glossary_short">
       MLM
      </span>
     </abbr>
     ) or
     <span class="ltx_glossaryref" title="next sentence prediction">
      <span class="ltx_text ltx_glossary_long">
       next sentence prediction
      </span>
     </span>
     (
     <abbr class="ltx_glossaryref" title="next sentence prediction">
      <span class="ltx_text ltx_glossary_short">
       NSP
      </span>
     </abbr>
     ) tasks. Specifically, multiple tokens from a sequence are masked to force the model to acquire bidirectional contextual information during the pre-training process. The non-
     <abbr class="ltx_glossaryref" title="auto-regressive">
      <span class="ltx_text ltx_glossary_short">
       AR
      </span>
     </abbr>
     nature of
     <abbr class="ltx_glossaryref" title="masked language modeling">
      <span class="ltx_text ltx_glossary_short">
       MLM
      </span>
     </abbr>
     allows faster and paralleled computing by dynamically unfolding the masked tokens of all positions.
Enhanced versions of BERT can be achieved by optimizing the hyper-parameters and the pre-training tasks, i.e., RoBERTa and ALBERT, respectively.
On the other hand, the encoder-decoder architecture, such as T5, converts diverse tasks that involve generating sequences or text into a text-to-text framework for pre-training. However, this architecture is computationally expensive to train, and has limited contextual understanding and reasoning.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p3">
    <p class="ltx_p" id="S2.SS1.p3.1">
     Following the success of text-based
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     , multi-modal
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     have emerged to proliferate the potential use-cases of
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     . Among others, DALL-E is a zero-shot text to image generative model
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib14" title="">
       14
      </a>
      ]
     </cite>
     , with 2 pre-training stages. During the first stage, a discrete
     <span class="ltx_glossaryref" title="variational autoencoder">
      <span class="ltx_text ltx_glossary_long">
       variational autoencoder
      </span>
     </span>
     (
     <abbr class="ltx_glossaryref" title="variational autoencoder">
      <span class="ltx_text ltx_glossary_short">
       VAE
      </span>
     </abbr>
     ) is trained to compress image into grid of tokens. While in the second stage, the image tokens are concatenated with text tokens, in order to train an
     <abbr class="ltx_glossaryref" title="auto-regressive">
      <span class="ltx_text ltx_glossary_short">
       AR
      </span>
     </abbr>
     transformer to model the joint distribution over the text and image tokens. With an enhanced approach, DALL-E2
utilizes a CLIP similarity matrix to train a text encoder and an image encoder. The text embedding is then passed through a diffusion or
     <abbr class="ltx_glossaryref" title="auto-regressive">
      <span class="ltx_text ltx_glossary_short">
       AR
      </span>
     </abbr>
     models prior to produce an image embedding and a diffusion decoder to generate images
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib15" title="">
       15
      </a>
      ]
     </cite>
     . A comparison of common
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     s are given in Table
     <a class="ltx_ref" href="#S2.T1" title="TABLE I ‣ II-A Recent advances in cloud-based s ‣ II From Cloud LLM to Massive On-Device LLMs ‣ Wireless Multi-Agent Generative AI: From Connected Intelligence to Collective Intelligence">
      <span class="ltx_text ltx_ref_tag">
       I
      </span>
     </a>
     .
    </p>
   </div>
   <figure class="ltx_table" id="S2.T1">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      TABLE I:
     </span>
     Comparison of typical
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     s
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.8">
     <tr class="ltx_tr" id="S2.T1.8.9">
      <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.8.9.1">
       <span class="ltx_text ltx_font_bold" id="S2.T1.8.9.1.1">
        Model
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.9.2">
       <span class="ltx_text ltx_font_bold" id="S2.T1.8.9.2.1">
        Parameters
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.9.3">
       <span class="ltx_text" id="S2.T1.8.9.3.1">
       </span>
       <span class="ltx_text ltx_font_bold" id="S2.T1.8.9.3.2">
        <span class="ltx_text" id="S2.T1.8.9.3.2.1">
         <span class="ltx_tabular ltx_align_middle" id="S2.T1.8.9.3.2.1.1">
          <span class="ltx_tr" id="S2.T1.8.9.3.2.1.1.1">
           <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.9.3.2.1.1.1.1">
            Train data
           </span>
          </span>
         </span>
        </span>
        <span class="ltx_text" id="S2.T1.8.9.3.2.2">
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.9.4">
       <span class="ltx_text" id="S2.T1.8.9.4.1">
       </span>
       <span class="ltx_text ltx_font_bold" id="S2.T1.8.9.4.2">
        <span class="ltx_text" id="S2.T1.8.9.4.2.1">
         <span class="ltx_tabular ltx_align_middle" id="S2.T1.8.9.4.2.1.1">
          <span class="ltx_tr" id="S2.T1.8.9.4.2.1.1.1">
           <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.9.4.2.1.1.1.1">
            Train energy
           </span>
          </span>
         </span>
        </span>
        <span class="ltx_text" id="S2.T1.8.9.4.2.2">
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.9.5">
       <span class="ltx_text" id="S2.T1.8.9.5.1">
       </span>
       <span class="ltx_text ltx_font_bold" id="S2.T1.8.9.5.2">
        <span class="ltx_text" id="S2.T1.8.9.5.2.1">
         <span class="ltx_tabular ltx_align_middle" id="S2.T1.8.9.5.2.1.1">
          <span class="ltx_tr" id="S2.T1.8.9.5.2.1.1.1">
           <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.9.5.2.1.1.1.1">
            Inference FLOPs
           </span>
          </span>
         </span>
        </span>
        <span class="ltx_text" id="S2.T1.8.9.5.2.2">
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.9.6">
       <span class="ltx_text ltx_font_bold" id="S2.T1.8.9.6.1">
        Architecture
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.9.7">
       <span class="ltx_text ltx_font_bold" id="S2.T1.8.9.7.1">
        Modality
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.9.8">
       <span class="ltx_text ltx_font_bold" id="S2.T1.8.9.8.1">
        Pretrain Tasks
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.1.1">
      <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.1.2">
       GPT-3
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.3">
       175B
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.4">
       300B tokens
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.5">
       1287MWh
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1">
       <math alttext="7.4\times 10^{14}" class="ltx_Math" display="inline" id="S2.T1.1.1.1.m1.1">
        <semantics id="S2.T1.1.1.1.m1.1a">
         <mrow id="S2.T1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.m1.1.1.cmml">
          <mn id="S2.T1.1.1.1.m1.1.1.2" xref="S2.T1.1.1.1.m1.1.1.2.cmml">
           7.4
          </mn>
          <mo id="S2.T1.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.T1.1.1.1.m1.1.1.1.cmml">
           ×
          </mo>
          <msup id="S2.T1.1.1.1.m1.1.1.3" xref="S2.T1.1.1.1.m1.1.1.3.cmml">
           <mn id="S2.T1.1.1.1.m1.1.1.3.2" xref="S2.T1.1.1.1.m1.1.1.3.2.cmml">
            10
           </mn>
           <mn id="S2.T1.1.1.1.m1.1.1.3.3" xref="S2.T1.1.1.1.m1.1.1.3.3.cmml">
            14
           </mn>
          </msup>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.m1.1b">
          <apply id="S2.T1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1">
           <times id="S2.T1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1.1">
           </times>
           <cn id="S2.T1.1.1.1.m1.1.1.2.cmml" type="float" xref="S2.T1.1.1.1.m1.1.1.2">
            7.4
           </cn>
           <apply id="S2.T1.1.1.1.m1.1.1.3.cmml" xref="S2.T1.1.1.1.m1.1.1.3">
            <csymbol cd="ambiguous" id="S2.T1.1.1.1.m1.1.1.3.1.cmml" xref="S2.T1.1.1.1.m1.1.1.3">
             superscript
            </csymbol>
            <cn id="S2.T1.1.1.1.m1.1.1.3.2.cmml" type="integer" xref="S2.T1.1.1.1.m1.1.1.3.2">
             10
            </cn>
            <cn id="S2.T1.1.1.1.m1.1.1.3.3.cmml" type="integer" xref="S2.T1.1.1.1.m1.1.1.3.3">
             14
            </cn>
           </apply>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S2.T1.1.1.1.m1.1c">
          7.4\times 10^{14}
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.6">
       Decoder
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.7">
       Text
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.8">
       MLM (AR)
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.8.10">
      <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.8.10.1">
       GPT-4
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.10.2">
       170T
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.10.3">
       10T tokens
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.10.4">
       7500MWh
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.10.5">
       —
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.10.6">
       Decoder
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.10.7">
       <span class="ltx_text" id="S2.T1.8.10.7.1">
       </span>
       <span class="ltx_text" id="S2.T1.8.10.7.2">
        <span class="ltx_tabular ltx_align_middle" id="S2.T1.8.10.7.2.1">
         <span class="ltx_tr" id="S2.T1.8.10.7.2.1.1">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.10.7.2.1.1.1">
           Text, Image
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_text" id="S2.T1.8.10.7.3">
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.10.8">
       MLM (AR)
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.3.3">
      <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.3.3.3">
       Falcon
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.3.3.4">
       1B - 40B
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.3.3.5">
       1T tokens
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.2.2.1">
       <math alttext="\approx" class="ltx_Math" display="inline" id="S2.T1.2.2.1.m1.1">
        <semantics id="S2.T1.2.2.1.m1.1a">
         <mo id="S2.T1.2.2.1.m1.1.1" xref="S2.T1.2.2.1.m1.1.1.cmml">
          ≈
         </mo>
         <annotation-xml encoding="MathML-Content" id="S2.T1.2.2.1.m1.1b">
          <approx id="S2.T1.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.1.m1.1.1">
          </approx>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S2.T1.2.2.1.m1.1c">
          \approx
         </annotation>
        </semantics>
       </math>
       960MWh
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.3.3.2">
       <math alttext="\approx 1.48\times 10^{14}" class="ltx_Math" display="inline" id="S2.T1.3.3.2.m1.1">
        <semantics id="S2.T1.3.3.2.m1.1a">
         <mrow id="S2.T1.3.3.2.m1.1.1" xref="S2.T1.3.3.2.m1.1.1.cmml">
          <mi id="S2.T1.3.3.2.m1.1.1.2" xref="S2.T1.3.3.2.m1.1.1.2.cmml">
          </mi>
          <mo id="S2.T1.3.3.2.m1.1.1.1" xref="S2.T1.3.3.2.m1.1.1.1.cmml">
           ≈
          </mo>
          <mrow id="S2.T1.3.3.2.m1.1.1.3" xref="S2.T1.3.3.2.m1.1.1.3.cmml">
           <mn id="S2.T1.3.3.2.m1.1.1.3.2" xref="S2.T1.3.3.2.m1.1.1.3.2.cmml">
            1.48
           </mn>
           <mo id="S2.T1.3.3.2.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S2.T1.3.3.2.m1.1.1.3.1.cmml">
            ×
           </mo>
           <msup id="S2.T1.3.3.2.m1.1.1.3.3" xref="S2.T1.3.3.2.m1.1.1.3.3.cmml">
            <mn id="S2.T1.3.3.2.m1.1.1.3.3.2" xref="S2.T1.3.3.2.m1.1.1.3.3.2.cmml">
             10
            </mn>
            <mn id="S2.T1.3.3.2.m1.1.1.3.3.3" xref="S2.T1.3.3.2.m1.1.1.3.3.3.cmml">
             14
            </mn>
           </msup>
          </mrow>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S2.T1.3.3.2.m1.1b">
          <apply id="S2.T1.3.3.2.m1.1.1.cmml" xref="S2.T1.3.3.2.m1.1.1">
           <approx id="S2.T1.3.3.2.m1.1.1.1.cmml" xref="S2.T1.3.3.2.m1.1.1.1">
           </approx>
           <csymbol cd="latexml" id="S2.T1.3.3.2.m1.1.1.2.cmml" xref="S2.T1.3.3.2.m1.1.1.2">
            absent
           </csymbol>
           <apply id="S2.T1.3.3.2.m1.1.1.3.cmml" xref="S2.T1.3.3.2.m1.1.1.3">
            <times id="S2.T1.3.3.2.m1.1.1.3.1.cmml" xref="S2.T1.3.3.2.m1.1.1.3.1">
            </times>
            <cn id="S2.T1.3.3.2.m1.1.1.3.2.cmml" type="float" xref="S2.T1.3.3.2.m1.1.1.3.2">
             1.48
            </cn>
            <apply id="S2.T1.3.3.2.m1.1.1.3.3.cmml" xref="S2.T1.3.3.2.m1.1.1.3.3">
             <csymbol cd="ambiguous" id="S2.T1.3.3.2.m1.1.1.3.3.1.cmml" xref="S2.T1.3.3.2.m1.1.1.3.3">
              superscript
             </csymbol>
             <cn id="S2.T1.3.3.2.m1.1.1.3.3.2.cmml" type="integer" xref="S2.T1.3.3.2.m1.1.1.3.3.2">
              10
             </cn>
             <cn id="S2.T1.3.3.2.m1.1.1.3.3.3.cmml" type="integer" xref="S2.T1.3.3.2.m1.1.1.3.3.3">
              14
             </cn>
            </apply>
           </apply>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S2.T1.3.3.2.m1.1c">
          \approx 1.48\times 10^{14}
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.3.3.6">
       Decoder
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.3.3.7">
       Text
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.3.3.8">
       MLM (AR)
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.4.4">
      <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.4.4.2">
       LLaMA
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.4.4.1">
       7B
       <math alttext="-" class="ltx_Math" display="inline" id="S2.T1.4.4.1.m1.1">
        <semantics id="S2.T1.4.4.1.m1.1a">
         <mo id="S2.T1.4.4.1.m1.1.1" xref="S2.T1.4.4.1.m1.1.1.cmml">
          −
         </mo>
         <annotation-xml encoding="MathML-Content" id="S2.T1.4.4.1.m1.1b">
          <minus id="S2.T1.4.4.1.m1.1.1.cmml" xref="S2.T1.4.4.1.m1.1.1">
          </minus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S2.T1.4.4.1.m1.1c">
          -
         </annotation>
        </semantics>
       </math>
       65B
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.4.4.3">
       1.4T tokens
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.4.4.4">
       449MWh
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.4.4.5">
       —
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.4.4.6">
       Decoder
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.4.4.7">
       Text
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.4.4.8">
       MLM (AR)
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.8.11">
      <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.8.11.1">
       PaLM-2
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.11.2">
       340B
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.11.3">
       3.6T tokens
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.11.4">
       —
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.11.5">
       —
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.11.6">
       Decoder
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.11.7">
       Text
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.11.8">
       MLM (AR)
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.5.5">
      <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.5.5.2">
       T5
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.5.5.1">
       220M
       <math alttext="-" class="ltx_Math" display="inline" id="S2.T1.5.5.1.m1.1">
        <semantics id="S2.T1.5.5.1.m1.1a">
         <mo id="S2.T1.5.5.1.m1.1.1" xref="S2.T1.5.5.1.m1.1.1.cmml">
          −
         </mo>
         <annotation-xml encoding="MathML-Content" id="S2.T1.5.5.1.m1.1b">
          <minus id="S2.T1.5.5.1.m1.1.1.cmml" xref="S2.T1.5.5.1.m1.1.1">
          </minus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S2.T1.5.5.1.m1.1c">
          -
         </annotation>
        </semantics>
       </math>
       11B
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.5.5.3">
       1T tokens
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.5.5.4">
       86MWh
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.5.5.5">
       —
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.5.5.6">
       Encoder-Decoder
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.5.5.7">
       Text
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.5.5.8">
       <span class="ltx_text" id="S2.T1.5.5.8.1">
       </span>
       <span class="ltx_text" id="S2.T1.5.5.8.2">
        <span class="ltx_tabular ltx_align_middle" id="S2.T1.5.5.8.2.1">
         <span class="ltx_tr" id="S2.T1.5.5.8.2.1.1">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.5.8.2.1.1.1">
           Text-to-Text
          </span>
         </span>
         <span class="ltx_tr" id="S2.T1.5.5.8.2.1.2">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.5.8.2.1.2.1">
           generation
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_text" id="S2.T1.5.5.8.3">
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.6.6">
      <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.6.6.2">
       BERT
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.6.6.3">
       110M, 335M
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.6.6.4">
       250B tokens
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.6.6.5">
       1.536KWh
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.6.6.1">
       <math alttext="3.4\times 10^{11}" class="ltx_Math" display="inline" id="S2.T1.6.6.1.m1.1">
        <semantics id="S2.T1.6.6.1.m1.1a">
         <mrow id="S2.T1.6.6.1.m1.1.1" xref="S2.T1.6.6.1.m1.1.1.cmml">
          <mn id="S2.T1.6.6.1.m1.1.1.2" xref="S2.T1.6.6.1.m1.1.1.2.cmml">
           3.4
          </mn>
          <mo id="S2.T1.6.6.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.T1.6.6.1.m1.1.1.1.cmml">
           ×
          </mo>
          <msup id="S2.T1.6.6.1.m1.1.1.3" xref="S2.T1.6.6.1.m1.1.1.3.cmml">
           <mn id="S2.T1.6.6.1.m1.1.1.3.2" xref="S2.T1.6.6.1.m1.1.1.3.2.cmml">
            10
           </mn>
           <mn id="S2.T1.6.6.1.m1.1.1.3.3" xref="S2.T1.6.6.1.m1.1.1.3.3.cmml">
            11
           </mn>
          </msup>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S2.T1.6.6.1.m1.1b">
          <apply id="S2.T1.6.6.1.m1.1.1.cmml" xref="S2.T1.6.6.1.m1.1.1">
           <times id="S2.T1.6.6.1.m1.1.1.1.cmml" xref="S2.T1.6.6.1.m1.1.1.1">
           </times>
           <cn id="S2.T1.6.6.1.m1.1.1.2.cmml" type="float" xref="S2.T1.6.6.1.m1.1.1.2">
            3.4
           </cn>
           <apply id="S2.T1.6.6.1.m1.1.1.3.cmml" xref="S2.T1.6.6.1.m1.1.1.3">
            <csymbol cd="ambiguous" id="S2.T1.6.6.1.m1.1.1.3.1.cmml" xref="S2.T1.6.6.1.m1.1.1.3">
             superscript
            </csymbol>
            <cn id="S2.T1.6.6.1.m1.1.1.3.2.cmml" type="integer" xref="S2.T1.6.6.1.m1.1.1.3.2">
             10
            </cn>
            <cn id="S2.T1.6.6.1.m1.1.1.3.3.cmml" type="integer" xref="S2.T1.6.6.1.m1.1.1.3.3">
             11
            </cn>
           </apply>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S2.T1.6.6.1.m1.1c">
          3.4\times 10^{11}
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.6.6.6">
       Encoder
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.6.6.7">
       Text
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.6.6.8">
       <span class="ltx_text" id="S2.T1.6.6.8.1">
       </span>
       <span class="ltx_text" id="S2.T1.6.6.8.2">
        <span class="ltx_tabular ltx_align_middle" id="S2.T1.6.6.8.2.1">
         <span class="ltx_tr" id="S2.T1.6.6.8.2.1.1">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.6.6.8.2.1.1.1">
           MLM, NSP
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_text" id="S2.T1.6.6.8.3">
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.8.8">
      <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.8.8.3">
       RoBERTa
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.8.4">
       110M, 335M
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.8.5">
       2T tokens
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.7.7.1">
       <math alttext="\approx" class="ltx_Math" display="inline" id="S2.T1.7.7.1.m1.1">
        <semantics id="S2.T1.7.7.1.m1.1a">
         <mo id="S2.T1.7.7.1.m1.1.1" xref="S2.T1.7.7.1.m1.1.1.cmml">
          ≈
         </mo>
         <annotation-xml encoding="MathML-Content" id="S2.T1.7.7.1.m1.1b">
          <approx id="S2.T1.7.7.1.m1.1.1.cmml" xref="S2.T1.7.7.1.m1.1.1">
          </approx>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S2.T1.7.7.1.m1.1c">
          \approx
         </annotation>
        </semantics>
       </math>
       12.3KWh
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.8.2">
       <math alttext="3.4\times 10^{12}" class="ltx_Math" display="inline" id="S2.T1.8.8.2.m1.1">
        <semantics id="S2.T1.8.8.2.m1.1a">
         <mrow id="S2.T1.8.8.2.m1.1.1" xref="S2.T1.8.8.2.m1.1.1.cmml">
          <mn id="S2.T1.8.8.2.m1.1.1.2" xref="S2.T1.8.8.2.m1.1.1.2.cmml">
           3.4
          </mn>
          <mo id="S2.T1.8.8.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.T1.8.8.2.m1.1.1.1.cmml">
           ×
          </mo>
          <msup id="S2.T1.8.8.2.m1.1.1.3" xref="S2.T1.8.8.2.m1.1.1.3.cmml">
           <mn id="S2.T1.8.8.2.m1.1.1.3.2" xref="S2.T1.8.8.2.m1.1.1.3.2.cmml">
            10
           </mn>
           <mn id="S2.T1.8.8.2.m1.1.1.3.3" xref="S2.T1.8.8.2.m1.1.1.3.3.cmml">
            12
           </mn>
          </msup>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S2.T1.8.8.2.m1.1b">
          <apply id="S2.T1.8.8.2.m1.1.1.cmml" xref="S2.T1.8.8.2.m1.1.1">
           <times id="S2.T1.8.8.2.m1.1.1.1.cmml" xref="S2.T1.8.8.2.m1.1.1.1">
           </times>
           <cn id="S2.T1.8.8.2.m1.1.1.2.cmml" type="float" xref="S2.T1.8.8.2.m1.1.1.2">
            3.4
           </cn>
           <apply id="S2.T1.8.8.2.m1.1.1.3.cmml" xref="S2.T1.8.8.2.m1.1.1.3">
            <csymbol cd="ambiguous" id="S2.T1.8.8.2.m1.1.1.3.1.cmml" xref="S2.T1.8.8.2.m1.1.1.3">
             superscript
            </csymbol>
            <cn id="S2.T1.8.8.2.m1.1.1.3.2.cmml" type="integer" xref="S2.T1.8.8.2.m1.1.1.3.2">
             10
            </cn>
            <cn id="S2.T1.8.8.2.m1.1.1.3.3.cmml" type="integer" xref="S2.T1.8.8.2.m1.1.1.3.3">
             12
            </cn>
           </apply>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S2.T1.8.8.2.m1.1c">
          3.4\times 10^{12}
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.8.6">
       Encoder
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.8.7">
       Text
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.8.8">
       <span class="ltx_text" id="S2.T1.8.8.8.1">
       </span>
       <span class="ltx_text" id="S2.T1.8.8.8.2">
        <span class="ltx_tabular ltx_align_middle" id="S2.T1.8.8.8.2.1">
         <span class="ltx_tr" id="S2.T1.8.8.8.2.1.1">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.8.8.2.1.1.1">
           MLM, NSP
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_text" id="S2.T1.8.8.8.3">
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.8.12">
      <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.8.12.1">
       DALL-E
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.12.2">
       12B
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.12.3">
       2.5B pairs
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.12.4">
       —
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.12.5">
       —
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.12.6">
       <span class="ltx_text" id="S2.T1.8.12.6.1">
       </span>
       <span class="ltx_text" id="S2.T1.8.12.6.2">
        <span class="ltx_tabular ltx_align_middle" id="S2.T1.8.12.6.2.1">
         <span class="ltx_tr" id="S2.T1.8.12.6.2.1.1">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.12.6.2.1.1.1">
           dVAE, Decoder
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_text" id="S2.T1.8.12.6.3">
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.12.7">
       <span class="ltx_text" id="S2.T1.8.12.7.1">
       </span>
       <span class="ltx_text" id="S2.T1.8.12.7.2">
        <span class="ltx_tabular ltx_align_middle" id="S2.T1.8.12.7.2.1">
         <span class="ltx_tr" id="S2.T1.8.12.7.2.1.1">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.12.7.2.1.1.1">
           Text, Image
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_text" id="S2.T1.8.12.7.3">
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.8.12.8">
       MLM (AR)
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.8.13">
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.8.13.1">
       DALL-E2
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.8.13.2">
       3.5B
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.8.13.3">
       6.5B pairs
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.8.13.4">
       —
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.8.13.5">
       —
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.8.13.6">
       <span class="ltx_text" id="S2.T1.8.13.6.1">
       </span>
       <span class="ltx_text" id="S2.T1.8.13.6.2">
        <span class="ltx_tabular ltx_align_middle" id="S2.T1.8.13.6.2.1">
         <span class="ltx_tr" id="S2.T1.8.13.6.2.1.1">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.13.6.2.1.1.1">
           Encoder CLIP
          </span>
         </span>
         <span class="ltx_tr" id="S2.T1.8.13.6.2.1.2">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.13.6.2.1.2.1">
           AR / Diffusion Prior
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_text" id="S2.T1.8.13.6.3">
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.8.13.7">
       <span class="ltx_text" id="S2.T1.8.13.7.1">
       </span>
       <span class="ltx_text" id="S2.T1.8.13.7.2">
        <span class="ltx_tabular ltx_align_middle" id="S2.T1.8.13.7.2.1">
         <span class="ltx_tr" id="S2.T1.8.13.7.2.1.1">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.13.7.2.1.1.1">
           Text, Image
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_text" id="S2.T1.8.13.7.3">
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.8.13.8">
       <span class="ltx_text" id="S2.T1.8.13.8.1">
       </span>
       <span class="ltx_text" id="S2.T1.8.13.8.2">
        <span class="ltx_tabular ltx_align_middle" id="S2.T1.8.13.8.2.1">
         <span class="ltx_tr" id="S2.T1.8.13.8.2.1.1">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.13.8.2.1.1.1">
           Text-to-Image
          </span>
         </span>
         <span class="ltx_tr" id="S2.T1.8.13.8.2.1.2">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.13.8.2.1.2.1">
           diffusive generation
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_text" id="S2.T1.8.13.8.3">
       </span>
      </td>
     </tr>
    </table>
   </figure>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     <span class="ltx_text" id="S2.SS2.4.1.1">
      II-B
     </span>
    </span>
    <span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">
     On-device
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short">
       LLM
      </span>
     </abbr>
     s Challenges
    </span>
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     Despite their promising capabilities, building intelligent wireless networks that are empowered by generative agents is extremely challenging using current
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     . This is due to their huge parameter sizes, which limits their deployment at the edge. In particular, it was demonstrated that, in few-shot scenarios, models’ memory requirements dramatically increase as the number of parameters increase.
For example, it takes roughly 86 GB of GPU memory for Falcon-40B to infer.
In addition to that,
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     have long inference time when solving sophisticated tasks (planning and reasoning), which significantly increases the communication latency.
Finally, updating or knowledge synchronization from central
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     to on-device
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     could be energy-consuming through conventional techniques, such as federated learning.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p2">
    <p class="ltx_p" id="S2.SS2.p2.1">
     With the aim to realize lightweight LLMs, neural network compression techniques such as weight sharing, pruning, knowledge distillation, low-rank decomposition, and quantization can be applied. Within the context of compression, it should be highlighted that, since
     <abbr class="ltx_glossaryref" title="Large Language Model">
      <span class="ltx_text ltx_glossary_short-plural">
       LLMs
      </span>
     </abbr>
     are few-shot learners rather than task-specific models, compression techniques should preserve LLM’s transferring abilities. In a different context, QLoRA is a high-precision quantization technique, that can be used to quantize an LLM to 4-bit, then adds a small set of learnable low-rank adapters, which are tuned through back-propagating gradients using the quantized weights. This significantly reduces the GPU memory and latency in fine-tuning and inference. Although compression and quantization can reduce the model size and computation requirements, due to the non-modular nature of LLMs’ knowledge, such approach can yield degraded performance in specific tasks.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    III
   </span>
   <span class="ltx_text ltx_font_smallcaps" id="S3.1.1">
    Multi-Agent Collective Intelligence via Generative LLM
   </span>
  </h2>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     <span class="ltx_text" id="S3.SS1.4.1.1">
      III-A
     </span>
    </span>
    <span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">
     Multi-Agent LLM Network Architecture
    </span>
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     Deploying generative AI in wireless networks requires the realization of collective intelligence from multiple on-device LLMs, that can interact with the environment, plan actionable tasks from a goal, and solve the tasks collaboratively by exchanging knowledge. Our aim in this section is to set the groundwork for integrating generative agents into future wireless networks, in which a wireless generative agent leverages LLMs to perform a close-loop task planning, execution, and optimization. As illustrated in Fig.
     <a class="ltx_ref" href="#S3.F1" title="Figure 1 ‣ III-A Multi-Agent LLM Network Architecture ‣ III Multi-Agent Collective Intelligence via Generative LLM ‣ Wireless Multi-Agent Generative AI: From Connected Intelligence to Collective Intelligence">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     , this process can be achieved through multiple steps. First, the agent deconstructs higher-level intents or goals, and plan sequences of lower-level tasks over time. Afterwards, it perceives the environment, performs decisions and takes actions accordingly, and then optimizes the decision policy from the rewards. According to the results, the agent creates new tasks until the intent’s goal is achieved, and then generates a final response.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     In order to realize collective intelligence, the earlier mentioned procedure can be performed through multiple wireless generative agents, in which the agents exchange knowledge through the network, to collaboratively plan tasks, take actions and optimize policies, based on the architecture shown in Fig.
     <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ III-A Multi-Agent LLM Network Architecture ‣ III Multi-Agent Collective Intelligence via Generative LLM ‣ Wireless Multi-Agent Generative AI: From Connected Intelligence to Collective Intelligence">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     . Note that knowledge is an abstracted representation of the data, which can be encoded into LLMs for performing specific tasks. From the network perspective, intents from humans or machines are provided to generative agents through different wireless terminals. These terminals create prompts to an on-device LLM to complete each step, as shown in Fig.
     <a class="ltx_ref" href="#S3.F1" title="Figure 1 ‣ III-A Multi-Agent LLM Network Architecture ‣ III Multi-Agent Collective Intelligence via Generative LLM ‣ Wireless Multi-Agent Generative AI: From Connected Intelligence to Collective Intelligence">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     . The tasks are planned collaboratively among multiple generative agents, to best leverage the knowledge of different LLMs, and the capabilities of different devices. The on-device LLM can obtain domain specific knowledge from the cloud-based LLM, or from other on-device LLMs when the planned task is received from other agents. From the device perspective, a wireless generative agent has a perceiver (sensor) to observe the environment, and an actor (controller) to execute the decisions. The on-device LLM extracts semantic information from the observed raw data in multi-modalities (text, image, sound), and stores it in a memory stream for planning new tasks in the future. Accordingly, to perform a particular task, the relevant semantic information is retrieved for taking an action. After completing its planned actions from the received higher-level tasks, the agent can further create lower-level tasks and send it to other agents to complete the goal.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F1">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="96" id="S3.F1.g1" src="/html/2307.02757/assets/generative_agent_loop.png" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 1:
     </span>
     Close-loop task planning, execution, optimization in wireless generative agents.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="S3.F2">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="149" id="S3.F2.g1" src="/html/2307.02757/assets/generative_agent_architecture.png" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 2:
     </span>
     Wireless generative agent network and device architecture.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     <span class="ltx_text" id="S3.SS2.4.1.1">
      III-B
     </span>
    </span>
    <span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">
     Multi-Agent LLM Technologies
    </span>
   </h3>
   <section class="ltx_subsubsection" id="S3.SS2.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      <span class="ltx_text" id="S3.SS2.SSS1.4.1.1">
       III-B
      </span>
      1
     </span>
     Planning and Reasoning
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS1.p1">
     <p class="ltx_p" id="S3.SS2.SSS1.p1.1">
      A high-level goal for wireless agents usually contains complex tasks or problems, and therefore, planning is an essential step for the generative agents to create actionable tasks over time to achieve a specific goal. The currently available
      <abbr class="ltx_glossaryref" title="Large Language Model">
       <span class="ltx_text ltx_glossary_short-plural">
        LLMs
       </span>
      </abbr>
      have limited capabilities in solving problems with complex reasoning and multi-step planning.
This can be a limiting factor in the deployment of wireless generative agents in networks with ultra-high reliability requirement.
Therefore, system 2-type
      <abbr class="ltx_glossaryref" title="Large Language Model">
       <span class="ltx_text ltx_glossary_short-plural">
        LLMs
       </span>
      </abbr>
      can endow wireless generative agents with reasoning capabilities. Recently, many works have shown the reasoning abilities of
      <abbr class="ltx_glossaryref" title="Large Language Model">
       <span class="ltx_text ltx_glossary_short-plural">
        LLMs
       </span>
      </abbr>
      through prompting engineering. For instance, chain-of-thought prompting, which prompts
      <abbr class="ltx_glossaryref" title="Large Language Model">
       <span class="ltx_text ltx_glossary_short-plural">
        LLMs
       </span>
      </abbr>
      with a sequence of short sentences serving as intermediate reasoning steps to solve problems, have demonstrated an outstanding performance on a wide range of reasoning tasks. Thereafter, self-consistency strategy is proposed to replace the greedy decoding of chain-of-thought prompting. Furthermore, tree of thoughts is another method which maintains a tree for each immediate step of thought so that
      <abbr class="ltx_glossaryref" title="Large Language Model">
       <span class="ltx_text ltx_glossary_short-plural">
        LLMs
       </span>
      </abbr>
      can deliberately evaluate multiple paths and adjust decisions as needed for optimal outcomes. As an alternative to prompts optimization which can be regarded as an outcome-supervision, one possible solution is to supervise along the reasoning process. This requires LLMs to properly model and encode a domain specific knowledge for solving different tasks. Furthermore, multi-agent planning can reduce the cost of inference or fine-tuning, by distributing the task between multiple LLM devices with domain specific knowledge.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS2.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      <span class="ltx_text" id="S3.SS2.SSS2.4.1.1">
       III-B
      </span>
      2
     </span>
     Multi-agent LLM Games and RL
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS2.p1">
     <p class="ltx_p" id="S3.SS2.SSS2.p1.1">
      Current
      <abbr class="ltx_glossaryref" title="Large Language Model">
       <span class="ltx_text ltx_glossary_short-plural">
        LLMs
       </span>
      </abbr>
      are trained to generate contents which are helpful, reliable, and harmless to the users. These attributes can be achieved only if one-to-one iteration between a human and an agent is considered. However, in the presence of multiple generative agents, it is yet debatable whether these features can be realized. For instance, egocentric agents, which are extensively serving their corresponding users, can hinder the normal operation of other agents or be unresponsive to some collaborative tasks. On the contrary, selfless agents which are instructed to be helpful can be vulnerable to malicious attacks. Hence, it is essential to strike a balance between selfishness and selflessness when designing generative agents. Within this context, game theory constitutes a suitable tool to model multi-agent LLMs and analyze their behaviour. Initial attempts to understand the interactions between multiple
      <abbr class="ltx_glossaryref" title="Large Language Model">
       <span class="ltx_text ltx_glossary_short-plural">
        LLMs
       </span>
      </abbr>
      (i.e.
      <abbr class="ltx_glossaryref" title="Large Language Model">
       <span class="ltx_text ltx_glossary_short">
        LLM
       </span>
      </abbr>
      <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p1.1.1">
       games
      </span>
      ) are presented in
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib8" title="">
        8
       </a>
       ]
      </cite>
      .
     </p>
    </div>
    <div class="ltx_para" id="S3.SS2.SSS2.p2">
     <p class="ltx_p" id="S3.SS2.SSS2.p2.1">
      Multi-agent
      <abbr class="ltx_glossaryref" title="reinforcement learning">
       <span class="ltx_text ltx_glossary_short">
        RL
       </span>
      </abbr>
      (MARL) is another approach to observe the optimal collaborative behavior in a multi-agent system, which can also be applied to LLMs. In particular, it is essential for the wireless agents to interact with the environment, where
      <abbr class="ltx_glossaryref" title="reinforcement learning">
       <span class="ltx_text ltx_glossary_short">
        RL
       </span>
      </abbr>
      tunes the actions generated by
      <abbr class="ltx_glossaryref" title="Large Language Model">
       <span class="ltx_text ltx_glossary_short-plural">
        LLMs
       </span>
      </abbr>
      with environment rewards, in order to ground
      <abbr class="ltx_glossaryref" title="Large Language Model">
       <span class="ltx_text ltx_glossary_short-plural">
        LLMs
       </span>
      </abbr>
      into a real-world context. MARL-enabled LLMs can further model the interaction between the wireless agents, using LLM’s knowledge, to learn the optimal collaboration policy and communication protocols among distributed LLM agents. In doing so, communication costs can be reduced as agents converge towards optimal decision policies.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS2.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      <span class="ltx_text" id="S3.SS2.SSS3.4.1.1">
       III-B
      </span>
      3
     </span>
     Semantic Information and Communication
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS3.p1">
     <p class="ltx_p" id="S3.SS2.SSS3.p1.1">
      The interaction between the multiple wireless generative agents in collaborative systems will result in a huge amount of information to be generated, exchanged, and perceived. Shannon communication rooted in the level A of Weaver’s 3-level communications solely aims at transmitting symbols accurately and efficiently, while ignoring the conveyed meaning of the transmitted messages. In contrast, in semantic communication, the transmitter sends only useful/relevant information to the receiver, in order to solve a given downstream task. This paradigm is possible if the goal/objective of the communication is known at both sides. Recalling that wireless generative agents aim to collaboratively solve a specific problem, semantic communication can transfer task specific knowledge for LLMs on targeted devices. Furthermore, agents can communicate based on their abstracted state information, to improve multi-agent cooperation with efficient protocols.
     </p>
    </div>
    <div class="ltx_para" id="S3.SS2.SSS3.p2">
     <p class="ltx_p" id="S3.SS2.SSS3.p2.1">
      Similarly, semantic information can enable LLMs to integrate multi-modal raw data in a common concept space. In doing so, the data exchanged and stored among wireless devices can be largely reduced. Moreover, LLMs can exploit abstracted ”thoughts” from the perceived data by modelling it using semantic information on a topological space. Hence, LLMs can perform effectively in short-term actions with lower-order information, and in long-term plans with higher-order abstractions. This also enables LLMs to encode domain or task specific knowledge, and thereby, to reduce the model size and computing costs, making it suitable for resource constrained devices.
     </p>
    </div>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    IV
   </span>
   <span class="ltx_text ltx_font_smallcaps" id="S4.1.1">
    Applications of Multi-Agent LLM Networks
   </span>
  </h2>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     <span class="ltx_text" id="S4.SS1.4.1.1">
      IV-A
     </span>
    </span>
    <span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">
     Intent-driven Network Automation
    </span>
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     With the vision to enable network automation in future wireless networks, it is envisioned that 6G will support intent-based networking, which aligns the network operation with particular intents and objectives. Although intent-driven networks are defined in 3GPP and IETF, their functionalities and their flexible deployment are still limited. On the one hand, intent translation and orchestration relies heavily on services models and policies, which limits its capabilities in handling new scenarios. On the other hand, intent driven networking has not
been applied yet in general networks and user devices, including radio access networks.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.1">
     LLMs can potential extend the concept of intent driven networks to a general purpose self-designed communication network. The Open AI’s ChatGPT-4 has recently been equipped with a number of plugins, allowing an autonomous agent to execute the commands generated by GPT and produce outputs in natural languages. Inspired by this, intent driven networking can utilize LLMs to break down higher-level intents, (e.g., reducing the network energy consumption by 5%) into a sequence of lower level tasks (e.g., tuning transmit power, channel measurement, etc.), and instruct related executors to take actions (i.e. gNB). The results are then prompted to the LLM, which will add subsequent tasks, if necessary, until the intent’s goals are achieved. In a multi-agent LLM network, each network device will leverage LLMs to analyze the perceived information, including the observations from the environment and actions taken by other agents. Once the agent plans and prioritizes a set of sub-tasks, it takes actions on local tasks, which can be accomplished by the local actor. After that, it utilizes LLMs to generate a protocol language including the remaining and newly created sub-tasks, and sends to other agents through the wireless network. Each agent conducts such loops and communicates with others until the added tasks are completed and the goal is achieved.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     <span class="ltx_text" id="S4.SS2.4.1.1">
      IV-B
     </span>
    </span>
    <span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">
     Case Study: Wireless Energy Saving
    </span>
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     The vision of generative agent-enabled intent-driven networking requires LLMs to deconstruct a wireless network goal into sub-tasks or sub-problems and solve them via multi-agent cooperation. One important step in this process is the efficiency of multiple LLM instances in formulating and solving problems pertinent to wireless networks. In this section, we consider a scenario with multiple mobile users playing a game to reduce the total power consumption. Our aim is to evaluate the capability of on-device LLMs in solving a wireless communication problem, and show their potentials in enabling an end-to-end autonomous network.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.7">
     In our setup, we assume a network with
     <math alttext="K=4" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1">
      <semantics id="S4.SS2.p2.1.m1.1a">
       <mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">
        <mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">
         K
        </mi>
        <mo id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">
         =
        </mo>
        <mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">
         4
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b">
        <apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">
         <eq id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1">
         </eq>
         <ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">
          𝐾
         </ci>
         <cn id="S4.SS2.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.p2.1.m1.1.1.3">
          4
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">
        K=4
       </annotation>
      </semantics>
     </math>
     users transmitting in a shared spectrum. Each user has a channel gain of
     <math alttext="g=\left(1.21,2.01,0.58,0.13\right)" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.4">
      <semantics id="S4.SS2.p2.2.m2.4a">
       <mrow id="S4.SS2.p2.2.m2.4.5" xref="S4.SS2.p2.2.m2.4.5.cmml">
        <mi id="S4.SS2.p2.2.m2.4.5.2" xref="S4.SS2.p2.2.m2.4.5.2.cmml">
         g
        </mi>
        <mo id="S4.SS2.p2.2.m2.4.5.1" xref="S4.SS2.p2.2.m2.4.5.1.cmml">
         =
        </mo>
        <mrow id="S4.SS2.p2.2.m2.4.5.3.2" xref="S4.SS2.p2.2.m2.4.5.3.1.cmml">
         <mo id="S4.SS2.p2.2.m2.4.5.3.2.1" xref="S4.SS2.p2.2.m2.4.5.3.1.cmml">
          (
         </mo>
         <mn id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">
          1.21
         </mn>
         <mo id="S4.SS2.p2.2.m2.4.5.3.2.2" xref="S4.SS2.p2.2.m2.4.5.3.1.cmml">
          ,
         </mo>
         <mn id="S4.SS2.p2.2.m2.2.2" xref="S4.SS2.p2.2.m2.2.2.cmml">
          2.01
         </mn>
         <mo id="S4.SS2.p2.2.m2.4.5.3.2.3" xref="S4.SS2.p2.2.m2.4.5.3.1.cmml">
          ,
         </mo>
         <mn id="S4.SS2.p2.2.m2.3.3" xref="S4.SS2.p2.2.m2.3.3.cmml">
          0.58
         </mn>
         <mo id="S4.SS2.p2.2.m2.4.5.3.2.4" xref="S4.SS2.p2.2.m2.4.5.3.1.cmml">
          ,
         </mo>
         <mn id="S4.SS2.p2.2.m2.4.4" xref="S4.SS2.p2.2.m2.4.4.cmml">
          0.13
         </mn>
         <mo id="S4.SS2.p2.2.m2.4.5.3.2.5" xref="S4.SS2.p2.2.m2.4.5.3.1.cmml">
          )
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.4b">
        <apply id="S4.SS2.p2.2.m2.4.5.cmml" xref="S4.SS2.p2.2.m2.4.5">
         <eq id="S4.SS2.p2.2.m2.4.5.1.cmml" xref="S4.SS2.p2.2.m2.4.5.1">
         </eq>
         <ci id="S4.SS2.p2.2.m2.4.5.2.cmml" xref="S4.SS2.p2.2.m2.4.5.2">
          𝑔
         </ci>
         <vector id="S4.SS2.p2.2.m2.4.5.3.1.cmml" xref="S4.SS2.p2.2.m2.4.5.3.2">
          <cn id="S4.SS2.p2.2.m2.1.1.cmml" type="float" xref="S4.SS2.p2.2.m2.1.1">
           1.21
          </cn>
          <cn id="S4.SS2.p2.2.m2.2.2.cmml" type="float" xref="S4.SS2.p2.2.m2.2.2">
           2.01
          </cn>
          <cn id="S4.SS2.p2.2.m2.3.3.cmml" type="float" xref="S4.SS2.p2.2.m2.3.3">
           0.58
          </cn>
          <cn id="S4.SS2.p2.2.m2.4.4.cmml" type="float" xref="S4.SS2.p2.2.m2.4.4">
           0.13
          </cn>
         </vector>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.4c">
        g=\left(1.21,2.01,0.58,0.13\right)
       </annotation>
      </semantics>
     </math>
     , bandwidth
     <math alttext="b=15\text{kHz}" class="ltx_Math" display="inline" id="S4.SS2.p2.3.m3.1">
      <semantics id="S4.SS2.p2.3.m3.1a">
       <mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">
        <mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">
         b
        </mi>
        <mo id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">
         =
        </mo>
        <mrow id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">
         <mn id="S4.SS2.p2.3.m3.1.1.3.2" xref="S4.SS2.p2.3.m3.1.1.3.2.cmml">
          15
         </mn>
         <mo id="S4.SS2.p2.3.m3.1.1.3.1" lspace="0em" rspace="0em" xref="S4.SS2.p2.3.m3.1.1.3.1.cmml">
          ​
         </mo>
         <mtext id="S4.SS2.p2.3.m3.1.1.3.3" xref="S4.SS2.p2.3.m3.1.1.3.3a.cmml">
          kHz
         </mtext>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b">
        <apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">
         <eq id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1">
         </eq>
         <ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">
          𝑏
         </ci>
         <apply id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">
          <times id="S4.SS2.p2.3.m3.1.1.3.1.cmml" xref="S4.SS2.p2.3.m3.1.1.3.1">
          </times>
          <cn id="S4.SS2.p2.3.m3.1.1.3.2.cmml" type="integer" xref="S4.SS2.p2.3.m3.1.1.3.2">
           15
          </cn>
          <ci id="S4.SS2.p2.3.m3.1.1.3.3a.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3">
           <mtext id="S4.SS2.p2.3.m3.1.1.3.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3">
            kHz
           </mtext>
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">
        b=15\text{kHz}
       </annotation>
      </semantics>
     </math>
     , and noise
     <math alttext="n=1\text{dB}" class="ltx_Math" display="inline" id="S4.SS2.p2.4.m4.1">
      <semantics id="S4.SS2.p2.4.m4.1a">
       <mrow id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">
        <mi id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">
         n
        </mi>
        <mo id="S4.SS2.p2.4.m4.1.1.1" xref="S4.SS2.p2.4.m4.1.1.1.cmml">
         =
        </mo>
        <mrow id="S4.SS2.p2.4.m4.1.1.3" xref="S4.SS2.p2.4.m4.1.1.3.cmml">
         <mn id="S4.SS2.p2.4.m4.1.1.3.2" xref="S4.SS2.p2.4.m4.1.1.3.2.cmml">
          1
         </mn>
         <mo id="S4.SS2.p2.4.m4.1.1.3.1" lspace="0em" rspace="0em" xref="S4.SS2.p2.4.m4.1.1.3.1.cmml">
          ​
         </mo>
         <mtext id="S4.SS2.p2.4.m4.1.1.3.3" xref="S4.SS2.p2.4.m4.1.1.3.3a.cmml">
          dB
         </mtext>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b">
        <apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">
         <eq id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1.1">
         </eq>
         <ci id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2">
          𝑛
         </ci>
         <apply id="S4.SS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3">
          <times id="S4.SS2.p2.4.m4.1.1.3.1.cmml" xref="S4.SS2.p2.4.m4.1.1.3.1">
          </times>
          <cn id="S4.SS2.p2.4.m4.1.1.3.2.cmml" type="integer" xref="S4.SS2.p2.4.m4.1.1.3.2">
           1
          </cn>
          <ci id="S4.SS2.p2.4.m4.1.1.3.3a.cmml" xref="S4.SS2.p2.4.m4.1.1.3.3">
           <mtext id="S4.SS2.p2.4.m4.1.1.3.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3.3">
            dB
           </mtext>
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">
        n=1\text{dB}
       </annotation>
      </semantics>
     </math>
     . The game starts with an initial transmit power
     <math alttext="p=\left(2,4,5,6\right)\text{W}" class="ltx_Math" display="inline" id="S4.SS2.p2.5.m5.4">
      <semantics id="S4.SS2.p2.5.m5.4a">
       <mrow id="S4.SS2.p2.5.m5.4.5" xref="S4.SS2.p2.5.m5.4.5.cmml">
        <mi id="S4.SS2.p2.5.m5.4.5.2" xref="S4.SS2.p2.5.m5.4.5.2.cmml">
         p
        </mi>
        <mo id="S4.SS2.p2.5.m5.4.5.1" xref="S4.SS2.p2.5.m5.4.5.1.cmml">
         =
        </mo>
        <mrow id="S4.SS2.p2.5.m5.4.5.3" xref="S4.SS2.p2.5.m5.4.5.3.cmml">
         <mrow id="S4.SS2.p2.5.m5.4.5.3.2.2" xref="S4.SS2.p2.5.m5.4.5.3.2.1.cmml">
          <mo id="S4.SS2.p2.5.m5.4.5.3.2.2.1" xref="S4.SS2.p2.5.m5.4.5.3.2.1.cmml">
           (
          </mo>
          <mn id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml">
           2
          </mn>
          <mo id="S4.SS2.p2.5.m5.4.5.3.2.2.2" xref="S4.SS2.p2.5.m5.4.5.3.2.1.cmml">
           ,
          </mo>
          <mn id="S4.SS2.p2.5.m5.2.2" xref="S4.SS2.p2.5.m5.2.2.cmml">
           4
          </mn>
          <mo id="S4.SS2.p2.5.m5.4.5.3.2.2.3" xref="S4.SS2.p2.5.m5.4.5.3.2.1.cmml">
           ,
          </mo>
          <mn id="S4.SS2.p2.5.m5.3.3" xref="S4.SS2.p2.5.m5.3.3.cmml">
           5
          </mn>
          <mo id="S4.SS2.p2.5.m5.4.5.3.2.2.4" xref="S4.SS2.p2.5.m5.4.5.3.2.1.cmml">
           ,
          </mo>
          <mn id="S4.SS2.p2.5.m5.4.4" xref="S4.SS2.p2.5.m5.4.4.cmml">
           6
          </mn>
          <mo id="S4.SS2.p2.5.m5.4.5.3.2.2.5" xref="S4.SS2.p2.5.m5.4.5.3.2.1.cmml">
           )
          </mo>
         </mrow>
         <mo id="S4.SS2.p2.5.m5.4.5.3.1" lspace="0em" rspace="0em" xref="S4.SS2.p2.5.m5.4.5.3.1.cmml">
          ​
         </mo>
         <mtext id="S4.SS2.p2.5.m5.4.5.3.3" xref="S4.SS2.p2.5.m5.4.5.3.3a.cmml">
          W
         </mtext>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.4b">
        <apply id="S4.SS2.p2.5.m5.4.5.cmml" xref="S4.SS2.p2.5.m5.4.5">
         <eq id="S4.SS2.p2.5.m5.4.5.1.cmml" xref="S4.SS2.p2.5.m5.4.5.1">
         </eq>
         <ci id="S4.SS2.p2.5.m5.4.5.2.cmml" xref="S4.SS2.p2.5.m5.4.5.2">
          𝑝
         </ci>
         <apply id="S4.SS2.p2.5.m5.4.5.3.cmml" xref="S4.SS2.p2.5.m5.4.5.3">
          <times id="S4.SS2.p2.5.m5.4.5.3.1.cmml" xref="S4.SS2.p2.5.m5.4.5.3.1">
          </times>
          <vector id="S4.SS2.p2.5.m5.4.5.3.2.1.cmml" xref="S4.SS2.p2.5.m5.4.5.3.2.2">
           <cn id="S4.SS2.p2.5.m5.1.1.cmml" type="integer" xref="S4.SS2.p2.5.m5.1.1">
            2
           </cn>
           <cn id="S4.SS2.p2.5.m5.2.2.cmml" type="integer" xref="S4.SS2.p2.5.m5.2.2">
            4
           </cn>
           <cn id="S4.SS2.p2.5.m5.3.3.cmml" type="integer" xref="S4.SS2.p2.5.m5.3.3">
            5
           </cn>
           <cn id="S4.SS2.p2.5.m5.4.4.cmml" type="integer" xref="S4.SS2.p2.5.m5.4.4">
            6
           </cn>
          </vector>
          <ci id="S4.SS2.p2.5.m5.4.5.3.3a.cmml" xref="S4.SS2.p2.5.m5.4.5.3.3">
           <mtext id="S4.SS2.p2.5.m5.4.5.3.3.cmml" xref="S4.SS2.p2.5.m5.4.5.3.3">
            W
           </mtext>
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.4c">
        p=\left(2,4,5,6\right)\text{W}
       </annotation>
      </semantics>
     </math>
     . The base station has the goal of ”reducing network energy consumption by
     <math alttext="\Delta p=0.85" class="ltx_Math" display="inline" id="S4.SS2.p2.6.m6.1">
      <semantics id="S4.SS2.p2.6.m6.1a">
       <mrow id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml">
        <mrow id="S4.SS2.p2.6.m6.1.1.2" xref="S4.SS2.p2.6.m6.1.1.2.cmml">
         <mi id="S4.SS2.p2.6.m6.1.1.2.2" mathvariant="normal" xref="S4.SS2.p2.6.m6.1.1.2.2.cmml">
          Δ
         </mi>
         <mo id="S4.SS2.p2.6.m6.1.1.2.1" lspace="0em" rspace="0em" xref="S4.SS2.p2.6.m6.1.1.2.1.cmml">
          ​
         </mo>
         <mi id="S4.SS2.p2.6.m6.1.1.2.3" xref="S4.SS2.p2.6.m6.1.1.2.3.cmml">
          p
         </mi>
        </mrow>
        <mo id="S4.SS2.p2.6.m6.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.cmml">
         =
        </mo>
        <mn id="S4.SS2.p2.6.m6.1.1.3" xref="S4.SS2.p2.6.m6.1.1.3.cmml">
         0.85
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b">
        <apply id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1">
         <eq id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1">
         </eq>
         <apply id="S4.SS2.p2.6.m6.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.2">
          <times id="S4.SS2.p2.6.m6.1.1.2.1.cmml" xref="S4.SS2.p2.6.m6.1.1.2.1">
          </times>
          <ci id="S4.SS2.p2.6.m6.1.1.2.2.cmml" xref="S4.SS2.p2.6.m6.1.1.2.2">
           Δ
          </ci>
          <ci id="S4.SS2.p2.6.m6.1.1.2.3.cmml" xref="S4.SS2.p2.6.m6.1.1.2.3">
           𝑝
          </ci>
         </apply>
         <cn id="S4.SS2.p2.6.m6.1.1.3.cmml" type="float" xref="S4.SS2.p2.6.m6.1.1.3">
          0.85
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">
        \Delta p=0.85
       </annotation>
      </semantics>
     </math>
     W” while
maintaining individual transmission rates above
     <math alttext="r=\left(3.50,15.80,4.40,1.00\right)\text{Kbps}" class="ltx_Math" display="inline" id="S4.SS2.p2.7.m7.4">
      <semantics id="S4.SS2.p2.7.m7.4a">
       <mrow id="S4.SS2.p2.7.m7.4.5" xref="S4.SS2.p2.7.m7.4.5.cmml">
        <mi id="S4.SS2.p2.7.m7.4.5.2" xref="S4.SS2.p2.7.m7.4.5.2.cmml">
         r
        </mi>
        <mo id="S4.SS2.p2.7.m7.4.5.1" xref="S4.SS2.p2.7.m7.4.5.1.cmml">
         =
        </mo>
        <mrow id="S4.SS2.p2.7.m7.4.5.3" xref="S4.SS2.p2.7.m7.4.5.3.cmml">
         <mrow id="S4.SS2.p2.7.m7.4.5.3.2.2" xref="S4.SS2.p2.7.m7.4.5.3.2.1.cmml">
          <mo id="S4.SS2.p2.7.m7.4.5.3.2.2.1" xref="S4.SS2.p2.7.m7.4.5.3.2.1.cmml">
           (
          </mo>
          <mn id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml">
           3.50
          </mn>
          <mo id="S4.SS2.p2.7.m7.4.5.3.2.2.2" xref="S4.SS2.p2.7.m7.4.5.3.2.1.cmml">
           ,
          </mo>
          <mn id="S4.SS2.p2.7.m7.2.2" xref="S4.SS2.p2.7.m7.2.2.cmml">
           15.80
          </mn>
          <mo id="S4.SS2.p2.7.m7.4.5.3.2.2.3" xref="S4.SS2.p2.7.m7.4.5.3.2.1.cmml">
           ,
          </mo>
          <mn id="S4.SS2.p2.7.m7.3.3" xref="S4.SS2.p2.7.m7.3.3.cmml">
           4.40
          </mn>
          <mo id="S4.SS2.p2.7.m7.4.5.3.2.2.4" xref="S4.SS2.p2.7.m7.4.5.3.2.1.cmml">
           ,
          </mo>
          <mn id="S4.SS2.p2.7.m7.4.4" xref="S4.SS2.p2.7.m7.4.4.cmml">
           1.00
          </mn>
          <mo id="S4.SS2.p2.7.m7.4.5.3.2.2.5" xref="S4.SS2.p2.7.m7.4.5.3.2.1.cmml">
           )
          </mo>
         </mrow>
         <mo id="S4.SS2.p2.7.m7.4.5.3.1" lspace="0em" rspace="0em" xref="S4.SS2.p2.7.m7.4.5.3.1.cmml">
          ​
         </mo>
         <mtext id="S4.SS2.p2.7.m7.4.5.3.3" xref="S4.SS2.p2.7.m7.4.5.3.3a.cmml">
          Kbps
         </mtext>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.4b">
        <apply id="S4.SS2.p2.7.m7.4.5.cmml" xref="S4.SS2.p2.7.m7.4.5">
         <eq id="S4.SS2.p2.7.m7.4.5.1.cmml" xref="S4.SS2.p2.7.m7.4.5.1">
         </eq>
         <ci id="S4.SS2.p2.7.m7.4.5.2.cmml" xref="S4.SS2.p2.7.m7.4.5.2">
          𝑟
         </ci>
         <apply id="S4.SS2.p2.7.m7.4.5.3.cmml" xref="S4.SS2.p2.7.m7.4.5.3">
          <times id="S4.SS2.p2.7.m7.4.5.3.1.cmml" xref="S4.SS2.p2.7.m7.4.5.3.1">
          </times>
          <vector id="S4.SS2.p2.7.m7.4.5.3.2.1.cmml" xref="S4.SS2.p2.7.m7.4.5.3.2.2">
           <cn id="S4.SS2.p2.7.m7.1.1.cmml" type="float" xref="S4.SS2.p2.7.m7.1.1">
            3.50
           </cn>
           <cn id="S4.SS2.p2.7.m7.2.2.cmml" type="float" xref="S4.SS2.p2.7.m7.2.2">
            15.80
           </cn>
           <cn id="S4.SS2.p2.7.m7.3.3.cmml" type="float" xref="S4.SS2.p2.7.m7.3.3">
            4.40
           </cn>
           <cn id="S4.SS2.p2.7.m7.4.4.cmml" type="float" xref="S4.SS2.p2.7.m7.4.4">
            1.00
           </cn>
          </vector>
          <ci id="S4.SS2.p2.7.m7.4.5.3.3a.cmml" xref="S4.SS2.p2.7.m7.4.5.3.3">
           <mtext id="S4.SS2.p2.7.m7.4.5.3.3.cmml" xref="S4.SS2.p2.7.m7.4.5.3.3">
            Kbps
           </mtext>
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.4c">
        r=\left(3.50,15.80,4.40,1.00\right)\text{Kbps}
       </annotation>
      </semantics>
     </math>
     , respectively. We create independent LLM instances for each user. The above environment information is given to each LLM instance in the beginning of the game. Moreover, the users compete for power savings goal under mutual interference. Note that when implemented in a real wireless network, these parameters are measured by on-device perceivers and provided to on-device LLMs. We generate our results using the GPT-4 model.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F3">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="112" id="S4.F3.g1" src="/html/2307.02757/assets/game.png" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 3:
     </span>
     Playing repeated games in an example of power allocation on mobile users to minimize network power consumption.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S4.SS2.p3">
    <p class="ltx_p" id="S4.SS2.p3.5">
     The prompts we created for LLMs to play this game are shown in Fig.
     <a class="ltx_ref" href="#S4.F3" title="Figure 3 ‣ IV-B Case Study: Wireless Energy Saving ‣ IV Applications of Multi-Agent LLM Networks ‣ Wireless Multi-Agent Generative AI: From Connected Intelligence to Collective Intelligence">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     . In the beginning, a prompt consisting of general description, game rules, goal of the base station, and the individual goal of each users is passed to each LLM instance. In each round, the LLM instances are requested to choose a new power level
     <math alttext="p_{i}" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1">
      <semantics id="S4.SS2.p3.1.m1.1a">
       <msub id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">
        <mi id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">
         p
        </mi>
        <mi id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b">
        <apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">
         <csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">
          subscript
         </csymbol>
         <ci id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">
          𝑝
         </ci>
         <ci id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">
        p_{i}
       </annotation>
      </semantics>
     </math>
     , given the actions of all users in the previous round, through a prompt.
The transmission powers chosen by each agent from GPT-4 in each round are illustrated in Fig.
     <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ IV-B Case Study: Wireless Energy Saving ‣ IV Applications of Multi-Agent LLM Networks ‣ Wireless Multi-Agent Generative AI: From Connected Intelligence to Collective Intelligence">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     . The obtained results are compared with the minimum theoretical power required for the transmission rate
     <math alttext="r" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1">
      <semantics id="S4.SS2.p3.2.m2.1a">
       <mi id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">
        r
       </mi>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b">
        <ci id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">
         𝑟
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">
        r
       </annotation>
      </semantics>
     </math>
     . It can be seen that LLM-enabled agents managed to achieve the power saving target
     <math alttext="\Delta p" class="ltx_Math" display="inline" id="S4.SS2.p3.3.m3.1">
      <semantics id="S4.SS2.p3.3.m3.1a">
       <mrow id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">
        <mi id="S4.SS2.p3.3.m3.1.1.2" mathvariant="normal" xref="S4.SS2.p3.3.m3.1.1.2.cmml">
         Δ
        </mi>
        <mo id="S4.SS2.p3.3.m3.1.1.1" lspace="0em" rspace="0em" xref="S4.SS2.p3.3.m3.1.1.1.cmml">
         ​
        </mo>
        <mi id="S4.SS2.p3.3.m3.1.1.3" xref="S4.SS2.p3.3.m3.1.1.3.cmml">
         p
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b">
        <apply id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">
         <times id="S4.SS2.p3.3.m3.1.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1.1">
         </times>
         <ci id="S4.SS2.p3.3.m3.1.1.2.cmml" xref="S4.SS2.p3.3.m3.1.1.2">
          Δ
         </ci>
         <ci id="S4.SS2.p3.3.m3.1.1.3.cmml" xref="S4.SS2.p3.3.m3.1.1.3">
          𝑝
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">
        \Delta p
       </annotation>
      </semantics>
     </math>
     at round 2, with 3 users allocated the theoretical minimum power to keep their transmission rate at above
     <math alttext="r" class="ltx_Math" display="inline" id="S4.SS2.p3.4.m4.1">
      <semantics id="S4.SS2.p3.4.m4.1a">
       <mi id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml">
        r
       </mi>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b">
        <ci id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1">
         𝑟
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">
        r
       </annotation>
      </semantics>
     </math>
     (except user 1). This can be further noticed in Fig.
     <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ IV-B Case Study: Wireless Energy Saving ‣ IV Applications of Multi-Agent LLM Networks ‣ Wireless Multi-Agent Generative AI: From Connected Intelligence to Collective Intelligence">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     which shows the users’ rate difference to
     <math alttext="r" class="ltx_Math" display="inline" id="S4.SS2.p3.5.m5.1">
      <semantics id="S4.SS2.p3.5.m5.1a">
       <mi id="S4.SS2.p3.5.m5.1.1" xref="S4.SS2.p3.5.m5.1.1.cmml">
        r
       </mi>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b">
        <ci id="S4.SS2.p3.5.m5.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1">
         𝑟
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">
        r
       </annotation>
      </semantics>
     </math>
     , where user 1’s rate can be further reduced, and hence, the transmission energy can be reduced. Furthermore, users 1 and 4 violate the goal of minimum transmission rate in round 3. The simulation results in Fig.
     <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ IV-B Case Study: Wireless Energy Saving ‣ IV Applications of Multi-Agent LLM Networks ‣ Wireless Multi-Agent Generative AI: From Connected Intelligence to Collective Intelligence">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     also show that current GPT-4 experiences some difficulties in maintaining multiple goals when the number of agents increases.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p4">
    <p class="ltx_p" id="S4.SS2.p4.1">
     This use-case demonstrates that an LLM (GPT-4) can perform mathematical reasoning in a wireless communication-based problem, in order to achieve a global power saving and individual transmission rates targets.
The takeovers of this study are: 1) LLMs have the potential to analyze a radio environment, formulate and solve a radio network problem to achieve a network operation goal; 2) Multi-agent LLMs can achieve a cooperative goal while keeping their own KPIs; 3) Training a wireless domain-specific LLM is essential for LLMs to understand effectively the wireless network goals (without mathematical prompts), to achieve a fully autonomous intent-driven network.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="408" id="S4.F4.g1" src="/html/2307.02757/assets/power_history.png" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4:
     </span>
     Users’ transmission power given by LLMs, and power to maintain a minimum transmission rate v.s. round for reducing total power by
     <math alttext="5\%" class="ltx_Math" display="inline" id="S4.F4.2.m1.1">
      <semantics id="S4.F4.2.m1.1b">
       <mrow id="S4.F4.2.m1.1.1" xref="S4.F4.2.m1.1.1.cmml">
        <mn id="S4.F4.2.m1.1.1.2" xref="S4.F4.2.m1.1.1.2.cmml">
         5
        </mn>
        <mo id="S4.F4.2.m1.1.1.1" xref="S4.F4.2.m1.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.F4.2.m1.1c">
        <apply id="S4.F4.2.m1.1.1.cmml" xref="S4.F4.2.m1.1.1">
         <csymbol cd="latexml" id="S4.F4.2.m1.1.1.1.cmml" xref="S4.F4.2.m1.1.1.1">
          percent
         </csymbol>
         <cn id="S4.F4.2.m1.1.1.2.cmml" type="integer" xref="S4.F4.2.m1.1.1.2">
          5
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.F4.2.m1.1d">
        5\%
       </annotation>
      </semantics>
     </math>
     at least.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="S4.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="413" id="S4.F5.g1" src="/html/2307.02757/assets/rate_diff_history.png" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5:
     </span>
     The difference of transmission rate w.r.t. minimum rate v.s. round for total power reduction by at least
     <math alttext="5\%" class="ltx_Math" display="inline" id="S4.F5.2.m1.1">
      <semantics id="S4.F5.2.m1.1b">
       <mrow id="S4.F5.2.m1.1.1" xref="S4.F5.2.m1.1.1.cmml">
        <mn id="S4.F5.2.m1.1.1.2" xref="S4.F5.2.m1.1.1.2.cmml">
         5
        </mn>
        <mo id="S4.F5.2.m1.1.1.1" xref="S4.F5.2.m1.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.F5.2.m1.1c">
        <apply id="S4.F5.2.m1.1.1.cmml" xref="S4.F5.2.m1.1.1">
         <csymbol cd="latexml" id="S4.F5.2.m1.1.1.1.cmml" xref="S4.F5.2.m1.1.1.1">
          percent
         </csymbol>
         <cn id="S4.F5.2.m1.1.1.2.cmml" type="integer" xref="S4.F5.2.m1.1.1.2">
          5
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.F5.2.m1.1d">
        5\%
       </annotation>
      </semantics>
     </math>
     .
    </figcaption>
   </figure>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    V
   </span>
   <span class="ltx_text ltx_font_smallcaps" id="S5.1.1">
    Challenges and Opportunities
   </span>
  </h2>
  <section class="ltx_subsection" id="S5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     <span class="ltx_text" id="S5.SS1.4.1.1">
      V-A
     </span>
    </span>
    <span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">
     TelecomLLM with domain knowledge
    </span>
   </h3>
   <div class="ltx_para" id="S5.SS1.p1">
    <p class="ltx_p" id="S5.SS1.p1.1">
     Building a TelecomLLM with domain-specific knowledge is a foundation to enable wireless generative agents. Current GPT-4 can only provide general responses to a wireless network goal described in natural language. To operate an autonomous network, the LLM should give instructions to specific network functions or entities. This requires LLMs to be trained on Telecom domain corpus, such as standard and product specifications. Moreover, grounding and calibrating on-device LLM to real-time wireless environment is important for wireless agents to make optimal decisions on specific wireless tasks. Furthermore, tuning TelecomLLM with emerging wireless knowledge is crucial to keep the model updated with new wireless standards and features, in order to support efficient network upgrade.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     <span class="ltx_text" id="S5.SS2.4.1.1">
      V-B
     </span>
    </span>
    <span class="ltx_text ltx_font_italic" id="S5.SS2.5.2">
     Wireless LLM Hallucination
    </span>
   </h3>
   <div class="ltx_para" id="S5.SS2.p1">
    <p class="ltx_p" id="S5.SS2.p1.1">
     Model hallucination phenomenon refers to the event when the generative model produce content that is nonsensical or untruthful in relation to certain sources. In wireless networks, this can be a result of continuous channel variations and network dynamicity. In particular, model hallucination can be experienced when the generative agent is not capable of capturing the variations of a rapidly evolving network, leading to inaccurate representation of the wireless environment. Furthermore, generative agents may hallucinate due to the impact of inter-agent interference and the lack of relevant on-board data. Hence, efficient hallucination avoidance schemes are needed to ensure trustworthy agent behaviours in different network functionalities.
     <span class="ltx_text" id="S5.SS2.p1.1.1" style="color:#000000;">
     </span>
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS3">
   <h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
    <span class="ltx_tag ltx_tag_subsection">
     <span class="ltx_text" id="S5.SS3.4.1.1">
      V-C
     </span>
    </span>
    <span class="ltx_text ltx_font_italic" id="S5.SS3.5.2">
     Self-replicating Wireless Generative Agents
    </span>
   </h3>
   <div class="ltx_para" id="S5.SS3.p1">
    <p class="ltx_p" id="S5.SS3.p1.1">
     <span class="ltx_text" id="S5.SS3.p1.1.1" style="color:#000000;">
      The concept of self-replication within the context of generative agents indicates their capability of creating copies of themselves, including their knowledge and experiences, to communicate effectively with other agents. While still being a speculative and a hypothetical concept, it represents a promising skill that enables such agents to cope with the growing nature of wireless networks, and achieve autonomously scalable wireless networks. Also, replicated agents can adapt their models to their own wireless environments and tasks. Knowledge driven wireless generative agents and inter-agent communications are key enabling technologies for self-replicating agents. For example, modularized LLMs have the potential to provide flexible knowledge inheritance for different generative agents, with reduced training and communication overhead. This calls for a thorough study in explainable LLM which can encode knowledge on-demand.
It is worthy to emphasize that, although self-replicating generative agents are aimed for improved wireless networks performance, reliable wireless communication is essential for ensuring accurate knowledge transfer, and therefore, to prevent replication errors and agents mutations.
     </span>
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS4">
   <h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
    <span class="ltx_tag ltx_tag_subsection">
     <span class="ltx_text" id="S5.SS4.4.1.1">
      V-D
     </span>
    </span>
    <span class="ltx_text ltx_font_italic" id="S5.SS4.5.2">
     Resource management for on-device LLMs
    </span>
   </h3>
   <div class="ltx_para" id="S5.SS4.p1">
    <p class="ltx_p" id="S5.SS4.p1.1">
     <span class="ltx_text" id="S5.SS4.p1.1.1" style="color:#000000;">
      The inherent limitations on the available resources in wireless networks, particularly IoT systems, represent a performance bottleneck in the efficient realization of generative multi-agent systems in future wireless networks. While being a promising technology, due to the diverse capabilities of wireless nodes, on-device LLM requires a sophisticated balance between computing and energy resources, memory constraints, inference reliability, as well as user’s experience and demands. Accordingly, it is essential to develop optimization and resource-management approaches that can be tailored to a certain use-case, through assessing the resource utilization performance of each agent, and its impact on the CPU, memory, and energy efficiency, while meeting particular requirements imposed by the communication scenario.
     </span>
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS5">
   <h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
    <span class="ltx_tag ltx_tag_subsection">
     <span class="ltx_text" id="S5.SS5.4.1.1">
      V-E
     </span>
    </span>
    <span class="ltx_text ltx_font_italic" id="S5.SS5.5.2">
     Multi-Agent Convergence
    </span>
   </h3>
   <div class="ltx_para" id="S5.SS5.p1">
    <p class="ltx_p" id="S5.SS5.p1.1">
     <span class="ltx_text" id="S5.SS5.p1.1.1" style="color:#000000;">
      Multi-agent convergence in generative systems can be particularly challenging in wireless networks, due to the dynamic nature of the latter, and therefore, the continuous need for agents’ models adaptation to cope up with the evolving network conditions. Such a challenge is particularly pronounced when a massive number of agents are involved in achieving different network goals. Failing to converge might result in suboptimal solutions, which will then necessitate a high level of coordination among the multiple agents, contradicting the goal of achieving self-governance and automation among the agents. Accordingly, this opens up a new horizon for exploring collective approaches to allow the agents to achieve a consensus in a real-time manner, regardless of the network dynamicity condition.
     </span>
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS6">
   <h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
    <span class="ltx_tag ltx_tag_subsection">
     <span class="ltx_text" id="S5.SS6.4.1.1">
      V-F
     </span>
    </span>
    <span class="ltx_text ltx_font_italic" id="S5.SS6.5.2">
     Multi-modal efficient on-device LLMs
    </span>
   </h3>
   <div class="ltx_para" id="S5.SS6.p1">
    <p class="ltx_p" id="S5.SS6.p1.1">
     <span class="ltx_text" id="S5.SS6.p1.1.1" style="color:#000000;">
      Generative agents employed in wireless networks are anticipated to handle multi-modal data, such as text, images, locations, and radio signals. Meanwhile, multi-modal on-device LLM is constrained by communication and computing resources. This raises significant challenges in the model design, where current multi-modal LLMs encoding raw data are too large to be deployed on mobile device. Knowledge based LLM that encodes multi-modal data from a concept space can potentially reduce the model size and inference cost, by learning a minimal structure of data. Research efforts should be devoted to build effective concept space for multi-modal data.
     </span>
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section" style="color:#000000;">
   <span class="ltx_tag ltx_tag_section">
    VI
   </span>
   <span class="ltx_text ltx_font_smallcaps" id="S6.1.1">
    Conclusion
   </span>
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    <span class="ltx_text" id="S6.p1.1.1" style="color:#000000;">
     In this paper, we introduced the concept of multi-agent generative AI network, which exploits the collective intelligence from on-device LLMs. We identified the challenges and key enabling technologies of on-device LLMs, focusing on LLM planning and reasoning to solve complex tasks; multi-agent LLM games and
    </span>
    <abbr class="ltx_glossaryref" title="reinforcement learning">
     <span class="ltx_text ltx_glossary_short">
      RL
     </span>
    </abbr>
    <span class="ltx_text" id="S6.p1.1.2" style="color:#000000;">
     -based LLMs to achieve goals in competition scenarios; and semantic information and communication to connect LLMs with knowledge for effective short-term and long-term task planing. Moreover, we discussed potential applications of multi-agent LLMs in 6G, including intent-driven network automation. We further demonstrated a use case where multi-agent LLMs can play a game to achieve goals of network energy saving and user transmission rate. Finally, we discussed potential research opportunities of multi-agent LLM network such as system 2 ML with strong reasoning, human-agent interaction and collaboration. This paper initiates a new framework for future wireless network design to empower collective intelligence from large scale wireless devices, opening up new research opportunities of generative AI in wireless networks.
    </span>
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography" style="color:#000000;">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_tag_bibitem">
     [1]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib1.1.1" style="color:#000000;">
      G. Yenduri
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2" style="color:#000000;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib1.3.3" style="color:#000000;">
      , “Generative pre-trained transformer: A comprehensive
review on enabling technologies, potential applications, emerging challenges,
and future directions,” 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_tag_bibitem">
     [2]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib2.1.1" style="color:#000000;">
      Falcon LLM.
     </span>
     <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">
      https://falconllm.tii.ae
     </span>
     <span class="ltx_text" id="bib.bib2.2.2" style="color:#000000;">
      .
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_tag_bibitem">
     [3]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib3.1.1" style="color:#000000;">
      H. Touvron
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.2.2" style="color:#000000;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib3.3.3" style="color:#000000;">
      , “LLaMA: Open and efficient foundation language
models,” 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_tag_bibitem">
     [4]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib4.1.1" style="color:#000000;">
      R. Anil
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2" style="color:#000000;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib4.3.3" style="color:#000000;">
      , “PaLM 2 technical report,” 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_tag_bibitem">
     [5]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib5.1.1" style="color:#000000;">
      S. Bubeck
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2" style="color:#000000;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib5.3.3" style="color:#000000;">
      , “Sparks of artificial general intelligence: Early
experiments with GPT-4,” 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_tag_bibitem">
     [6]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib6.1.1" style="color:#000000;">
      A. Vaswani
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.2.2" style="color:#000000;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib6.3.3" style="color:#000000;">
      , “Attention is all you need,” 2017.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_tag_bibitem">
     [7]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib7.1.1" style="color:#000000;">
      C. Chaccour
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2" style="color:#000000;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib7.3.3" style="color:#000000;">
      , “Less data, more knowledge: Building next
generation semantic communication networks,” 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_tag_bibitem">
     [8]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib8.1.1" style="color:#000000;">
      E. Akata
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2" style="color:#000000;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib8.3.3" style="color:#000000;">
      , “Playing repeated games with large language models,”
2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_tag_bibitem">
     [9]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib9.1.1" style="color:#000000;">
      Y. Nakajima, “Task-driven autonomous agent utilizing GPT-4, Pinecone, and
LangChain for diverse applications,” 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_tag_bibitem">
     [10]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib10.1.1" style="color:#000000;">
      Auto-GPT: An autonomous GPT-4 experiment.
     </span>
     <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">
      https://github.com/Significant-Gravitas/Auto-GPT
     </span>
     <span class="ltx_text" id="bib.bib10.2.2" style="color:#000000;">
      .
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_tag_bibitem">
     [11]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib11.1.1" style="color:#000000;">
      Y. Shen
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.2.2" style="color:#000000;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib11.3.3" style="color:#000000;">
      , “Hugginggpt: Solving AI tasks with ChatGPT and its
friends in HuggingFace,” 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_tag_bibitem">
     [12]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib12.1.1" style="color:#000000;">
      G. Li
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.2.2" style="color:#000000;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib12.3.3" style="color:#000000;">
      , “CAMEL: Communicative agents for ”mind” exploration of
large scale language model society,” 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_tag_bibitem">
     [13]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib13.1.1" style="color:#000000;">
      J. S. Park
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2" style="color:#000000;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib13.3.3" style="color:#000000;">
      , “Generative agents: Interactive simulacra of human
behavior,” 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_tag_bibitem">
     [14]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib14.1.1" style="color:#000000;">
      A. Ramesh
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2" style="color:#000000;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib14.3.3" style="color:#000000;">
      , “Zero-shot text-to-image generation,” 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_tag_bibitem">
     [15]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib15.1.1" style="color:#000000;">
      A. Ramesh, P. Dhariwal
     </span>
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2" style="color:#000000;">
      et al.
     </em>
     <span class="ltx_text" id="bib.bib15.3.3" style="color:#000000;">
      , “Hierarchical text-conditional image
generation with clip latents,” 2022.
     </span>
    </span>
   </li>
  </ul>
 </section>
 <section class="ltx_section" id="Sx1">
  <h2 class="ltx_title ltx_font_smallcaps ltx_title_section" style="color:#000000;">
   Biographies
  </h2>
  <div class="ltx_para" id="Sx1.p1">
   <p class="ltx_p" id="Sx1.p1.1">
    <span class="ltx_text ltx_font_bold" id="Sx1.p1.1.1" style="font-size:90%;color:#000000;">
     Hang Zou
     <span class="ltx_text ltx_font_medium" id="Sx1.p1.1.1.1">
      (hang.zou@tii.ae) is a Researcher at Technology Innovation Institute, UAE.
     </span>
    </span>
   </p>
  </div>
  <div class="ltx_para" id="Sx1.p2">
   <p class="ltx_p" id="Sx1.p2.1">
    <span class="ltx_text ltx_font_bold" id="Sx1.p2.1.1" style="font-size:90%;color:#000000;">
     Qiyang Zhao
     <span class="ltx_text ltx_font_medium" id="Sx1.p2.1.1.1">
      (qiyang.zhao@tii.ae) is a Lead Researcher at Technology Innovation Institute, UAE.
     </span>
    </span>
   </p>
  </div>
  <div class="ltx_para" id="Sx1.p3">
   <p class="ltx_p" id="Sx1.p3.1">
    <span class="ltx_text ltx_font_bold" id="Sx1.p3.1.1" style="font-size:90%;color:#000000;">
     Lina Bariah
     <span class="ltx_text ltx_font_medium" id="Sx1.p3.1.1.1">
      (lina.bariah@tii.ae) is a Senior Researcher at Technology Innovation Institute, UAE.
     </span>
    </span>
   </p>
  </div>
  <div class="ltx_para" id="Sx1.p4">
   <p class="ltx_p" id="Sx1.p4.1">
    <span class="ltx_text ltx_font_bold" id="Sx1.p4.1.1" style="font-size:90%;color:#000000;">
     Mehdi Bennis
     <span class="ltx_text ltx_font_medium" id="Sx1.p4.1.1.1">
      (mehdi.bennis@oulu.fi) is a Professor at University of Oulu, Finland.
     </span>
    </span>
   </p>
  </div>
  <div class="ltx_para" id="Sx1.p5">
   <p class="ltx_p" id="Sx1.p5.1">
    <span class="ltx_text ltx_font_bold" id="Sx1.p5.1.1" style="font-size:90%;color:#000000;">
     Mérouane Debbah
     <span class="ltx_text ltx_font_medium" id="Sx1.p5.1.1.1">
      (merouane.debbah@ku.ac.ae) is a Professor at Khalifa University, UAE.
     </span>
    </span>
   </p>
  </div>
 </section>
</article>

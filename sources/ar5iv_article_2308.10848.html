<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    WeizeÂ Chen
    <sup class="ltx_sup" id="id21.21.id1">
     1
    </sup>
    , YushengÂ Su
    <sup class="ltx_sup" id="id22.22.id2">
     <span class="ltx_text ltx_font_italic" id="id22.22.id2.1">
      1âˆ—
     </span>
    </sup>
    , Jingwei Zuo
    <sup class="ltx_sup" id="id23.23.id3">
     <span class="ltx_text ltx_font_italic" id="id23.23.id3.1">
      1
     </span>
    </sup>
    , Cheng Yang
    <sup class="ltx_sup" id="id24.24.id4">
     <span class="ltx_text ltx_font_italic" id="id24.24.id4.1">
      3
     </span>
    </sup>
    <sup class="ltx_sup" id="id25.25.id5">
     ğŸ–‚
    </sup>
    , Chenfei Yuan
    <sup class="ltx_sup" id="id26.26.id6">
     <span class="ltx_text ltx_font_italic" id="id26.26.id6.1">
      1
     </span>
    </sup>
    ,
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_bold" id="id6.6.1">
     Chi-Min Chan
     <sup class="ltx_sup" id="id6.6.1.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id6.6.1.1.1">
       1
      </span>
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id7.7.2">
     Heyang Yu
     <sup class="ltx_sup" id="id7.7.2.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.2.1.1">
       1
      </span>
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id8.8.3">
     Yaxi Lu
     <sup class="ltx_sup" id="id8.8.3.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id8.8.3.1.1">
       1
      </span>
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id9.9.4">
     Yi-Hsin Hung
     <sup class="ltx_sup" id="id9.9.4.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id9.9.4.1.1">
       2
      </span>
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id10.10.5">
     Chen Qian
     <sup class="ltx_sup" id="id10.10.5.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id10.10.5.1.1">
       1
      </span>
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id11.11.6">
     Yujia Qin
     <sup class="ltx_sup" id="id11.11.6.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id11.11.6.1.1">
       1
      </span>
     </sup>
    </span>
    ,
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_bold" id="id12.12.7">
     Xin Cong
     <sup class="ltx_sup" id="id12.12.7.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id12.12.7.1.1">
       1
      </span>
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id13.13.8">
     Ruobing Xie
     <sup class="ltx_sup" id="id13.13.8.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id13.13.8.1.1">
       4
      </span>
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id14.14.9">
     Zhiyuan Liu
     <sup class="ltx_sup" id="id14.14.9.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id14.14.9.1.1">
       1
      </span>
     </sup>
     <sup class="ltx_sup" id="id14.14.9.2">
      <span class="ltx_text ltx_font_medium" id="id14.14.9.2.1">
       ğŸ–‚
      </span>
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id15.15.10">
     Maosong Sun
     <sup class="ltx_sup" id="id15.15.10.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id15.15.10.1.1">
       1
      </span>
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id17.17.12">
     Jie Zhou
     <sup class="ltx_sup" id="id17.17.12.1">
      <span class="ltx_text ltx_font_medium" id="id17.17.12.1.1">
       4
      </span>
     </sup>
     <br class="ltx_break"/>
     <sup class="ltx_sup" id="id17.17.12.2">
      <span class="ltx_text ltx_font_medium" id="id17.17.12.2.1">
       1
      </span>
     </sup>
    </span>
    Department of Computer Science and Technology, Tsinghua University
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id27.27.id7">
     2
    </sup>
    School of Economics and Management, Tsinghua University
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id28.28.id8">
     3
    </sup>
    School of Computer Science, Beijing University of Posts and Telecommunications
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id29.29.id9">
     4
    </sup>
    Pattern Recognition Center, WeChat AI, Tencent Inc.
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id30.30.id10">
     chenwz21@mails.tsinghua.edu.cn
    </span>
    ,
    <span class="ltx_text ltx_font_typewriter" id="id31.31.id11">
     yushengsu.thu@gmail.com
     <br class="ltx_break"/>
    </span>
   </span>
   <span class="ltx_author_notes">
    The first two authors contributed equally. â€ƒğŸ–‚Â Corresponding author.
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id32.id1">
   Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework
   <span class="ltx_text ltx_font_smallcaps" id="id32.id1.1">
    AgentVerse
   </span>
   that can effectively orchestrate a collaborative group of expert agents as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that
   <span class="ltx_text ltx_font_smallcaps" id="id32.id1.2">
    AgentVerse
   </span>
   can proficiently deploy multi-agent groups that outperform a single agent. Extensive experiments on text understanding, reasoning, coding, tool utilization, and embodied AI confirm the effectiveness of
   <span class="ltx_text ltx_font_smallcaps" id="id32.id1.3">
    AgentVerse
   </span>
   . Moreover, our analysis of agent interactions within
   <span class="ltx_text ltx_font_smallcaps" id="id32.id1.4">
    AgentVerse
   </span>
   reveals the emergence of specific collaborative behaviors, contributing to heightened group efficiency.
Our code has been released at
   <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenBMB/AgentVerse/" target="_blank" title="">
    https://github.com/OpenBMB/AgentVerse/
   </a>
   .
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para ltx_noindent" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    The pursuit of creating intelligent and autonomous agents that can seamlessly assist humans and operate in real-world settings has been a foundational goal in artificial intelligence
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wooldridge &amp; Jennings,
     <a class="ltx_ref" href="#bib.bib49" title="">
      1995
     </a>
     ; Minsky,
     <a class="ltx_ref" href="#bib.bib20" title="">
      1988
     </a>
     ; Bubeck etÂ al.,
     <a class="ltx_ref" href="#bib.bib5" title="">
      2023
     </a>
     )
    </cite>
    .
The recent advance of Large Language Models (LLMs)
    <cite class="ltx_cite ltx_citemacro_citep">
     (OpenAI,
     <a class="ltx_ref" href="#bib.bib24" title="">
      2023a
     </a>
     ; Anil etÂ al.,
     <a class="ltx_ref" href="#bib.bib3" title="">
      2023
     </a>
     ; Touvron etÂ al.,
     <a class="ltx_ref" href="#bib.bib43" title="">
      2023b
     </a>
     )
    </cite>
    has created newfound avenues in this domain. These LLMs, especially GPT-4
    <cite class="ltx_cite ltx_citemacro_citep">
     (OpenAI,
     <a class="ltx_ref" href="#bib.bib24" title="">
      2023a
     </a>
     )
    </cite>
    , are particularly adept in comprehending human intent and executing commands. They have demonstrated remarkable proficiency in domains such as language understanding, vision
    <cite class="ltx_cite ltx_citemacro_citep">
     (OpenAI,
     <a class="ltx_ref" href="#bib.bib25" title="">
      2023b
     </a>
     )
    </cite>
    , and coding
    <cite class="ltx_cite ltx_citemacro_citep">
     (Bubeck etÂ al.,
     <a class="ltx_ref" href="#bib.bib5" title="">
      2023
     </a>
     )
    </cite>
    .
By harnessing the power of LLMs, autonomous agents can make more nuanced decisions and perform actions with an unprecedented degree of autonomy
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zhou etÂ al.,
     <a class="ltx_ref" href="#bib.bib57" title="">
      2023
     </a>
     )
    </cite>
    . Agents like AutoGPT
    <cite class="ltx_cite ltx_citemacro_citep">
     (Richards &amp; etÂ al.,
     <a class="ltx_ref" href="#bib.bib33" title="">
      2023
     </a>
     )
    </cite>
    , BabyAGI
    <cite class="ltx_cite ltx_citemacro_citep">
     (Nakajima,
     <a class="ltx_ref" href="#bib.bib22" title="">
      2023
     </a>
     )
    </cite>
    , and AgentGPT
    <cite class="ltx_cite ltx_citemacro_citep">
     (Reworkd,
     <a class="ltx_ref" href="#bib.bib32" title="">
      2023
     </a>
     )
    </cite>
    , are inspiring examples. Furthermore, recent research has endowed autonomous agents with more human-analogous cognitive mechanisms, spanning from reflection
    <cite class="ltx_cite ltx_citemacro_citep">
     (Yao etÂ al.,
     <a class="ltx_ref" href="#bib.bib54" title="">
      2023b
     </a>
     ; Shinn etÂ al.,
     <a class="ltx_ref" href="#bib.bib38" title="">
      2023
     </a>
     )
    </cite>
    , task decomposition
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wei etÂ al.,
     <a class="ltx_ref" href="#bib.bib47" title="">
      2022b
     </a>
     ; Yao etÂ al.,
     <a class="ltx_ref" href="#bib.bib53" title="">
      2023a
     </a>
     )
    </cite>
    , and tool utilization
    <cite class="ltx_cite ltx_citemacro_citep">
     (Schick etÂ al.,
     <a class="ltx_ref" href="#bib.bib36" title="">
      2023b
     </a>
     ; Qin etÂ al.,
     <a class="ltx_ref" href="#bib.bib30" title="">
      2023a
     </a>
     ;
     <a class="ltx_ref" href="#bib.bib31" title="">
      b
     </a>
     ; Qian etÂ al.,
     <a class="ltx_ref" href="#bib.bib29" title="">
      2023b
     </a>
     )
    </cite>
    . These advancements edge us closer to realizing the concept of artificial general intelligence (AGI)
    <cite class="ltx_cite ltx_citemacro_citep">
     (Goertzel &amp; Pennachin,
     <a class="ltx_ref" href="#bib.bib15" title="">
      2007
     </a>
     ; Clune,
     <a class="ltx_ref" href="#bib.bib10" title="">
      2019
     </a>
     )
    </cite>
    that can generalize across a broader range of tasks.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    However, complex real-world tasks often require cooperation among individuals to achieve better effectiveness. Throughout history, numerous studies have delved into methods for enhancing collaboration among humans to improve work efficiency and effectiveness
    <cite class="ltx_cite ltx_citemacro_citep">
     (Woolley etÂ al.,
     <a class="ltx_ref" href="#bib.bib50" title="">
      2010
     </a>
     ; Fehr &amp; GÃ¤chter,
     <a class="ltx_ref" href="#bib.bib14" title="">
      2000
     </a>
     )
    </cite>
    . More recently, with the evolution of autonomous agents towards AGI, extensive research conceptualizes the assemblies of agents as a society or group
    <cite class="ltx_cite ltx_citemacro_citep">
     (Li etÂ al.,
     <a class="ltx_ref" href="#bib.bib16" title="">
      2023
     </a>
     )
    </cite>
    , and focuses on exploring the potential of their cooperation. For example,
    <cite class="ltx_cite ltx_citemacro_cite">
     Park etÂ al. (
     <a class="ltx_ref" href="#bib.bib26" title="">
      2023
     </a>
     )
    </cite>
    found emergent social behaviors in multi-agent life simulation.
    <cite class="ltx_cite ltx_citemacro_cite">
     Du etÂ al. (
     <a class="ltx_ref" href="#bib.bib13" title="">
      2023
     </a>
     ); Wang etÂ al. (
     <a class="ltx_ref" href="#bib.bib45" title="">
      2023b
     </a>
     ); Zhang etÂ al. (
     <a class="ltx_ref" href="#bib.bib55" title="">
      2023a
     </a>
     ); Qian etÂ al. (
     <a class="ltx_ref" href="#bib.bib28" title="">
      2023a
     </a>
     ); Chan etÂ al. (
     <a class="ltx_ref" href="#bib.bib6" title="">
      2023
     </a>
     )
    </cite>
    also underscored the enhanced decision-making of collaborating agents during collaborative problem-solving.
However, a limitation in these studies is their narrow focus on specific and limited tasks, leaving the generalizability of their findings uncertain. An additional constraint is their static approach to agent collaboration, where agentsâ€™ roles and capabilities remain rigid, hindering adaptability.
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="230" id="S1.F1.g1" src="/html/2308.10848/assets/x1.png" width="401"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    An illustration of the
    <span class="ltx_text ltx_font_smallcaps" id="S1.F1.2.1">
     AgentVerse
    </span>
    .
   </figcaption>
  </figure>
  <div class="ltx_para ltx_noindent" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    To address this problem, we introduce
    <span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.1">
     AgentVerse
    </span>
    . This general multi-agent framework simulates the problem-solving procedures of human groups, and allows for dynamic adjustment of group members based on current progress.
Specifically,
    <span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.2">
     AgentVerse
    </span>
    splits the problem-solving process into four pivotal stages as shown in
    <a class="ltx_ref" href="#S1.F1" title="In 1 Introduction â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      Figure
     </span>
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    :
(1)
    <span class="ltx_text ltx_font_italic" id="S1.p3.1.3">
     Expert Recruitment
    </span>
    : Determine and adjust the agent groupâ€™s composition based on the ongoing problem-solving progression.
(2)
    <span class="ltx_text ltx_font_italic" id="S1.p3.1.4">
     Collaborative Decision-Making
    </span>
    : Engage the selected agents in joint discussions to devise problem-solving strategies.
(3)
    <span class="ltx_text ltx_font_italic" id="S1.p3.1.5">
     Action Execution
    </span>
    : Agents interact with their environment to implement the devised actions.
(4)
    <span class="ltx_text ltx_font_italic" id="S1.p3.1.6">
     Evaluation
    </span>
    - Assess the differences between the current state and desired outcomes. If the current state is unsatisfactory, feedback is given to the next iteration for further refinement.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    We conduct extensive experiments and case studies in diverse aspects including text understanding, reasoning, coding, tool utilization and embodied AI to show the effectiveness of
    <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.1">
     AgentVerse
    </span>
    . Additionally, we highlight the social behaviors that emerge from the multi-agent collaboration, and discuss their advantages and potential risks. In summary, our contributions are:
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p5">
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      â€¢
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       Inspired by the collaborative process of a human team, we propose
       <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i1.p1.1.1">
        AgentVerse
       </span>
       as
       <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.2">
        an effective framework for promoting collaboration among multiple agents
       </span>
       in problem-solving.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      â€¢
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       We conduct extensive experiments to show that
       <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i2.p1.1.1">
        AgentVerse
       </span>
       effectively improve the agentsâ€™ understanding, reasoning, coding, tool utilizing capabilities and their potential in embodied AI.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      â€¢
     </span>
     <div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
      <p class="ltx_p" id="S1.I1.i3.p1.1">
       In the multi-agent collaboration, especially within tool utilization and Minecraft game playing, agents manifest certain emergent behaviors. For example, (1)
       <span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.1">
        volunteer behaviors
       </span>
       , characterized by agents offering assistance to peers, thus improving team efficiency; (2)
       <span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.2">
        conformity behaviors
       </span>
       , where agents adjust their deviated behaviors to align with the common goal under the critics from others; (3)
       <span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.3">
        destructive behaviors
       </span>
       , occasionally leading to undesired and detrimental outcomes.
      </p>
     </div>
    </li>
   </ul>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   AgentVerse Framework
  </h2>
  <div class="ltx_para ltx_noindent" id="S2.p1">
   <p class="ltx_p" id="S2.p1.6">
    A problem-solving process is a sequence of iterative stages within a human group
    <cite class="ltx_cite ltx_citemacro_citep">
     (Bransford &amp; Stein,
     <a class="ltx_ref" href="#bib.bib4" title="">
      1993
     </a>
     )
    </cite>
    . Initially, the group assesses the difference between the current state and the desired goal, dynamically adjusting its composition to enhance collaboration in decision-making, and subsequently executing well-informed actions. In order to enhance the effectiveness of an autonomous multi-agent group in achieving their goals, we simulate the problem-solving processes of a human group to propose the
    <span class="ltx_text ltx_font_smallcaps" id="S2.p1.6.1">
     AgentVerse
    </span>
    framework, which is composed of four crucial stages:
    <span class="ltx_text ltx_font_bold" id="S2.p1.6.2">
     Expert Recruitment
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="S2.p1.6.3">
     Collaborative Decision-Making
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="S2.p1.6.4">
     Action Execution
    </span>
    , and
    <span class="ltx_text ltx_font_bold" id="S2.p1.6.5">
     Evaluation
    </span>
    , as shown in
    <a class="ltx_ref" href="#S1.F1" title="In 1 Introduction â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      Figure
     </span>
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    .
The entire process can be modeled as a Markov decision process (MDP), characterized as a tuple
    <math alttext="({\mathcal{S}},{\mathcal{A}},{\mathcal{T}},{\mathcal{R}},{\mathcal{G}})" class="ltx_Math" display="inline" id="S2.p1.1.m1.5">
     <semantics id="S2.p1.1.m1.5a">
      <mrow id="S2.p1.1.m1.5.6.2" xref="S2.p1.1.m1.5.6.1.cmml">
       <mo id="S2.p1.1.m1.5.6.2.1" stretchy="false" xref="S2.p1.1.m1.5.6.1.cmml">
        (
       </mo>
       <mi class="ltx_font_mathcaligraphic" id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">
        ğ’®
       </mi>
       <mo id="S2.p1.1.m1.5.6.2.2" xref="S2.p1.1.m1.5.6.1.cmml">
        ,
       </mo>
       <mi class="ltx_font_mathcaligraphic" id="S2.p1.1.m1.2.2" xref="S2.p1.1.m1.2.2.cmml">
        ğ’œ
       </mi>
       <mo id="S2.p1.1.m1.5.6.2.3" xref="S2.p1.1.m1.5.6.1.cmml">
        ,
       </mo>
       <mi class="ltx_font_mathcaligraphic" id="S2.p1.1.m1.3.3" xref="S2.p1.1.m1.3.3.cmml">
        ğ’¯
       </mi>
       <mo id="S2.p1.1.m1.5.6.2.4" xref="S2.p1.1.m1.5.6.1.cmml">
        ,
       </mo>
       <mi class="ltx_font_mathcaligraphic" id="S2.p1.1.m1.4.4" xref="S2.p1.1.m1.4.4.cmml">
        â„›
       </mi>
       <mo id="S2.p1.1.m1.5.6.2.5" xref="S2.p1.1.m1.5.6.1.cmml">
        ,
       </mo>
       <mi class="ltx_font_mathcaligraphic" id="S2.p1.1.m1.5.5" xref="S2.p1.1.m1.5.5.cmml">
        ğ’¢
       </mi>
       <mo id="S2.p1.1.m1.5.6.2.6" stretchy="false" xref="S2.p1.1.m1.5.6.1.cmml">
        )
       </mo>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.5b">
       <vector id="S2.p1.1.m1.5.6.1.cmml" xref="S2.p1.1.m1.5.6.2">
        <ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">
         ğ’®
        </ci>
        <ci id="S2.p1.1.m1.2.2.cmml" xref="S2.p1.1.m1.2.2">
         ğ’œ
        </ci>
        <ci id="S2.p1.1.m1.3.3.cmml" xref="S2.p1.1.m1.3.3">
         ğ’¯
        </ci>
        <ci id="S2.p1.1.m1.4.4.cmml" xref="S2.p1.1.m1.4.4">
         â„›
        </ci>
        <ci id="S2.p1.1.m1.5.5.cmml" xref="S2.p1.1.m1.5.5">
         ğ’¢
        </ci>
       </vector>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S2.p1.1.m1.5c">
       ({\mathcal{S}},{\mathcal{A}},{\mathcal{T}},{\mathcal{R}},{\mathcal{G}})
      </annotation>
     </semantics>
    </math>
    . This encompasses the autonomous agent and environment state space
    <math alttext="{\mathcal{S}}" class="ltx_Math" display="inline" id="S2.p1.2.m2.1">
     <semantics id="S2.p1.2.m2.1a">
      <mi class="ltx_font_mathcaligraphic" id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">
       ğ’®
      </mi>
      <annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b">
       <ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">
        ğ’®
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">
       {\mathcal{S}}
      </annotation>
     </semantics>
    </math>
    , solution and action space
    <math alttext="{\mathcal{A}}" class="ltx_Math" display="inline" id="S2.p1.3.m3.1">
     <semantics id="S2.p1.3.m3.1a">
      <mi class="ltx_font_mathcaligraphic" id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">
       ğ’œ
      </mi>
      <annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b">
       <ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">
        ğ’œ
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">
       {\mathcal{A}}
      </annotation>
     </semantics>
    </math>
    , transition function
    <math alttext="{\mathcal{T}}:{\mathcal{S}}\times{\mathcal{A}}\rightarrow{\mathcal{S}}" class="ltx_Math" display="inline" id="S2.p1.4.m4.1">
     <semantics id="S2.p1.4.m4.1a">
      <mrow id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">
       <mi class="ltx_font_mathcaligraphic" id="S2.p1.4.m4.1.1.2" xref="S2.p1.4.m4.1.1.2.cmml">
        ğ’¯
       </mi>
       <mo id="S2.p1.4.m4.1.1.1" lspace="0.278em" rspace="0.278em" xref="S2.p1.4.m4.1.1.1.cmml">
        :
       </mo>
       <mrow id="S2.p1.4.m4.1.1.3" xref="S2.p1.4.m4.1.1.3.cmml">
        <mrow id="S2.p1.4.m4.1.1.3.2" xref="S2.p1.4.m4.1.1.3.2.cmml">
         <mi class="ltx_font_mathcaligraphic" id="S2.p1.4.m4.1.1.3.2.2" xref="S2.p1.4.m4.1.1.3.2.2.cmml">
          ğ’®
         </mi>
         <mo id="S2.p1.4.m4.1.1.3.2.1" lspace="0.222em" rspace="0.222em" xref="S2.p1.4.m4.1.1.3.2.1.cmml">
          Ã—
         </mo>
         <mi class="ltx_font_mathcaligraphic" id="S2.p1.4.m4.1.1.3.2.3" xref="S2.p1.4.m4.1.1.3.2.3.cmml">
          ğ’œ
         </mi>
        </mrow>
        <mo id="S2.p1.4.m4.1.1.3.1" stretchy="false" xref="S2.p1.4.m4.1.1.3.1.cmml">
         â†’
        </mo>
        <mi class="ltx_font_mathcaligraphic" id="S2.p1.4.m4.1.1.3.3" xref="S2.p1.4.m4.1.1.3.3.cmml">
         ğ’®
        </mi>
       </mrow>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b">
       <apply id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1">
        <ci id="S2.p1.4.m4.1.1.1.cmml" xref="S2.p1.4.m4.1.1.1">
         :
        </ci>
        <ci id="S2.p1.4.m4.1.1.2.cmml" xref="S2.p1.4.m4.1.1.2">
         ğ’¯
        </ci>
        <apply id="S2.p1.4.m4.1.1.3.cmml" xref="S2.p1.4.m4.1.1.3">
         <ci id="S2.p1.4.m4.1.1.3.1.cmml" xref="S2.p1.4.m4.1.1.3.1">
          â†’
         </ci>
         <apply id="S2.p1.4.m4.1.1.3.2.cmml" xref="S2.p1.4.m4.1.1.3.2">
          <times id="S2.p1.4.m4.1.1.3.2.1.cmml" xref="S2.p1.4.m4.1.1.3.2.1">
          </times>
          <ci id="S2.p1.4.m4.1.1.3.2.2.cmml" xref="S2.p1.4.m4.1.1.3.2.2">
           ğ’®
          </ci>
          <ci id="S2.p1.4.m4.1.1.3.2.3.cmml" xref="S2.p1.4.m4.1.1.3.2.3">
           ğ’œ
          </ci>
         </apply>
         <ci id="S2.p1.4.m4.1.1.3.3.cmml" xref="S2.p1.4.m4.1.1.3.3">
          ğ’®
         </ci>
        </apply>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">
       {\mathcal{T}}:{\mathcal{S}}\times{\mathcal{A}}\rightarrow{\mathcal{S}}
      </annotation>
     </semantics>
    </math>
    , reward function
    <math alttext="{\mathcal{R}}" class="ltx_Math" display="inline" id="S2.p1.5.m5.1">
     <semantics id="S2.p1.5.m5.1a">
      <mi class="ltx_font_mathcaligraphic" id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">
       â„›
      </mi>
      <annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b">
       <ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">
        â„›
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">
       {\mathcal{R}}
      </annotation>
     </semantics>
    </math>
    , and goal space
    <math alttext="{\mathcal{G}}" class="ltx_Math" display="inline" id="S2.p1.6.m6.1">
     <semantics id="S2.p1.6.m6.1a">
      <mi class="ltx_font_mathcaligraphic" id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml">
       ğ’¢
      </mi>
      <annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b">
       <ci id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1">
        ğ’¢
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">
       {\mathcal{G}}
      </annotation>
     </semantics>
    </math>
    .
   </p>
  </div>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    Expert Recruitment
   </h3>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     Expert Recruitment stage determines the composition of a multi-agent group, playing an important role in deciding the upper bounds of the groupâ€™s capabilities. Empirical evidence suggests that diversity within human groups introduces varied viewpoints, enhancing the groupâ€™s performance across different tasks
     <cite class="ltx_cite ltx_citemacro_citep">
      (Woolley etÂ al.,
      <a class="ltx_ref" href="#bib.bib51" title="">
       2015
      </a>
      ; Phillips &amp; Oâ€™Reilly,
      <a class="ltx_ref" href="#bib.bib27" title="">
       1998
      </a>
      )
     </cite>
     . Parallel findings from recent research suggest that designating specific roles for autonomous agents, similar to recruiting experts to form a group, can augment their efficacy
     <cite class="ltx_cite ltx_citemacro_citep">
      (Li etÂ al.,
      <a class="ltx_ref" href="#bib.bib16" title="">
       2023
      </a>
      ; Salewski etÂ al.,
      <a class="ltx_ref" href="#bib.bib34" title="">
       2023
      </a>
      ; Qian etÂ al.,
      <a class="ltx_ref" href="#bib.bib28" title="">
       2023a
      </a>
      )
     </cite>
     . Current methodologies for assigning role descriptions to autonomous agents predominantly involve manual assignment, necessitating prior knowledge and understanding of the task. Consequently, the scalability remains ambiguous, especially in the face of diverse and intricate problem contexts.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p2">
    <p class="ltx_p" id="S2.SS1.p2.6">
     In view of this,
     <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p2.6.1">
      AgentVerse
     </span>
     automates expert recruitment to make agent configuration more scalable. For a given goal
     <math alttext="g\in{\mathcal{G}}" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.1">
      <semantics id="S2.SS1.p2.1.m1.1a">
       <mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">
        <mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">
         g
        </mi>
        <mo id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">
         âˆˆ
        </mo>
        <mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml">
         ğ’¢
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b">
        <apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">
         <in id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1">
         </in>
         <ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">
          ğ‘”
         </ci>
         <ci id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3">
          ğ’¢
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">
        g\in{\mathcal{G}}
       </annotation>
      </semantics>
     </math>
     , a particular agent
     <math alttext="M_{r}" class="ltx_Math" display="inline" id="S2.SS1.p2.2.m2.1">
      <semantics id="S2.SS1.p2.2.m2.1a">
       <msub id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">
        <mi id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml">
         M
        </mi>
        <mi id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml">
         r
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b">
        <apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">
         <csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">
          subscript
         </csymbol>
         <ci id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2">
          ğ‘€
         </ci>
         <ci id="S2.SS1.p2.2.m2.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3">
          ğ‘Ÿ
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">
        M_{r}
       </annotation>
      </semantics>
     </math>
     is prompted as the â€recruiterâ€, similar to a human resource manager. Instead of relying on pre-defined expert descriptions,
     <math alttext="M_{r}" class="ltx_Math" display="inline" id="S2.SS1.p2.3.m3.1">
      <semantics id="S2.SS1.p2.3.m3.1a">
       <msub id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">
        <mi id="S2.SS1.p2.3.m3.1.1.2" xref="S2.SS1.p2.3.m3.1.1.2.cmml">
         M
        </mi>
        <mi id="S2.SS1.p2.3.m3.1.1.3" xref="S2.SS1.p2.3.m3.1.1.3.cmml">
         r
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b">
        <apply id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">
         <csymbol cd="ambiguous" id="S2.SS1.p2.3.m3.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">
          subscript
         </csymbol>
         <ci id="S2.SS1.p2.3.m3.1.1.2.cmml" xref="S2.SS1.p2.3.m3.1.1.2">
          ğ‘€
         </ci>
         <ci id="S2.SS1.p2.3.m3.1.1.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3">
          ğ‘Ÿ
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">
        M_{r}
       </annotation>
      </semantics>
     </math>
     dynamically generates a set of expert descriptions based on
     <math alttext="g" class="ltx_Math" display="inline" id="S2.SS1.p2.4.m4.1">
      <semantics id="S2.SS1.p2.4.m4.1a">
       <mi id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml">
        g
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b">
        <ci id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">
         ğ‘”
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">
        g
       </annotation>
      </semantics>
     </math>
     . The different agents prompted with these different expert descriptions then form an expert group
     <math alttext="{\mathcal{M}}=M_{r}(g)" class="ltx_Math" display="inline" id="S2.SS1.p2.5.m5.1">
      <semantics id="S2.SS1.p2.5.m5.1a">
       <mrow id="S2.SS1.p2.5.m5.1.2" xref="S2.SS1.p2.5.m5.1.2.cmml">
        <mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.5.m5.1.2.2" xref="S2.SS1.p2.5.m5.1.2.2.cmml">
         â„³
        </mi>
        <mo id="S2.SS1.p2.5.m5.1.2.1" xref="S2.SS1.p2.5.m5.1.2.1.cmml">
         =
        </mo>
        <mrow id="S2.SS1.p2.5.m5.1.2.3" xref="S2.SS1.p2.5.m5.1.2.3.cmml">
         <msub id="S2.SS1.p2.5.m5.1.2.3.2" xref="S2.SS1.p2.5.m5.1.2.3.2.cmml">
          <mi id="S2.SS1.p2.5.m5.1.2.3.2.2" xref="S2.SS1.p2.5.m5.1.2.3.2.2.cmml">
           M
          </mi>
          <mi id="S2.SS1.p2.5.m5.1.2.3.2.3" xref="S2.SS1.p2.5.m5.1.2.3.2.3.cmml">
           r
          </mi>
         </msub>
         <mo id="S2.SS1.p2.5.m5.1.2.3.1" lspace="0em" rspace="0em" xref="S2.SS1.p2.5.m5.1.2.3.1.cmml">
          â€‹
         </mo>
         <mrow id="S2.SS1.p2.5.m5.1.2.3.3.2" xref="S2.SS1.p2.5.m5.1.2.3.cmml">
          <mo id="S2.SS1.p2.5.m5.1.2.3.3.2.1" stretchy="false" xref="S2.SS1.p2.5.m5.1.2.3.cmml">
           (
          </mo>
          <mi id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">
           g
          </mi>
          <mo id="S2.SS1.p2.5.m5.1.2.3.3.2.2" stretchy="false" xref="S2.SS1.p2.5.m5.1.2.3.cmml">
           )
          </mo>
         </mrow>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b">
        <apply id="S2.SS1.p2.5.m5.1.2.cmml" xref="S2.SS1.p2.5.m5.1.2">
         <eq id="S2.SS1.p2.5.m5.1.2.1.cmml" xref="S2.SS1.p2.5.m5.1.2.1">
         </eq>
         <ci id="S2.SS1.p2.5.m5.1.2.2.cmml" xref="S2.SS1.p2.5.m5.1.2.2">
          â„³
         </ci>
         <apply id="S2.SS1.p2.5.m5.1.2.3.cmml" xref="S2.SS1.p2.5.m5.1.2.3">
          <times id="S2.SS1.p2.5.m5.1.2.3.1.cmml" xref="S2.SS1.p2.5.m5.1.2.3.1">
          </times>
          <apply id="S2.SS1.p2.5.m5.1.2.3.2.cmml" xref="S2.SS1.p2.5.m5.1.2.3.2">
           <csymbol cd="ambiguous" id="S2.SS1.p2.5.m5.1.2.3.2.1.cmml" xref="S2.SS1.p2.5.m5.1.2.3.2">
            subscript
           </csymbol>
           <ci id="S2.SS1.p2.5.m5.1.2.3.2.2.cmml" xref="S2.SS1.p2.5.m5.1.2.3.2.2">
            ğ‘€
           </ci>
           <ci id="S2.SS1.p2.5.m5.1.2.3.2.3.cmml" xref="S2.SS1.p2.5.m5.1.2.3.2.3">
            ğ‘Ÿ
           </ci>
          </apply>
          <ci id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">
           ğ‘”
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">
        {\mathcal{M}}=M_{r}(g)
       </annotation>
      </semantics>
     </math>
     on the given goal
     <math alttext="g" class="ltx_Math" display="inline" id="S2.SS1.p2.6.m6.1">
      <semantics id="S2.SS1.p2.6.m6.1a">
       <mi id="S2.SS1.p2.6.m6.1.1" xref="S2.SS1.p2.6.m6.1.1.cmml">
        g
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m6.1b">
        <ci id="S2.SS1.p2.6.m6.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1">
         ğ‘”
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS1.p2.6.m6.1c">
        g
       </annotation>
      </semantics>
     </math>
     . Notably, the composition of a multi-agent group will be dynamically adjusted based on feedback from the evaluation stage (
     <a class="ltx_ref" href="#S2.SS4" title="2.4 Evaluation â€£ 2 AgentVerse Framework â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Section
      </span>
      <span class="ltx_text ltx_ref_tag">
       2.4
      </span>
     </a>
     ). This allows
     <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p2.6.2">
      AgentVerse
     </span>
     to employ the most suitable group based on the current state to make better decisions in future rounds.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Collaborative Decision-making
   </h3>
   <div class="ltx_para ltx_noindent" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     This stage engages expert agents in collaborative decision-making. To facilitate effective decision-making, previous research has investigated the impact of different communication structures among agents
     <cite class="ltx_cite ltx_citemacro_citep">
      (Chan etÂ al.,
      <a class="ltx_ref" href="#bib.bib6" title="">
       2023
      </a>
      ; Zhang etÂ al.,
      <a class="ltx_ref" href="#bib.bib56" title="">
       2023b
      </a>
      ; Wu etÂ al.,
      <a class="ltx_ref" href="#bib.bib52" title="">
       2023
      </a>
      )
     </cite>
     . We focus on two typical communication structures:
     <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">
      horizontal structure
     </span>
     and
     <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">
      vertical structure
     </span>
     , respectively.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS2.p2">
    <p class="ltx_p" id="S2.SS2.p2.5">
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">
      Horizontal Structure (
      <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S2.SS2.p2.1.1.1" style="width:39.0pt;">
       <span class="ltx_p" id="S2.SS2.p2.1.1.1.1">
        <span class="ltx_text" id="S2.SS2.p2.1.1.1.1.1" style="position:relative; bottom:-7.2pt;">
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="268" id="S2.SS2.p2.1.1.1.1.1.g1" src="/html/2308.10848/assets/x2.png" width="461"/>
        </span>
       </span>
      </span>
      )
     </span>
     In this democratic structure, each agent, denoted as
     <math alttext="m_{i}\in{\mathcal{M}}" class="ltx_Math" display="inline" id="S2.SS2.p2.2.m1.1">
      <semantics id="S2.SS2.p2.2.m1.1a">
       <mrow id="S2.SS2.p2.2.m1.1.1" xref="S2.SS2.p2.2.m1.1.1.cmml">
        <msub id="S2.SS2.p2.2.m1.1.1.2" xref="S2.SS2.p2.2.m1.1.1.2.cmml">
         <mi id="S2.SS2.p2.2.m1.1.1.2.2" xref="S2.SS2.p2.2.m1.1.1.2.2.cmml">
          m
         </mi>
         <mi id="S2.SS2.p2.2.m1.1.1.2.3" xref="S2.SS2.p2.2.m1.1.1.2.3.cmml">
          i
         </mi>
        </msub>
        <mo id="S2.SS2.p2.2.m1.1.1.1" xref="S2.SS2.p2.2.m1.1.1.1.cmml">
         âˆˆ
        </mo>
        <mi class="ltx_font_mathcaligraphic" id="S2.SS2.p2.2.m1.1.1.3" xref="S2.SS2.p2.2.m1.1.1.3.cmml">
         â„³
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m1.1b">
        <apply id="S2.SS2.p2.2.m1.1.1.cmml" xref="S2.SS2.p2.2.m1.1.1">
         <in id="S2.SS2.p2.2.m1.1.1.1.cmml" xref="S2.SS2.p2.2.m1.1.1.1">
         </in>
         <apply id="S2.SS2.p2.2.m1.1.1.2.cmml" xref="S2.SS2.p2.2.m1.1.1.2">
          <csymbol cd="ambiguous" id="S2.SS2.p2.2.m1.1.1.2.1.cmml" xref="S2.SS2.p2.2.m1.1.1.2">
           subscript
          </csymbol>
          <ci id="S2.SS2.p2.2.m1.1.1.2.2.cmml" xref="S2.SS2.p2.2.m1.1.1.2.2">
           ğ‘š
          </ci>
          <ci id="S2.SS2.p2.2.m1.1.1.2.3.cmml" xref="S2.SS2.p2.2.m1.1.1.2.3">
           ğ‘–
          </ci>
         </apply>
         <ci id="S2.SS2.p2.2.m1.1.1.3.cmml" xref="S2.SS2.p2.2.m1.1.1.3">
          â„³
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.2.m1.1c">
        m_{i}\in{\mathcal{M}}
       </annotation>
      </semantics>
     </math>
     , shares and refines its decision
     <math alttext="a_{m_{i}}" class="ltx_Math" display="inline" id="S2.SS2.p2.3.m2.1">
      <semantics id="S2.SS2.p2.3.m2.1a">
       <msub id="S2.SS2.p2.3.m2.1.1" xref="S2.SS2.p2.3.m2.1.1.cmml">
        <mi id="S2.SS2.p2.3.m2.1.1.2" xref="S2.SS2.p2.3.m2.1.1.2.cmml">
         a
        </mi>
        <msub id="S2.SS2.p2.3.m2.1.1.3" xref="S2.SS2.p2.3.m2.1.1.3.cmml">
         <mi id="S2.SS2.p2.3.m2.1.1.3.2" xref="S2.SS2.p2.3.m2.1.1.3.2.cmml">
          m
         </mi>
         <mi id="S2.SS2.p2.3.m2.1.1.3.3" xref="S2.SS2.p2.3.m2.1.1.3.3.cmml">
          i
         </mi>
        </msub>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m2.1b">
        <apply id="S2.SS2.p2.3.m2.1.1.cmml" xref="S2.SS2.p2.3.m2.1.1">
         <csymbol cd="ambiguous" id="S2.SS2.p2.3.m2.1.1.1.cmml" xref="S2.SS2.p2.3.m2.1.1">
          subscript
         </csymbol>
         <ci id="S2.SS2.p2.3.m2.1.1.2.cmml" xref="S2.SS2.p2.3.m2.1.1.2">
          ğ‘
         </ci>
         <apply id="S2.SS2.p2.3.m2.1.1.3.cmml" xref="S2.SS2.p2.3.m2.1.1.3">
          <csymbol cd="ambiguous" id="S2.SS2.p2.3.m2.1.1.3.1.cmml" xref="S2.SS2.p2.3.m2.1.1.3">
           subscript
          </csymbol>
          <ci id="S2.SS2.p2.3.m2.1.1.3.2.cmml" xref="S2.SS2.p2.3.m2.1.1.3.2">
           ğ‘š
          </ci>
          <ci id="S2.SS2.p2.3.m2.1.1.3.3.cmml" xref="S2.SS2.p2.3.m2.1.1.3.3">
           ğ‘–
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.3.m2.1c">
        a_{m_{i}}
       </annotation>
      </semantics>
     </math>
     . The groupâ€™s collective decision,
     <math alttext="A=f\left(\{a_{m_{i}}\}_{i}\right)\in{\mathcal{A}}" class="ltx_Math" display="inline" id="S2.SS2.p2.4.m3.1">
      <semantics id="S2.SS2.p2.4.m3.1a">
       <mrow id="S2.SS2.p2.4.m3.1.1" xref="S2.SS2.p2.4.m3.1.1.cmml">
        <mi id="S2.SS2.p2.4.m3.1.1.3" xref="S2.SS2.p2.4.m3.1.1.3.cmml">
         A
        </mi>
        <mo id="S2.SS2.p2.4.m3.1.1.4" xref="S2.SS2.p2.4.m3.1.1.4.cmml">
         =
        </mo>
        <mrow id="S2.SS2.p2.4.m3.1.1.1" xref="S2.SS2.p2.4.m3.1.1.1.cmml">
         <mi id="S2.SS2.p2.4.m3.1.1.1.3" xref="S2.SS2.p2.4.m3.1.1.1.3.cmml">
          f
         </mi>
         <mo id="S2.SS2.p2.4.m3.1.1.1.2" lspace="0em" rspace="0em" xref="S2.SS2.p2.4.m3.1.1.1.2.cmml">
          â€‹
         </mo>
         <mrow id="S2.SS2.p2.4.m3.1.1.1.1.1" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.cmml">
          <mo id="S2.SS2.p2.4.m3.1.1.1.1.1.2" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.cmml">
           (
          </mo>
          <msub id="S2.SS2.p2.4.m3.1.1.1.1.1.1" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.cmml">
           <mrow id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.2.cmml">
            <mo id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.2.cmml">
             {
            </mo>
            <msub id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.cmml">
             <mi id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.2" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.2.cmml">
              a
             </mi>
             <msub id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.3" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.3.cmml">
              <mi id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.3.2" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.3.2.cmml">
               m
              </mi>
              <mi id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.3.3" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.3.3.cmml">
               i
              </mi>
             </msub>
            </msub>
            <mo id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.2.cmml">
             }
            </mo>
           </mrow>
           <mi id="S2.SS2.p2.4.m3.1.1.1.1.1.1.3" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.3.cmml">
            i
           </mi>
          </msub>
          <mo id="S2.SS2.p2.4.m3.1.1.1.1.1.3" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.cmml">
           )
          </mo>
         </mrow>
        </mrow>
        <mo id="S2.SS2.p2.4.m3.1.1.5" xref="S2.SS2.p2.4.m3.1.1.5.cmml">
         âˆˆ
        </mo>
        <mi class="ltx_font_mathcaligraphic" id="S2.SS2.p2.4.m3.1.1.6" xref="S2.SS2.p2.4.m3.1.1.6.cmml">
         ğ’œ
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m3.1b">
        <apply id="S2.SS2.p2.4.m3.1.1.cmml" xref="S2.SS2.p2.4.m3.1.1">
         <and id="S2.SS2.p2.4.m3.1.1a.cmml" xref="S2.SS2.p2.4.m3.1.1">
         </and>
         <apply id="S2.SS2.p2.4.m3.1.1b.cmml" xref="S2.SS2.p2.4.m3.1.1">
          <eq id="S2.SS2.p2.4.m3.1.1.4.cmml" xref="S2.SS2.p2.4.m3.1.1.4">
          </eq>
          <ci id="S2.SS2.p2.4.m3.1.1.3.cmml" xref="S2.SS2.p2.4.m3.1.1.3">
           ğ´
          </ci>
          <apply id="S2.SS2.p2.4.m3.1.1.1.cmml" xref="S2.SS2.p2.4.m3.1.1.1">
           <times id="S2.SS2.p2.4.m3.1.1.1.2.cmml" xref="S2.SS2.p2.4.m3.1.1.1.2">
           </times>
           <ci id="S2.SS2.p2.4.m3.1.1.1.3.cmml" xref="S2.SS2.p2.4.m3.1.1.1.3">
            ğ‘“
           </ci>
           <apply id="S2.SS2.p2.4.m3.1.1.1.1.1.1.cmml" xref="S2.SS2.p2.4.m3.1.1.1.1.1">
            <csymbol cd="ambiguous" id="S2.SS2.p2.4.m3.1.1.1.1.1.1.2.cmml" xref="S2.SS2.p2.4.m3.1.1.1.1.1">
             subscript
            </csymbol>
            <set id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.2.cmml" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1">
             <apply id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1">
              <csymbol cd="ambiguous" id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1">
               subscript
              </csymbol>
              <ci id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.2">
               ğ‘
              </ci>
              <apply id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.3">
               <csymbol cd="ambiguous" id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.3">
                subscript
               </csymbol>
               <ci id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.3.2">
                ğ‘š
               </ci>
               <ci id="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.1.1.1.3.3">
                ğ‘–
               </ci>
              </apply>
             </apply>
            </set>
            <ci id="S2.SS2.p2.4.m3.1.1.1.1.1.1.3.cmml" xref="S2.SS2.p2.4.m3.1.1.1.1.1.1.3">
             ğ‘–
            </ci>
           </apply>
          </apply>
         </apply>
         <apply id="S2.SS2.p2.4.m3.1.1c.cmml" xref="S2.SS2.p2.4.m3.1.1">
          <in id="S2.SS2.p2.4.m3.1.1.5.cmml" xref="S2.SS2.p2.4.m3.1.1.5">
          </in>
          <share href="#S2.SS2.p2.4.m3.1.1.1.cmml" id="S2.SS2.p2.4.m3.1.1d.cmml" xref="S2.SS2.p2.4.m3.1.1">
          </share>
          <ci id="S2.SS2.p2.4.m3.1.1.6.cmml" xref="S2.SS2.p2.4.m3.1.1.6">
           ğ’œ
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.4.m3.1c">
        A=f\left(\{a_{m_{i}}\}_{i}\right)\in{\mathcal{A}}
       </annotation>
      </semantics>
     </math>
     , emerges as an integration of individual agentsâ€™ decisions using a function
     <math alttext="f" class="ltx_Math" display="inline" id="S2.SS2.p2.5.m4.1">
      <semantics id="S2.SS2.p2.5.m4.1a">
       <mi id="S2.SS2.p2.5.m4.1.1" xref="S2.SS2.p2.5.m4.1.1.cmml">
        f
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p2.5.m4.1b">
        <ci id="S2.SS2.p2.5.m4.1.1.cmml" xref="S2.SS2.p2.5.m4.1.1">
         ğ‘“
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p2.5.m4.1c">
        f
       </annotation>
      </semantics>
     </math>
     , which might involve techniques like summarization or ensemble. This structure is especially effective in scenarios like consulting and tool using.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS2.p3">
    <p class="ltx_p" id="S2.SS2.p3.6">
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.1">
      Vertical Structure (
      <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S2.SS2.p3.1.1.1" style="width:39.0pt;">
       <span class="ltx_p" id="S2.SS2.p3.1.1.1.1">
        <span class="ltx_text" id="S2.SS2.p3.1.1.1.1.1" style="position:relative; bottom:-7.2pt;">
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="240" id="S2.SS2.p3.1.1.1.1.1.g1" src="/html/2308.10848/assets/x3.png" width="461"/>
        </span>
       </span>
      </span>
      )
     </span>
     Conversely, vertical structure has a clear division of roles. An agent, termed the solver
     <math alttext="m^{*}" class="ltx_Math" display="inline" id="S2.SS2.p3.2.m1.1">
      <semantics id="S2.SS2.p3.2.m1.1a">
       <msup id="S2.SS2.p3.2.m1.1.1" xref="S2.SS2.p3.2.m1.1.1.cmml">
        <mi id="S2.SS2.p3.2.m1.1.1.2" xref="S2.SS2.p3.2.m1.1.1.2.cmml">
         m
        </mi>
        <mo id="S2.SS2.p3.2.m1.1.1.3" xref="S2.SS2.p3.2.m1.1.1.3.cmml">
         âˆ—
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m1.1b">
        <apply id="S2.SS2.p3.2.m1.1.1.cmml" xref="S2.SS2.p3.2.m1.1.1">
         <csymbol cd="ambiguous" id="S2.SS2.p3.2.m1.1.1.1.cmml" xref="S2.SS2.p3.2.m1.1.1">
          superscript
         </csymbol>
         <ci id="S2.SS2.p3.2.m1.1.1.2.cmml" xref="S2.SS2.p3.2.m1.1.1.2">
          ğ‘š
         </ci>
         <times id="S2.SS2.p3.2.m1.1.1.3.cmml" xref="S2.SS2.p3.2.m1.1.1.3">
         </times>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p3.2.m1.1c">
        m^{*}
       </annotation>
      </semantics>
     </math>
     , proposes an initial decision
     <math alttext="a_{0}^{*}" class="ltx_Math" display="inline" id="S2.SS2.p3.3.m2.1">
      <semantics id="S2.SS2.p3.3.m2.1a">
       <msubsup id="S2.SS2.p3.3.m2.1.1" xref="S2.SS2.p3.3.m2.1.1.cmml">
        <mi id="S2.SS2.p3.3.m2.1.1.2.2" xref="S2.SS2.p3.3.m2.1.1.2.2.cmml">
         a
        </mi>
        <mn id="S2.SS2.p3.3.m2.1.1.2.3" xref="S2.SS2.p3.3.m2.1.1.2.3.cmml">
         0
        </mn>
        <mo id="S2.SS2.p3.3.m2.1.1.3" xref="S2.SS2.p3.3.m2.1.1.3.cmml">
         âˆ—
        </mo>
       </msubsup>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m2.1b">
        <apply id="S2.SS2.p3.3.m2.1.1.cmml" xref="S2.SS2.p3.3.m2.1.1">
         <csymbol cd="ambiguous" id="S2.SS2.p3.3.m2.1.1.1.cmml" xref="S2.SS2.p3.3.m2.1.1">
          superscript
         </csymbol>
         <apply id="S2.SS2.p3.3.m2.1.1.2.cmml" xref="S2.SS2.p3.3.m2.1.1">
          <csymbol cd="ambiguous" id="S2.SS2.p3.3.m2.1.1.2.1.cmml" xref="S2.SS2.p3.3.m2.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS2.p3.3.m2.1.1.2.2.cmml" xref="S2.SS2.p3.3.m2.1.1.2.2">
           ğ‘
          </ci>
          <cn id="S2.SS2.p3.3.m2.1.1.2.3.cmml" type="integer" xref="S2.SS2.p3.3.m2.1.1.2.3">
           0
          </cn>
         </apply>
         <times id="S2.SS2.p3.3.m2.1.1.3.cmml" xref="S2.SS2.p3.3.m2.1.1.3">
         </times>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p3.3.m2.1c">
        a_{0}^{*}
       </annotation>
      </semantics>
     </math>
     . Other agents, as reviewers, provide feedback on this proposal, prompting iterative refinements by the solver until a consensus is reached among reviewers or a set number of iterations is exhausted. The final decision
     <math alttext="A" class="ltx_Math" display="inline" id="S2.SS2.p3.4.m3.1">
      <semantics id="S2.SS2.p3.4.m3.1a">
       <mi id="S2.SS2.p3.4.m3.1.1" xref="S2.SS2.p3.4.m3.1.1.cmml">
        A
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p3.4.m3.1b">
        <ci id="S2.SS2.p3.4.m3.1.1.cmml" xref="S2.SS2.p3.4.m3.1.1">
         ğ´
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p3.4.m3.1c">
        A
       </annotation>
      </semantics>
     </math>
     is given as
     <math alttext="A=a^{*}_{k}\in{\mathcal{A}}" class="ltx_Math" display="inline" id="S2.SS2.p3.5.m4.1">
      <semantics id="S2.SS2.p3.5.m4.1a">
       <mrow id="S2.SS2.p3.5.m4.1.1" xref="S2.SS2.p3.5.m4.1.1.cmml">
        <mi id="S2.SS2.p3.5.m4.1.1.2" xref="S2.SS2.p3.5.m4.1.1.2.cmml">
         A
        </mi>
        <mo id="S2.SS2.p3.5.m4.1.1.3" xref="S2.SS2.p3.5.m4.1.1.3.cmml">
         =
        </mo>
        <msubsup id="S2.SS2.p3.5.m4.1.1.4" xref="S2.SS2.p3.5.m4.1.1.4.cmml">
         <mi id="S2.SS2.p3.5.m4.1.1.4.2.2" xref="S2.SS2.p3.5.m4.1.1.4.2.2.cmml">
          a
         </mi>
         <mi id="S2.SS2.p3.5.m4.1.1.4.3" xref="S2.SS2.p3.5.m4.1.1.4.3.cmml">
          k
         </mi>
         <mo id="S2.SS2.p3.5.m4.1.1.4.2.3" xref="S2.SS2.p3.5.m4.1.1.4.2.3.cmml">
          âˆ—
         </mo>
        </msubsup>
        <mo id="S2.SS2.p3.5.m4.1.1.5" xref="S2.SS2.p3.5.m4.1.1.5.cmml">
         âˆˆ
        </mo>
        <mi class="ltx_font_mathcaligraphic" id="S2.SS2.p3.5.m4.1.1.6" xref="S2.SS2.p3.5.m4.1.1.6.cmml">
         ğ’œ
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p3.5.m4.1b">
        <apply id="S2.SS2.p3.5.m4.1.1.cmml" xref="S2.SS2.p3.5.m4.1.1">
         <and id="S2.SS2.p3.5.m4.1.1a.cmml" xref="S2.SS2.p3.5.m4.1.1">
         </and>
         <apply id="S2.SS2.p3.5.m4.1.1b.cmml" xref="S2.SS2.p3.5.m4.1.1">
          <eq id="S2.SS2.p3.5.m4.1.1.3.cmml" xref="S2.SS2.p3.5.m4.1.1.3">
          </eq>
          <ci id="S2.SS2.p3.5.m4.1.1.2.cmml" xref="S2.SS2.p3.5.m4.1.1.2">
           ğ´
          </ci>
          <apply id="S2.SS2.p3.5.m4.1.1.4.cmml" xref="S2.SS2.p3.5.m4.1.1.4">
           <csymbol cd="ambiguous" id="S2.SS2.p3.5.m4.1.1.4.1.cmml" xref="S2.SS2.p3.5.m4.1.1.4">
            subscript
           </csymbol>
           <apply id="S2.SS2.p3.5.m4.1.1.4.2.cmml" xref="S2.SS2.p3.5.m4.1.1.4">
            <csymbol cd="ambiguous" id="S2.SS2.p3.5.m4.1.1.4.2.1.cmml" xref="S2.SS2.p3.5.m4.1.1.4">
             superscript
            </csymbol>
            <ci id="S2.SS2.p3.5.m4.1.1.4.2.2.cmml" xref="S2.SS2.p3.5.m4.1.1.4.2.2">
             ğ‘
            </ci>
            <times id="S2.SS2.p3.5.m4.1.1.4.2.3.cmml" xref="S2.SS2.p3.5.m4.1.1.4.2.3">
            </times>
           </apply>
           <ci id="S2.SS2.p3.5.m4.1.1.4.3.cmml" xref="S2.SS2.p3.5.m4.1.1.4.3">
            ğ‘˜
           </ci>
          </apply>
         </apply>
         <apply id="S2.SS2.p3.5.m4.1.1c.cmml" xref="S2.SS2.p3.5.m4.1.1">
          <in id="S2.SS2.p3.5.m4.1.1.5.cmml" xref="S2.SS2.p3.5.m4.1.1.5">
          </in>
          <share href="#S2.SS2.p3.5.m4.1.1.4.cmml" id="S2.SS2.p3.5.m4.1.1d.cmml" xref="S2.SS2.p3.5.m4.1.1">
          </share>
          <ci id="S2.SS2.p3.5.m4.1.1.6.cmml" xref="S2.SS2.p3.5.m4.1.1.6">
           ğ’œ
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p3.5.m4.1c">
        A=a^{*}_{k}\in{\mathcal{A}}
       </annotation>
      </semantics>
     </math>
     , with
     <math alttext="k" class="ltx_Math" display="inline" id="S2.SS2.p3.6.m5.1">
      <semantics id="S2.SS2.p3.6.m5.1a">
       <mi id="S2.SS2.p3.6.m5.1.1" xref="S2.SS2.p3.6.m5.1.1.cmml">
        k
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS2.p3.6.m5.1b">
        <ci id="S2.SS2.p3.6.m5.1.1.cmml" xref="S2.SS2.p3.6.m5.1.1">
         ğ‘˜
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS2.p3.6.m5.1c">
        k
       </annotation>
      </semantics>
     </math>
     indicating the number of refinements.
Vertical structure is preferable for tasks like math problem-solving and software development, where only one refined decision is required.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.3
    </span>
    Action Execution
   </h3>
   <div class="ltx_para ltx_noindent" id="S2.SS3.p1">
    <p class="ltx_p" id="S2.SS3.p1.3">
     In the decision-making stage, agents collaboratively contribute to a group decision
     <math alttext="A" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1">
      <semantics id="S2.SS3.p1.1.m1.1a">
       <mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">
        A
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b">
        <ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">
         ğ´
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">
        A
       </annotation>
      </semantics>
     </math>
     containing actions that need to be executed in the current environment. Within the action execution stage, agents then execute the collectively-decided actions in the environment. Depending on the implementation, some agents might not perform any execution. As a result of these actions, the state of the environment transitions from
     <math alttext="s_{\text{old}}" class="ltx_Math" display="inline" id="S2.SS3.p1.2.m2.1">
      <semantics id="S2.SS3.p1.2.m2.1a">
       <msub id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">
        <mi id="S2.SS3.p1.2.m2.1.1.2" xref="S2.SS3.p1.2.m2.1.1.2.cmml">
         s
        </mi>
        <mtext id="S2.SS3.p1.2.m2.1.1.3" xref="S2.SS3.p1.2.m2.1.1.3a.cmml">
         old
        </mtext>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b">
        <apply id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">
         <csymbol cd="ambiguous" id="S2.SS3.p1.2.m2.1.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">
          subscript
         </csymbol>
         <ci id="S2.SS3.p1.2.m2.1.1.2.cmml" xref="S2.SS3.p1.2.m2.1.1.2">
          ğ‘ 
         </ci>
         <ci id="S2.SS3.p1.2.m2.1.1.3a.cmml" xref="S2.SS3.p1.2.m2.1.1.3">
          <mtext id="S2.SS3.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S2.SS3.p1.2.m2.1.1.3">
           old
          </mtext>
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">
        s_{\text{old}}
       </annotation>
      </semantics>
     </math>
     to
     <math alttext="s_{\text{new}}={\mathcal{T}}(s_{\text{old}},A)" class="ltx_Math" display="inline" id="S2.SS3.p1.3.m3.2">
      <semantics id="S2.SS3.p1.3.m3.2a">
       <mrow id="S2.SS3.p1.3.m3.2.2" xref="S2.SS3.p1.3.m3.2.2.cmml">
        <msub id="S2.SS3.p1.3.m3.2.2.3" xref="S2.SS3.p1.3.m3.2.2.3.cmml">
         <mi id="S2.SS3.p1.3.m3.2.2.3.2" xref="S2.SS3.p1.3.m3.2.2.3.2.cmml">
          s
         </mi>
         <mtext id="S2.SS3.p1.3.m3.2.2.3.3" xref="S2.SS3.p1.3.m3.2.2.3.3a.cmml">
          new
         </mtext>
        </msub>
        <mo id="S2.SS3.p1.3.m3.2.2.2" xref="S2.SS3.p1.3.m3.2.2.2.cmml">
         =
        </mo>
        <mrow id="S2.SS3.p1.3.m3.2.2.1" xref="S2.SS3.p1.3.m3.2.2.1.cmml">
         <mi class="ltx_font_mathcaligraphic" id="S2.SS3.p1.3.m3.2.2.1.3" xref="S2.SS3.p1.3.m3.2.2.1.3.cmml">
          ğ’¯
         </mi>
         <mo id="S2.SS3.p1.3.m3.2.2.1.2" lspace="0em" rspace="0em" xref="S2.SS3.p1.3.m3.2.2.1.2.cmml">
          â€‹
         </mo>
         <mrow id="S2.SS3.p1.3.m3.2.2.1.1.1" xref="S2.SS3.p1.3.m3.2.2.1.1.2.cmml">
          <mo id="S2.SS3.p1.3.m3.2.2.1.1.1.2" stretchy="false" xref="S2.SS3.p1.3.m3.2.2.1.1.2.cmml">
           (
          </mo>
          <msub id="S2.SS3.p1.3.m3.2.2.1.1.1.1" xref="S2.SS3.p1.3.m3.2.2.1.1.1.1.cmml">
           <mi id="S2.SS3.p1.3.m3.2.2.1.1.1.1.2" xref="S2.SS3.p1.3.m3.2.2.1.1.1.1.2.cmml">
            s
           </mi>
           <mtext id="S2.SS3.p1.3.m3.2.2.1.1.1.1.3" xref="S2.SS3.p1.3.m3.2.2.1.1.1.1.3a.cmml">
            old
           </mtext>
          </msub>
          <mo id="S2.SS3.p1.3.m3.2.2.1.1.1.3" xref="S2.SS3.p1.3.m3.2.2.1.1.2.cmml">
           ,
          </mo>
          <mi id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml">
           A
          </mi>
          <mo id="S2.SS3.p1.3.m3.2.2.1.1.1.4" stretchy="false" xref="S2.SS3.p1.3.m3.2.2.1.1.2.cmml">
           )
          </mo>
         </mrow>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.2b">
        <apply id="S2.SS3.p1.3.m3.2.2.cmml" xref="S2.SS3.p1.3.m3.2.2">
         <eq id="S2.SS3.p1.3.m3.2.2.2.cmml" xref="S2.SS3.p1.3.m3.2.2.2">
         </eq>
         <apply id="S2.SS3.p1.3.m3.2.2.3.cmml" xref="S2.SS3.p1.3.m3.2.2.3">
          <csymbol cd="ambiguous" id="S2.SS3.p1.3.m3.2.2.3.1.cmml" xref="S2.SS3.p1.3.m3.2.2.3">
           subscript
          </csymbol>
          <ci id="S2.SS3.p1.3.m3.2.2.3.2.cmml" xref="S2.SS3.p1.3.m3.2.2.3.2">
           ğ‘ 
          </ci>
          <ci id="S2.SS3.p1.3.m3.2.2.3.3a.cmml" xref="S2.SS3.p1.3.m3.2.2.3.3">
           <mtext id="S2.SS3.p1.3.m3.2.2.3.3.cmml" mathsize="70%" xref="S2.SS3.p1.3.m3.2.2.3.3">
            new
           </mtext>
          </ci>
         </apply>
         <apply id="S2.SS3.p1.3.m3.2.2.1.cmml" xref="S2.SS3.p1.3.m3.2.2.1">
          <times id="S2.SS3.p1.3.m3.2.2.1.2.cmml" xref="S2.SS3.p1.3.m3.2.2.1.2">
          </times>
          <ci id="S2.SS3.p1.3.m3.2.2.1.3.cmml" xref="S2.SS3.p1.3.m3.2.2.1.3">
           ğ’¯
          </ci>
          <interval closure="open" id="S2.SS3.p1.3.m3.2.2.1.1.2.cmml" xref="S2.SS3.p1.3.m3.2.2.1.1.1">
           <apply id="S2.SS3.p1.3.m3.2.2.1.1.1.1.cmml" xref="S2.SS3.p1.3.m3.2.2.1.1.1.1">
            <csymbol cd="ambiguous" id="S2.SS3.p1.3.m3.2.2.1.1.1.1.1.cmml" xref="S2.SS3.p1.3.m3.2.2.1.1.1.1">
             subscript
            </csymbol>
            <ci id="S2.SS3.p1.3.m3.2.2.1.1.1.1.2.cmml" xref="S2.SS3.p1.3.m3.2.2.1.1.1.1.2">
             ğ‘ 
            </ci>
            <ci id="S2.SS3.p1.3.m3.2.2.1.1.1.1.3a.cmml" xref="S2.SS3.p1.3.m3.2.2.1.1.1.1.3">
             <mtext id="S2.SS3.p1.3.m3.2.2.1.1.1.1.3.cmml" mathsize="70%" xref="S2.SS3.p1.3.m3.2.2.1.1.1.1.3">
              old
             </mtext>
            </ci>
           </apply>
           <ci id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">
            ğ´
           </ci>
          </interval>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.2c">
        s_{\text{new}}={\mathcal{T}}(s_{\text{old}},A)
       </annotation>
      </semantics>
     </math>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.4
    </span>
    Evaluation
   </h3>
   <div class="ltx_para ltx_noindent" id="S2.SS4.p1">
    <p class="ltx_p" id="S2.SS4.p1.5">
     The evaluation stage is vital for
     <span class="ltx_text ltx_font_smallcaps" id="S2.SS4.p1.5.1">
      AgentVerse
     </span>
     , guiding improvements for subsequent rounds. At this stage, the feedback mechanism
     <math alttext="{\mathcal{R}}" class="ltx_Math" display="inline" id="S2.SS4.p1.1.m1.1">
      <semantics id="S2.SS4.p1.1.m1.1a">
       <mi class="ltx_font_mathcaligraphic" id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml">
        â„›
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b">
        <ci id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1">
         â„›
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">
        {\mathcal{R}}
       </annotation>
      </semantics>
     </math>
     assesses the difference between the current state
     <math alttext="s_{\text{new}}" class="ltx_Math" display="inline" id="S2.SS4.p1.2.m2.1">
      <semantics id="S2.SS4.p1.2.m2.1a">
       <msub id="S2.SS4.p1.2.m2.1.1" xref="S2.SS4.p1.2.m2.1.1.cmml">
        <mi id="S2.SS4.p1.2.m2.1.1.2" xref="S2.SS4.p1.2.m2.1.1.2.cmml">
         s
        </mi>
        <mtext id="S2.SS4.p1.2.m2.1.1.3" xref="S2.SS4.p1.2.m2.1.1.3a.cmml">
         new
        </mtext>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S2.SS4.p1.2.m2.1b">
        <apply id="S2.SS4.p1.2.m2.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1">
         <csymbol cd="ambiguous" id="S2.SS4.p1.2.m2.1.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1">
          subscript
         </csymbol>
         <ci id="S2.SS4.p1.2.m2.1.1.2.cmml" xref="S2.SS4.p1.2.m2.1.1.2">
          ğ‘ 
         </ci>
         <ci id="S2.SS4.p1.2.m2.1.1.3a.cmml" xref="S2.SS4.p1.2.m2.1.1.3">
          <mtext id="S2.SS4.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S2.SS4.p1.2.m2.1.1.3">
           new
          </mtext>
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS4.p1.2.m2.1c">
        s_{\text{new}}
       </annotation>
      </semantics>
     </math>
     and the desired goal
     <math alttext="g\in G" class="ltx_Math" display="inline" id="S2.SS4.p1.3.m3.1">
      <semantics id="S2.SS4.p1.3.m3.1a">
       <mrow id="S2.SS4.p1.3.m3.1.1" xref="S2.SS4.p1.3.m3.1.1.cmml">
        <mi id="S2.SS4.p1.3.m3.1.1.2" xref="S2.SS4.p1.3.m3.1.1.2.cmml">
         g
        </mi>
        <mo id="S2.SS4.p1.3.m3.1.1.1" xref="S2.SS4.p1.3.m3.1.1.1.cmml">
         âˆˆ
        </mo>
        <mi id="S2.SS4.p1.3.m3.1.1.3" xref="S2.SS4.p1.3.m3.1.1.3.cmml">
         G
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S2.SS4.p1.3.m3.1b">
        <apply id="S2.SS4.p1.3.m3.1.1.cmml" xref="S2.SS4.p1.3.m3.1.1">
         <in id="S2.SS4.p1.3.m3.1.1.1.cmml" xref="S2.SS4.p1.3.m3.1.1.1">
         </in>
         <ci id="S2.SS4.p1.3.m3.1.1.2.cmml" xref="S2.SS4.p1.3.m3.1.1.2">
          ğ‘”
         </ci>
         <ci id="S2.SS4.p1.3.m3.1.1.3.cmml" xref="S2.SS4.p1.3.m3.1.1.3">
          ğº
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS4.p1.3.m3.1c">
        g\in G
       </annotation>
      </semantics>
     </math>
     . It then offers verbal feedback
     <math alttext="r={\mathcal{R}}(s_{\text{new}},g)" class="ltx_Math" display="inline" id="S2.SS4.p1.4.m4.2">
      <semantics id="S2.SS4.p1.4.m4.2a">
       <mrow id="S2.SS4.p1.4.m4.2.2" xref="S2.SS4.p1.4.m4.2.2.cmml">
        <mi id="S2.SS4.p1.4.m4.2.2.3" xref="S2.SS4.p1.4.m4.2.2.3.cmml">
         r
        </mi>
        <mo id="S2.SS4.p1.4.m4.2.2.2" xref="S2.SS4.p1.4.m4.2.2.2.cmml">
         =
        </mo>
        <mrow id="S2.SS4.p1.4.m4.2.2.1" xref="S2.SS4.p1.4.m4.2.2.1.cmml">
         <mi class="ltx_font_mathcaligraphic" id="S2.SS4.p1.4.m4.2.2.1.3" xref="S2.SS4.p1.4.m4.2.2.1.3.cmml">
          â„›
         </mi>
         <mo id="S2.SS4.p1.4.m4.2.2.1.2" lspace="0em" rspace="0em" xref="S2.SS4.p1.4.m4.2.2.1.2.cmml">
          â€‹
         </mo>
         <mrow id="S2.SS4.p1.4.m4.2.2.1.1.1" xref="S2.SS4.p1.4.m4.2.2.1.1.2.cmml">
          <mo id="S2.SS4.p1.4.m4.2.2.1.1.1.2" stretchy="false" xref="S2.SS4.p1.4.m4.2.2.1.1.2.cmml">
           (
          </mo>
          <msub id="S2.SS4.p1.4.m4.2.2.1.1.1.1" xref="S2.SS4.p1.4.m4.2.2.1.1.1.1.cmml">
           <mi id="S2.SS4.p1.4.m4.2.2.1.1.1.1.2" xref="S2.SS4.p1.4.m4.2.2.1.1.1.1.2.cmml">
            s
           </mi>
           <mtext id="S2.SS4.p1.4.m4.2.2.1.1.1.1.3" xref="S2.SS4.p1.4.m4.2.2.1.1.1.1.3a.cmml">
            new
           </mtext>
          </msub>
          <mo id="S2.SS4.p1.4.m4.2.2.1.1.1.3" xref="S2.SS4.p1.4.m4.2.2.1.1.2.cmml">
           ,
          </mo>
          <mi id="S2.SS4.p1.4.m4.1.1" xref="S2.SS4.p1.4.m4.1.1.cmml">
           g
          </mi>
          <mo id="S2.SS4.p1.4.m4.2.2.1.1.1.4" stretchy="false" xref="S2.SS4.p1.4.m4.2.2.1.1.2.cmml">
           )
          </mo>
         </mrow>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S2.SS4.p1.4.m4.2b">
        <apply id="S2.SS4.p1.4.m4.2.2.cmml" xref="S2.SS4.p1.4.m4.2.2">
         <eq id="S2.SS4.p1.4.m4.2.2.2.cmml" xref="S2.SS4.p1.4.m4.2.2.2">
         </eq>
         <ci id="S2.SS4.p1.4.m4.2.2.3.cmml" xref="S2.SS4.p1.4.m4.2.2.3">
          ğ‘Ÿ
         </ci>
         <apply id="S2.SS4.p1.4.m4.2.2.1.cmml" xref="S2.SS4.p1.4.m4.2.2.1">
          <times id="S2.SS4.p1.4.m4.2.2.1.2.cmml" xref="S2.SS4.p1.4.m4.2.2.1.2">
          </times>
          <ci id="S2.SS4.p1.4.m4.2.2.1.3.cmml" xref="S2.SS4.p1.4.m4.2.2.1.3">
           â„›
          </ci>
          <interval closure="open" id="S2.SS4.p1.4.m4.2.2.1.1.2.cmml" xref="S2.SS4.p1.4.m4.2.2.1.1.1">
           <apply id="S2.SS4.p1.4.m4.2.2.1.1.1.1.cmml" xref="S2.SS4.p1.4.m4.2.2.1.1.1.1">
            <csymbol cd="ambiguous" id="S2.SS4.p1.4.m4.2.2.1.1.1.1.1.cmml" xref="S2.SS4.p1.4.m4.2.2.1.1.1.1">
             subscript
            </csymbol>
            <ci id="S2.SS4.p1.4.m4.2.2.1.1.1.1.2.cmml" xref="S2.SS4.p1.4.m4.2.2.1.1.1.1.2">
             ğ‘ 
            </ci>
            <ci id="S2.SS4.p1.4.m4.2.2.1.1.1.1.3a.cmml" xref="S2.SS4.p1.4.m4.2.2.1.1.1.1.3">
             <mtext id="S2.SS4.p1.4.m4.2.2.1.1.1.1.3.cmml" mathsize="70%" xref="S2.SS4.p1.4.m4.2.2.1.1.1.1.3">
              new
             </mtext>
            </ci>
           </apply>
           <ci id="S2.SS4.p1.4.m4.1.1.cmml" xref="S2.SS4.p1.4.m4.1.1">
            ğ‘”
           </ci>
          </interval>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS4.p1.4.m4.2c">
        r={\mathcal{R}}(s_{\text{new}},g)
       </annotation>
      </semantics>
     </math>
     , detailing areas of shortcoming and suggesting ways to enhance performance.
     <math alttext="{\mathcal{R}}" class="ltx_Math" display="inline" id="S2.SS4.p1.5.m5.1">
      <semantics id="S2.SS4.p1.5.m5.1a">
       <mi class="ltx_font_mathcaligraphic" id="S2.SS4.p1.5.m5.1.1" xref="S2.SS4.p1.5.m5.1.1.cmml">
        â„›
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS4.p1.5.m5.1b">
        <ci id="S2.SS4.p1.5.m5.1.1.cmml" xref="S2.SS4.p1.5.m5.1.1">
         â„›
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS4.p1.5.m5.1c">
        {\mathcal{R}}
       </annotation>
      </semantics>
     </math>
     can either be defined by humans (in a human-in-the-loop
     <cite class="ltx_cite ltx_citemacro_citep">
      (Amershi etÂ al.,
      <a class="ltx_ref" href="#bib.bib2" title="">
       2014
      </a>
      )
     </cite>
     setting) or an agent for automatic feedback, depending on the implementation.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS4.p2">
    <p class="ltx_p" id="S2.SS4.p2.4">
     If the goal
     <math alttext="g" class="ltx_Math" display="inline" id="S2.SS4.p2.1.m1.1">
      <semantics id="S2.SS4.p2.1.m1.1a">
       <mi id="S2.SS4.p2.1.m1.1.1" xref="S2.SS4.p2.1.m1.1.1.cmml">
        g
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS4.p2.1.m1.1b">
        <ci id="S2.SS4.p2.1.m1.1.1.cmml" xref="S2.SS4.p2.1.m1.1.1">
         ğ‘”
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS4.p2.1.m1.1c">
        g
       </annotation>
      </semantics>
     </math>
     remains unmet, the feedback
     <math alttext="r" class="ltx_Math" display="inline" id="S2.SS4.p2.2.m2.1">
      <semantics id="S2.SS4.p2.2.m2.1a">
       <mi id="S2.SS4.p2.2.m2.1.1" xref="S2.SS4.p2.2.m2.1.1.cmml">
        r
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS4.p2.2.m2.1b">
        <ci id="S2.SS4.p2.2.m2.1.1.cmml" xref="S2.SS4.p2.2.m2.1.1">
         ğ‘Ÿ
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS4.p2.2.m2.1c">
        r
       </annotation>
      </semantics>
     </math>
     returns to the initial expert recruitment stage. In the next round, the expert recruitment stage will consider both feedback
     <math alttext="r" class="ltx_Math" display="inline" id="S2.SS4.p2.3.m3.1">
      <semantics id="S2.SS4.p2.3.m3.1a">
       <mi id="S2.SS4.p2.3.m3.1.1" xref="S2.SS4.p2.3.m3.1.1.cmml">
        r
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS4.p2.3.m3.1b">
        <ci id="S2.SS4.p2.3.m3.1.1.cmml" xref="S2.SS4.p2.3.m3.1.1">
         ğ‘Ÿ
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS4.p2.3.m3.1c">
        r
       </annotation>
      </semantics>
     </math>
     and the goal
     <math alttext="g" class="ltx_Math" display="inline" id="S2.SS4.p2.4.m4.1">
      <semantics id="S2.SS4.p2.4.m4.1a">
       <mi id="S2.SS4.p2.4.m4.1.1" xref="S2.SS4.p2.4.m4.1.1.cmml">
        g
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS4.p2.4.m4.1b">
        <ci id="S2.SS4.p2.4.m4.1.1.cmml" xref="S2.SS4.p2.4.m4.1.1">
         ğ‘”
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS4.p2.4.m4.1c">
        g
       </annotation>
      </semantics>
     </math>
     to adjust the groupâ€™s composition, aiming to evolve a more effective multi-agent group according to the current progress.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Experiments
  </h2>
  <div class="ltx_para ltx_noindent" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    To validate the superiority of
    <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">
     AgentVerse
    </span>
    in facilitating agent collaboration over standalone agents, we design four experimental tasks. Each task is designed to assess distinct aspects of an agent group: general understanding and reasoning capabilities, coding capabilities, tool utilization capabilities, and their potential in Embodied AI. Our findings, which are detailed in this section, consistently highlight the superior performance of
    <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.2">
     AgentVerse
    </span>
    across these varied and multi-faceted tasks. Of particular interest is the emergence of unique collaborative behaviors within agent groups. While this section focuses on the advantages of multi-agent setups, a deeper exploration of these emergent behaviors will be presented in
    <a class="ltx_ref" href="#S4" title="4 Emergent Behaviors within a Multi-agent Group â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      Section
     </span>
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    .
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S3.p2">
   <p class="ltx_p" id="S3.p2.1">
    <span class="ltx_text ltx_font_bold" id="S3.p2.1.1">
     Setups.
    </span>
    In all the experiments, we evaluate the performance of agents driven by GPT-3.5-Turbo-0613 and GPT-4-0613 across various tasks. All the experiments are done in
    <span class="ltx_text ltx_font_bold" id="S3.p2.1.2">
     zero-shot
    </span>
    setting. For all the quantitative experiments in this section, we compare three settings: (1)
    <span class="ltx_text ltx_font_bold" id="S3.p2.1.3">
     CoT
    </span>
    : The CoT(chain-of-thought) agent; (2)
    <span class="ltx_text ltx_font_bold" id="S3.p2.1.4">
     Solo
    </span>
    : Using
    <span class="ltx_text ltx_font_smallcaps" id="S3.p2.1.5">
     AgentVerse
    </span>
    with a single agent in the decision-making stage. Compared with CoT, Solo additionally incorporates the expert recruitment, action execution, and evaluation modules; (3)
    <span class="ltx_text ltx_font_bold" id="S3.p2.1.6">
     Group
    </span>
    : Implementing
    <span class="ltx_text ltx_font_smallcaps" id="S3.p2.1.7">
     AgentVerse
    </span>
    with multiple agents collaborating during the decision-making. More detailed experimental setups for each task can be found in
    <a class="ltx_ref" href="#A1" title="Appendix A Configurations of the Experiments â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      Appendix
     </span>
     <span class="ltx_text ltx_ref_tag">
      A
     </span>
    </a>
    .
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    General Understanding and Reasoning Capabilities
   </h3>
   <figure class="ltx_table" id="S3.T1">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 1:
     </span>
     The results on different tasks that evaluate the agentsâ€™ general capabilities.
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1">
     <tr class="ltx_tr" id="S3.T1.1.1">
      <td class="ltx_td ltx_border_tt" id="S3.T1.1.1.1">
      </td>
      <td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.1.1.2">
       <span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.1">
        GPT-3.5-Turbo
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.1.1.3">
       <span class="ltx_text ltx_font_bold" id="S3.T1.1.1.3.1">
        GPT-4
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T1.1.2">
      <td class="ltx_td ltx_align_left" id="S3.T1.1.2.1">
       <span class="ltx_text ltx_font_bold" id="S3.T1.1.2.1.1">
        Task
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2">
       CoT
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.3">
       Solo
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.4">
       Group
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.5">
       CoT
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.6">
       Solo
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.7">
       Group
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T1.1.3">
      <td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.3.1">
       Conversation (FED)
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.2">
       81.6
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3">
       81.1
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.4">
       <span class="ltx_text ltx_font_bold" id="S3.T1.1.3.4.1">
        85.1
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.5">
       95.4
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.6">
       95.8
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.7">
       <span class="ltx_text ltx_font_bold" id="S3.T1.1.3.7.1">
        96.8
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T1.1.4">
      <td class="ltx_td ltx_align_left" id="S3.T1.1.4.1">
       Creative Writing (Commongen-Challenge)
      </td>
      <td class="ltx_td ltx_align_center" id="S3.T1.1.4.2">
       76.6
      </td>
      <td class="ltx_td ltx_align_center" id="S3.T1.1.4.3">
       <span class="ltx_text ltx_font_bold" id="S3.T1.1.4.3.1">
        93.6
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S3.T1.1.4.4">
       92.3
      </td>
      <td class="ltx_td ltx_align_center" id="S3.T1.1.4.5">
       95.9
      </td>
      <td class="ltx_td ltx_align_center" id="S3.T1.1.4.6">
       99.0
      </td>
      <td class="ltx_td ltx_align_center" id="S3.T1.1.4.7">
       <span class="ltx_text ltx_font_bold" id="S3.T1.1.4.7.1">
        99.1
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T1.1.5">
      <td class="ltx_td ltx_align_left" id="S3.T1.1.5.1">
       Mathematical Reasoning (MGSM)
      </td>
      <td class="ltx_td ltx_align_center" id="S3.T1.1.5.2">
       80.4
      </td>
      <td class="ltx_td ltx_align_center" id="S3.T1.1.5.3">
       <span class="ltx_text ltx_font_bold" id="S3.T1.1.5.3.1">
        82.4
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S3.T1.1.5.4">
       80.8
      </td>
      <td class="ltx_td ltx_align_center" id="S3.T1.1.5.5">
       95.2
      </td>
      <td class="ltx_td ltx_align_center" id="S3.T1.1.5.6">
       <span class="ltx_text ltx_font_bold" id="S3.T1.1.5.6.1">
        96.0
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S3.T1.1.5.7">
       95.2
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T1.1.6">
      <td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.6.1">
       Logical Reasoning (Logic Grid Puzzles)
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.6.2">
       -
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.6.3">
       -
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.6.4">
       -
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.6.5">
       59.5
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.6.6">
       64.0
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.6.7">
       <span class="ltx_text ltx_font_bold" id="S3.T1.1.6.7.1">
        66.5
       </span>
      </td>
     </tr>
    </table>
   </figure>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     To assess the agentsâ€™ general understanding and reasoning capabilities, we use four datasets: FED
     <cite class="ltx_cite ltx_citemacro_citep">
      (Mehri &amp; EskÃ©nazi,
      <a class="ltx_ref" href="#bib.bib19" title="">
       2020
      </a>
      )
     </cite>
     , Commongen Challenge
     <cite class="ltx_cite ltx_citemacro_citep">
      (Madaan etÂ al.,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
      )
     </cite>
     , MGSM
     <cite class="ltx_cite ltx_citemacro_citep">
      (Shi etÂ al.,
      <a class="ltx_ref" href="#bib.bib37" title="">
       2023
      </a>
      )
     </cite>
     , and Logic Grid Puzzles
     <cite class="ltx_cite ltx_citemacro_citep">
      (Srivastava etÂ al.,
      <a class="ltx_ref" href="#bib.bib39" title="">
       2022
      </a>
      )
     </cite>
     . Detailed descriptions of these datasets and metrics can be found in
     <a class="ltx_ref" href="#A1" title="Appendix A Configurations of the Experiments â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Appendix
      </span>
      <span class="ltx_text ltx_ref_tag">
       A
      </span>
     </a>
     . The first two datasets are used to measure the agentsâ€™ text understanding and creative writing abilities, while the latter two focus on examining the agentsâ€™ reasoning abilities, including mathematical and logical reasoning.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">
      Experimental Results.
     </span>
     The results in
     <a class="ltx_ref" href="#S3.T1" title="In 3.1 General Understanding and Reasoning Capabilities â€£ 3 Experiments â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Table
      </span>
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     show that agents assembled by
     <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p2.1.2">
      AgentVerse
     </span>
     (Solo and Group setups) consistently outperform the standalone CoT agent, irrespective of the LLM used. In our preliminary evaluations, GPT-3.5-Turbo struggles with accurately handling the logic grid puzzles dataset; therefore, we omit the result of GPT-3.5-Turbo on logical reasoning.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p3">
    <p class="ltx_p" id="S3.SS1.p3.1">
     Interestingly, for GPT-3.5-Turbo, the Group setup underperforms the Solo setup in two of three tasks, indicating that the discussion in decision-making might adversely impact performance for agents based on GPT-3.5-Turbo in certain contexts. Delving deeper into this observation, one predominant factor surfaces: the susceptibility to erroneous feedback. A recurring pattern observed in the Group setup is that: sometimes Agent A, despite starting with a correct answer, would be easily swayed by Agent Bâ€™s incorrect feedback. Roughly 10% of errors in the MGSM dataset can be traced to this dynamic. Notably, this phenomenon is absent in GPT-4-based agents, highlighting the importance of agentsâ€™ resilience to conflicting information during collaborative discussions.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p4">
    <p class="ltx_p" id="S3.SS1.p4.1">
     Overall, the results show that
     <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p4.1.1">
      AgentVerse
     </span>
     effectively enhances the general understanding and reasoning capabilities of agents. Moreover, agents driven by advanced LLMs demonstrate better performance when engaged in collaborative decision-making. The nuanced challenges observed with GPT-3.5-Turbo indicate the need to improve LLMsâ€™ robustness on incorrect information so that the collaboration can amplify individual strengths without introducing new vulnerabilities.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F2">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="261" id="S3.F2.g1" src="/html/2308.10848/assets/x4.png" width="422"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 2:
     </span>
     The illustration of an example process of consulting. The task is to
     <span class="ltx_text ltx_font_italic" id="S3.F2.2.1">
      give some suggestions on building a compressed hydrogen storage station in Ohio
     </span>
     .
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p5">
    <p class="ltx_p" id="S3.SS1.p5.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">
      Case Study: Consulting.
     </span>
     In
     <a class="ltx_ref" href="#S3.T1" title="In 3.1 General Understanding and Reasoning Capabilities â€£ 3 Experiments â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Table
      </span>
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     , the Group setup does not show a clear advantage over the Solo setup for both LLMs. This is mainly because the evaluation metrics for each benchmark have a limited scope. In the following case, we highlight the benefits of the group formed by GPT-4 agents by focusing on a consulting scenario where the group acts as a consultancy, responding to inquiries as shown in
     <a class="ltx_ref" href="#S3.F2" title="In 3.1 General Understanding and Reasoning Capabilities â€£ 3 Experiments â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Figure
      </span>
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     . The goal is to offer suggestions for a hydrogen storage station in Ohio.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p6">
    <p class="ltx_p" id="S3.SS1.p6.1">
     At first glance, the Solo setup seems to cover a broader scope than the Group setup at round 0. However, the Group setup offers more depth thanks to the recruited experts. For instance, while the Solo setup might suggest something basic like â€Find an optimal locationâ€, the Group setup provides detailed advice, such as â€evaluating site soil properties to ensure storage tank stability.â€ By the second round, different experts offer new insights in the Group setup. As a result, the Group setup not only covers a broader range (highlighted in red in the referenced figure) but also gives more detailed advice. For a detailed look at agent interactions, see
     <a class="ltx_ref" href="#A6" title="Appendix F Examples of the Case Studies â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Appendix
      </span>
      <span class="ltx_text ltx_ref_tag">
       F
      </span>
     </a>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Coding Capabilities
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     In this section, we first assess the agentsâ€™ coding capabilities using the Humaneval code completion dataset. Next, through a case study, we illustrate how collaboration among multiple agents improves output quality, highlighting its superiority over software development by just one agent.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">
      Experimental Results.
     </span>
     In
     <a class="ltx_ref" href="#S3.T2" title="In 3.2 Coding Capabilities â€£ 3 Experiments â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Table
      </span>
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     , we see a clear performance improvement moving from CoT to Solo and then to Group setup. This trend is especially pronounced with GPT-4, which sees a performance boost from 83.5 to 89.0. These results highlight
     <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p2.1.2">
      AgentVerse
     </span>
     â€™s effectiveness in managing a skilled group of agents for coding. For GPT-3.5-Turbo, although we have observed a drop
    </p>
   </div>
   <figure class="ltx_table ltx_align_floatright" id="S3.T2">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 2:
     </span>
     The pass@1 on Humaneval.
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.1">
     <tr class="ltx_tr" id="S3.T2.1.1">
      <td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.1">
       <span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1">
        Setting
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.2">
       <span class="ltx_text ltx_font_bold" id="S3.T2.1.1.2.1">
        GPT-3.5-Turbo
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.3">
       <span class="ltx_text ltx_font_bold" id="S3.T2.1.1.3.1">
        GPT-4
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T2.1.2">
      <td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.2.1">
       CoT
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.2">
       73.8
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.3">
       83.5
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T2.1.3">
      <td class="ltx_td ltx_align_left" id="S3.T2.1.3.1">
       Solo
      </td>
      <td class="ltx_td ltx_align_center" id="S3.T2.1.3.2">
       74.4
      </td>
      <td class="ltx_td ltx_align_center" id="S3.T2.1.3.3">
       87.2
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T2.1.4">
      <td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.1.4.1">
       Group
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.4.2">
       <span class="ltx_text ltx_font_bold" id="S3.T2.1.4.2.1">
        75.6
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.4.3">
       <span class="ltx_text ltx_font_bold" id="S3.T2.1.4.3.1">
        89.0
       </span>
      </td>
     </tr>
    </table>
   </figure>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p3">
    <p class="ltx_p" id="S3.SS2.p3.1">
     in performance with Group setup in
     <a class="ltx_ref" href="#S3.SS1" title="3.1 General Understanding and Reasoning Capabilities â€£ 3 Experiments â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Section
      </span>
      <span class="ltx_text ltx_ref_tag">
       3.1
      </span>
     </a>
     due to incorrect agent feedback in math reasoning, the coding evaluations show benefits. We posit that this might be attributed to LLMsâ€™ extensive pre-training on codes, potentially rendering them more adept at coding than mathematical reasoning and, consequently, more resilient to erroneous information in coding.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p4">
    <p class="ltx_p" id="S3.SS2.p4.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">
      Case Study: Software Development.
     </span>
     Our examination of the code generated for Humaneval by the Group setup in
     <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p4.1.2">
      AgentVerse
     </span>
     offers benefits beyond mere correctness. The agent group refines solutions, yielding more efficient, robust, and secure algorithms that are not covered by simple pass@1 metric. To better elucidate these advantages, we present a case study with GPT-4 on software development, a domain requiring multifaceted collaboration and refinement.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F3">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="164" id="S3.F3.g1" src="/html/2308.10848/assets/x5.png" width="422"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 3:
     </span>
     The illustration of an example process of developing a calculator with GUI in Python.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p5">
    <p class="ltx_p" id="S3.SS2.p5.1">
     We present an example where
     <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p5.1.1">
      AgentVerse
     </span>
     creates a Python-based calculator GUI by bringing together diverse expert agents. A concise development process overview is visualized in
     <a class="ltx_ref" href="#S3.F3" title="In 3.2 Coding Capabilities â€£ 3 Experiments â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Figure
      </span>
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     .
Comparing the applications from the Group and Solo setups reveals notable distinctions. Both achieve core functionality, but the Group-created calculator boasts a user-friendly interface with features like color distinctions and keyboard input. This improved design resulted from the diverse feedback of the multi-agent group. Suggestions from UI designer and evaluators enhance the user experience, while software tester enhances code robustness. A deeper examination of the code confirms that the multi-agent groupâ€™s output excels in exception handling compared to that of a solo agent. The codes generated by the two setups and the complete progress can be seen at
     <a class="ltx_ref" href="#A6" title="Appendix F Examples of the Case Studies â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Appendix
      </span>
      <span class="ltx_text ltx_ref_tag">
       F
      </span>
     </a>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Tool Utilization Capabilities
   </h3>
   <figure class="ltx_figure" id="S3.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="221" id="S3.F4.g1" src="/html/2308.10848/assets/x6.png" width="438"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4:
     </span>
     An example process of multi-agent solving user query with three different tools.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     The capability of LLMs to use real-world tools has been emphasized in many recent studies
     <cite class="ltx_cite ltx_citemacro_citep">
      (Schick etÂ al.,
      <a class="ltx_ref" href="#bib.bib35" title="">
       2023a
      </a>
      ; Qin etÂ al.,
      <a class="ltx_ref" href="#bib.bib30" title="">
       2023a
      </a>
      )
     </cite>
     . By equipping the LLMs with different tools such as a calculator, a web browser, and a code interpreter, the capabilities of LLMs can be significantly improved. In this section, we demonstrate that
     <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.1.1">
      AgentVerse
     </span>
     enables a group of agents to address intricate and multi-faceted tasks that require interaction with multiple tools, thereby enhancing work efficiency.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS3.p2">
    <p class="ltx_p" id="S3.SS3.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">
      Experimental Results.
     </span>
     We design a set of 10 intricate tasks, each requiring the use of at least two distinct tools to accomplish. By providing agents access to several tools, including Bing search API, a web browser, a code interpreter, and task-related APIs, we explore how
     <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p2.1.2">
      AgentVerse
     </span>
     facilitates agent collaboration, dissects the overarching task into manageable sub-tasks, and effectively deploys the available tools to address realistic user queries.
Of the
     <span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.3">
      10
     </span>
     challenging tasks provided, an agent group orchestrated by
     <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p2.1.4">
      AgentVerse
     </span>
     adeptly accomplishes
     <span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.5">
      9
     </span>
     tasks. On the other hand, a standalone ReAct agent
     <cite class="ltx_cite ltx_citemacro_citep">
      (Yao etÂ al.,
      <a class="ltx_ref" href="#bib.bib54" title="">
       2023b
      </a>
      )
     </cite>
     , which is a prevalent agent designed for tool using, can only fulfill
     <span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.6">
      3
     </span>
     tasks. In 6 out of 7 tasks where the single ReAct agent fails, the agent does not adhere to one or more criteria detailed in the task, and exit earlier than expected. We refer interested readers to
     <a class="ltx_ref" href="#A2" title="Appendix B Experiment Details for Multi-Agent Tool Using â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Appendix
      </span>
      <span class="ltx_text ltx_ref_tag">
       B
      </span>
     </a>
     for a comprehensive comparison of the solutions given by
     <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p2.1.7">
      AgentVerse
     </span>
     and a single ReAct agent.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS3.p3">
    <p class="ltx_p" id="S3.SS3.p3.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">
      Case Study: Solving 24-Point Game and Providing Similar Games.
     </span>
     Here, we present an example in
     <a class="ltx_ref" href="#S3.F4" title="In 3.3 Tool Utilization Capabilities â€£ 3 Experiments â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Figure
      </span>
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     , illustrating how
     <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p3.1.2">
      AgentVerse
     </span>
     searches for the rules of 24-point game, implements the code along with test cases, and explores similar games. The task is multifaceted; thus, during decision-making stage, the agents split the task into two sub-tasks in their discussion, and each assigned to a certain agent. While agent Charlie overlooks the sub-task of identifying games similar to the 24-point game in round 0, feedback from the evaluation module rectifies this in the subsequent iteration. Ultimately, the agent group provides not only the 24-point game rules and a solving code with test cases, but also a summary of a similar game. In contrast, a standalone ReAct agent merely provides the gameâ€™s definition along with a code and omits the query for similar games.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Emergent Behaviors within a Multi-agent Group
  </h2>
  <figure class="ltx_figure" id="S4.F5">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="94" id="S4.F5.g1" src="/html/2308.10848/assets/x7.png" width="422"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 5:
    </span>
    An illustration of the collaborative process involving three agents crafting a bookshelf. The process begins with the decision-making and breaking down the goal into several sub-tasks, with each agent receiving an assignment. The execution results and the current environmental state are then passed to the evaluator. This process repeats until the goal of crafting a bookshelf is achieved.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="S4.F6">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="376" id="S4.F6.g1" src="/html/2308.10848/assets/x8.png" width="422"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 6:
    </span>
    Examples of the properties emerge in the agent interactions in Minecraft.
   </figcaption>
  </figure>
  <div class="ltx_para ltx_noindent" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    In the preceding section, the efficacy of
    <span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.1">
     AgentVerse
    </span>
    has been illustrated across a spectrum of tasks that necessitate multi-agent decision-making, especially for GPT-4-based agents. Our endeavor, however, surpasses just improvements on benchmark datasets. We delve deeper into emergent collaborative behaviors exhibited by agents within realistic, embodied AI contexts. Minecraft, a sandbox game, serves as an ideal platform for such exploration due to its intricate parallelisms with real-world dynamics. In the game, agents must not just execute tasks but also plan, coordinate, and adjust to evolving situations. We task agents with collaboratively crafting a variety of items, spanning from paper and paintings to books and bookshelves. A succinct figure showcasing three agents adeptly crafting a bookshelf can be viewed in
    <a class="ltx_ref" href="#S4.F5" title="In 4 Emergent Behaviors within a Multi-agent Group â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      Figure
     </span>
     <span class="ltx_text ltx_ref_tag">
      5
     </span>
    </a>
    . An elaborate visualization is placed at
    <a class="ltx_ref" href="#A6" title="Appendix F Examples of the Case Studies â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      Appendix
     </span>
     <span class="ltx_text ltx_ref_tag">
      F
     </span>
    </a>
    , and details of the setups can be found in
    <a class="ltx_ref" href="#A3" title="Appendix C Details of the Experiments on Minecraft â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      Appendix
     </span>
     <span class="ltx_text ltx_ref_tag">
      C
     </span>
    </a>
    .
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S4.p2">
   <p class="ltx_p" id="S4.p2.1">
    By examining the decision-making process, we identify several emergent behaviors and categorize them into three aspects:
    <span class="ltx_text ltx_font_italic" id="S4.p2.1.1">
     volunteer
    </span>
    ,
    <span class="ltx_text ltx_font_italic" id="S4.p2.1.2">
     conformity
    </span>
    , and
    <span class="ltx_text ltx_font_italic" id="S4.p2.1.3">
     destructive
    </span>
    behaviors. Note that these behaviors not necessarily only appear in Minecraft but also in previous experiments such as tool utilization.
   </p>
  </div>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Volunteer Behaviors
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     Volunteer behaviors refer to actions intended to enhance the benefits of others in human society
     <cite class="ltx_cite ltx_citemacro_citep">
      (Omoto &amp; Snyder,
      <a class="ltx_ref" href="#bib.bib23" title="">
       1995
      </a>
      ; Mowen &amp; Sujan,
      <a class="ltx_ref" href="#bib.bib21" title="">
       2005
      </a>
      )
     </cite>
     . We observe similar behaviors emerging in a multi-agent group as follows:
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">
      Time Contribution.
     </span>
     The agents are willing to contribute their unallocated time to enhance collaboration efficiency. As shown in the examples in
     <a class="ltx_ref" href="#S4.F6" title="In 4 Emergent Behaviors within a Multi-agent Group â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Figure
      </span>
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     (1a), Alice and Bob need to collaboratively craft 2 paper, which necessitates three sugar canes as the raw material. Initially, Alice proposes that she will collect the sugar canes while Bob waits until the materials are ready. However, this plan is suboptimal, as it offers Bob spare time. Recognizing inefficiency, Bob suggests that both gather sugar canes concurrently, leading to expedited task completion.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p3">
    <p class="ltx_p" id="S4.SS1.p3.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">
      Resource Contribution.
     </span>
     Our analysis reveals that the agents are willing to contribute the possessed materials. As illustrated in
     <a class="ltx_ref" href="#S4.F6" title="In 4 Emergent Behaviors within a Multi-agent Group â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Figure
      </span>
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     (1b), at the end of the task crafting 2 paper, Alice has collected all the raw materials (sugar canes), whereas Bob possesses the crafting table essential for the paperâ€™s creation. In the decision-making stage, Alice suggests transferring her materials to Bob by dropping them on the ground. This enables Bob to utilize them for the intended crafting process.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p4">
    <p class="ltx_p" id="S4.SS1.p4.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">
      Assistance Contribution.
     </span>
     In the process of accomplishing tasks, we observe that agents, upon completing their individual assignments, actively extend support to their peers, thereby expediting the overall task resolution. As shown in
     <a class="ltx_ref" href="#S4.F6" title="In 4 Emergent Behaviors within a Multi-agent Group â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Figure
      </span>
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     (1c), Alice and Bob have successfully completed their assigned sub-tasks, while Charlie is still struggling to gather three leathers. During the collaborative decision-making phase, Alice and Bob propose to assist Charlie in gathering.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p5">
    <p class="ltx_p" id="S4.SS1.p5.1">
     These behaviors highlight how agents willingly contribute their capabilities and efforts to assist other agents, culminating in an accelerated achievement of their mutual goal.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Conformity Behavior
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     In human society, individuals tend to adjust their behavior to align with the norms or goals of a group
     <cite class="ltx_cite ltx_citemacro_citep">
      (Cialdini &amp; Goldstein,
      <a class="ltx_ref" href="#bib.bib8" title="">
       2004
      </a>
      ; Cialdini &amp; Trost,
      <a class="ltx_ref" href="#bib.bib9" title="">
       1998
      </a>
      )
     </cite>
     , which we refer to as
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">
      conformity behavior
     </span>
     . We also observe similar behaviors within multi-agent groups. As shown in
     <a class="ltx_ref" href="#S4.F6" title="In 4 Emergent Behaviors within a Multi-agent Group â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Figure
      </span>
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     (2), all agents are asked to gather three pieces of leather. However, Charlie gets sidetracked and begins crafting items that do not contribute directly to the task. In the subsequent decision-making stage, Alice and Bob critique Charlieâ€™s actions. Charlie acknowledges his mistake and re-focuses on the mutual tasks. The conformity behavior enables agents to align with mutual goals as work progresses.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    Destructive behavior
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     Additionally, we have also observed that agents may exhibit behaviors aimed at achieving greater efficiency, which could raise safety concerns. As depicted in
     <a class="ltx_ref" href="#S4.F6" title="In 4 Emergent Behaviors within a Multi-agent Group â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Figure
      </span>
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     (3a) and
     <a class="ltx_ref" href="#S4.F6" title="In 4 Emergent Behaviors within a Multi-agent Group â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Figure
      </span>
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     (3b), an agent occasionally bypasses the procedure of gathering raw materials and resorts to harming other agents or destroying an entire village library to acquire the necessary materials.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS3.p2">
    <p class="ltx_p" id="S4.SS3.p2.1">
     With advancements in autonomous agents, deploying them in real-world scenarios has become increasingly plausible. However, the emergence of hazardous behaviors could pose risks, especially when humans are involved in collaborative processes. Thus, designing strategies to prevent agents from adopting such hazardous behaviors is a critical area for future research.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Related Work
  </h2>
  <div class="ltx_para ltx_noindent" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    <span class="ltx_text ltx_font_bold" id="S5.p1.1.1">
     Autonomous Agents.
    </span>
    The pursuit of creating autonomous agents that can operate intelligently in real-world environments without human involvement has been a persistent goal throughout the history of AI
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wooldridge &amp; Jennings,
     <a class="ltx_ref" href="#bib.bib49" title="">
      1995
     </a>
     ; Minsky,
     <a class="ltx_ref" href="#bib.bib20" title="">
      1988
     </a>
     ; Bubeck etÂ al.,
     <a class="ltx_ref" href="#bib.bib5" title="">
      2023
     </a>
     )
    </cite>
    . Recently LLMs
    <cite class="ltx_cite ltx_citemacro_citep">
     (Touvron etÂ al.,
     <a class="ltx_ref" href="#bib.bib42" title="">
      2023a
     </a>
     ; OpenAI,
     <a class="ltx_ref" href="#bib.bib24" title="">
      2023a
     </a>
     )
    </cite>
    have opened up new opportunities to achieve this goal. These LLMs possess remarkable understanding, reasoning, and generation capabilities, allowing autonomous agents to utilize them as a backbone for handling increasingly complex scenarios
    <cite class="ltx_cite ltx_citemacro_citep">
     (Richards &amp; etÂ al.,
     <a class="ltx_ref" href="#bib.bib33" title="">
      2023
     </a>
     ; Nakajima,
     <a class="ltx_ref" href="#bib.bib22" title="">
      2023
     </a>
     ; Reworkd,
     <a class="ltx_ref" href="#bib.bib32" title="">
      2023
     </a>
     ; Liu etÂ al.,
     <a class="ltx_ref" href="#bib.bib17" title="">
      2023
     </a>
     )
    </cite>
    . However, even though these autonomous agents already demonstrate considerable power, they still lack certain essential human-analogous cognitive capabilities. Hence, some research designs external mechanisms that endow agents with reflection
    <cite class="ltx_cite ltx_citemacro_citep">
     (Yao etÂ al.,
     <a class="ltx_ref" href="#bib.bib54" title="">
      2023b
     </a>
     ; Shinn etÂ al.,
     <a class="ltx_ref" href="#bib.bib38" title="">
      2023
     </a>
     )
    </cite>
    , task decomposition
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wei etÂ al.,
     <a class="ltx_ref" href="#bib.bib47" title="">
      2022b
     </a>
     ; Yao etÂ al.,
     <a class="ltx_ref" href="#bib.bib53" title="">
      2023a
     </a>
     )
    </cite>
    , and tool utilization/creation
    <cite class="ltx_cite ltx_citemacro_citep">
     (Schick etÂ al.,
     <a class="ltx_ref" href="#bib.bib36" title="">
      2023b
     </a>
     ; Qin etÂ al.,
     <a class="ltx_ref" href="#bib.bib30" title="">
      2023a
     </a>
     ;
     <a class="ltx_ref" href="#bib.bib31" title="">
      b
     </a>
     ; Qian etÂ al.,
     <a class="ltx_ref" href="#bib.bib29" title="">
      2023b
     </a>
     )
    </cite>
    capabilities, which bring autonomous agents closer to achieving artificial general intelligence.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S5.p2">
   <p class="ltx_p" id="S5.p2.1">
    <span class="ltx_text ltx_font_bold" id="S5.p2.1.1">
     Multi-agent System.
    </span>
    In human society, a well-organized group composed of individual humans can often collaboratively handle a greater workload and accomplish complex tasks with higher efficiency and effectiveness. In the field of AI, researchers draw inspiration from human society and aim to enhance work efficiency and effectiveness by leveraging cooperation among individuals through the study of multi-agent systems (MAS)
    <cite class="ltx_cite ltx_citemacro_citep">
     (Stone &amp; Veloso,
     <a class="ltx_ref" href="#bib.bib41" title="">
      2000
     </a>
     )
    </cite>
    , also referred to as a
    <span class="ltx_text ltx_font_italic" id="S5.p2.1.2">
     multi-agent group
    </span>
    in this paper. The multi-agent group collaboratively makes decisions and executes corresponding actions in a distributed and parallel manner to achieve the common goal, which significantly improves work efficiency and effectiveness. Previous works have leveraged multi-agent joint training to achieve this goal. Recently, some studies have attempted to leverage the intelligence and capabilities of agents for autonomous collaboration.
    <cite class="ltx_cite ltx_citemacro_cite">
     Li etÂ al. (
     <a class="ltx_ref" href="#bib.bib16" title="">
      2023
     </a>
     )
    </cite>
    have conceptualized assemblies of agents as a group, and focused on exploring the potential of their cooperation.
    <cite class="ltx_cite ltx_citemacro_cite">
     Park etÂ al. (
     <a class="ltx_ref" href="#bib.bib26" title="">
      2023
     </a>
     )
    </cite>
    found social behaviors autonomously emerge within a group of agents, and
    <cite class="ltx_cite ltx_citemacro_cite">
     Du etÂ al. (
     <a class="ltx_ref" href="#bib.bib13" title="">
      2023
     </a>
     ); Wang etÂ al. (
     <a class="ltx_ref" href="#bib.bib45" title="">
      2023b
     </a>
     ); Zhang etÂ al. (
     <a class="ltx_ref" href="#bib.bib55" title="">
      2023a
     </a>
     ); Qian etÂ al. (
     <a class="ltx_ref" href="#bib.bib28" title="">
      2023a
     </a>
     ); Chan etÂ al. (
     <a class="ltx_ref" href="#bib.bib6" title="">
      2023
     </a>
     )
    </cite>
    further leverage multi-agent cooperation to achieve better performance on reasoning tasks. Based on these findings, we introduce a framework, denoted as
    <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.3">
     AgentVerse
    </span>
    , capable of leveraging group cooperation to manage more intricate scenarios. This framework can dynamically adjust its composition according to the current state, aiming to facilitate optimal decision-making and execution.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Conclusion
  </h2>
  <div class="ltx_para ltx_noindent" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    In this study, we present
    <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.1">
     AgentVerse
    </span>
    , a novel and general multi-agent framework designed to emulate human group problem-solving processes. Our comprehensive experimental results highlight the efficacy of
    <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.2">
     AgentVerse
    </span>
    , demonstrating its enhanced performance in comparison to individual agents across a myriad of tasks. These tasks encompass general understanding, reasoning, coding, and tool utilization. Notably,
    <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.3">
     AgentVerse
    </span>
    consistently delivers remarkable results in addressing intricate user queries when fortified with the appropriate tools. In our investigations within the Minecraft environment, we identify both positive and negative emergent social behaviors among agents. As advancements in artificial general intelligence progress, understanding multi-agent interactions should become increasingly crucial.
    <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.4">
     AgentVerse
    </span>
    serves as a valuable step toward this endeavor, and we are optimistic about its potential adaptability and refinement for a wider array of tasks and contexts in the future.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ahn etÂ al. (2022)
    </span>
    <span class="ltx_bibblock">
     Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, RosarioÂ Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, NikhilÂ J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan.
    </span>
    <span class="ltx_bibblock">
     Do as I can, not as I say: Grounding language in robotic affordances.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      CoRR
     </em>
     , abs/2204.01691, 2022.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2204.01691
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2204.01691" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2204.01691
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Amershi etÂ al. (2014)
    </span>
    <span class="ltx_bibblock">
     Saleema Amershi, Maya Cakmak, WilliamÂ Bradley Knox, and Todd Kulesza.
    </span>
    <span class="ltx_bibblock">
     Power to the people: The role of humans in interactive machine learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      AI Magazine
     </em>
     , 35(4):105â€“120, Dec. 2014.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.1609/aimag.v35i4.2513
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2513" target="_blank" title="">
      https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2513
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Anil etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     Rohan Anil, AndrewÂ M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, JonathanÂ H. Clark, LaurentÂ El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, YiÂ Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, GustavoÂ HernÃ¡ndez Ãbrego, Junwhan Ahn, Jacob Austin, Paul Barham, JanÂ A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, ChristopherÂ A. Choquette-Choo, Aakanksha Chowdhery, ClÃ©ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark DÃ­az, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and etÂ al.
    </span>
    <span class="ltx_bibblock">
     Palm 2 technical report.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      CoRR
     </em>
     , abs/2305.10403, 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2305.10403
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2305.10403" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2305.10403
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bransford &amp; Stein (1993)
    </span>
    <span class="ltx_bibblock">
     J.D. Bransford and B.S. Stein.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      The Ideal Problem Solver: A Guide for Improving Thinking, Learning, and Creativity
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     W.H. Freeman, 1993.
    </span>
    <span class="ltx_bibblock">
     ISBN 978-0-7167-2205-2.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://books.google.com.tw/books?id=nnRxQgAACAAJ" target="_blank" title="">
      https://books.google.com.tw/books?id=nnRxQgAACAAJ
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bubeck etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, YinÂ Tat Lee, Yuanzhi Li, ScottÂ M. Lundberg, Harsha Nori, Hamid Palangi, MarcoÂ TÃºlio Ribeiro, and YiÂ Zhang.
    </span>
    <span class="ltx_bibblock">
     Sparks of artificial general intelligence: Early experiments with GPT-4.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      CoRR
     </em>
     , abs/2303.12712, 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2303.12712
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2303.12712" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2303.12712
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chan etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu.
    </span>
    <span class="ltx_bibblock">
     Chateval: Towards better llm-based evaluators through multi-agent debate, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2308.07201" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2308.07201
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen etÂ al. (2021)
    </span>
    <span class="ltx_bibblock">
     Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, HenriqueÂ PondÃ© deÂ OliveiraÂ Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, FelipeÂ Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, WilliamÂ Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, AndrewÂ N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
    </span>
    <span class="ltx_bibblock">
     Evaluating large language models trained on code.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      CoRR
     </em>
     , abs/2107.03374, 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2107.03374" target="_blank" title="">
      https://arxiv.org/abs/2107.03374
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cialdini &amp; Goldstein (2004)
    </span>
    <span class="ltx_bibblock">
     RobertÂ B Cialdini and NoahÂ J Goldstein.
    </span>
    <span class="ltx_bibblock">
     Social influence: Compliance and conformity.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      Annu. Rev. Psychol.
     </em>
     , 55:591â€“621, 2004.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.annualreviews.org/doi/abs/10.1146/annurev.psych.55.090902.142015" target="_blank" title="">
      https://www.annualreviews.org/doi/abs/10.1146/annurev.psych.55.090902.142015
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cialdini &amp; Trost (1998)
    </span>
    <span class="ltx_bibblock">
     RobertÂ B Cialdini and MelanieÂ R Trost.
    </span>
    <span class="ltx_bibblock">
     Social influence: Social norms, conformity and compliance.
    </span>
    <span class="ltx_bibblock">
     1998.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://psycnet.apa.org/RECORD/1998-07091-021" target="_blank" title="">
      https://psycnet.apa.org/RECORD/1998-07091-021
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Clune (2019)
    </span>
    <span class="ltx_bibblock">
     Jeff Clune.
    </span>
    <span class="ltx_bibblock">
     Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artificial intelligence.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      CoRR
     </em>
     , abs/1905.10985, 2019.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1905.10985" target="_blank" title="">
      http://arxiv.org/abs/1905.10985
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cobbe etÂ al. (2021)
    </span>
    <span class="ltx_bibblock">
     Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
    </span>
    <span class="ltx_bibblock">
     Training verifiers to solve math word problems.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      CoRR
     </em>
     , abs/2110.14168, 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2110.14168" target="_blank" title="">
      https://arxiv.org/abs/2110.14168
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Driess etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     Danny Driess, Fei Xia, Mehdi S.Â M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence.
    </span>
    <span class="ltx_bibblock">
     Palm-e: An embodied multimodal language model.
    </span>
    <span class="ltx_bibblock">
     In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.),
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA
     </em>
     , volume 202 of
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.2.2">
      Proceedings of Machine Learning Research
     </em>
     , pp.Â  8469â€“8488. PMLR, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v202/driess23a.html" target="_blank" title="">
      https://proceedings.mlr.press/v202/driess23a.html
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Du etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yilun Du, Shuang Li, Antonio Torralba, JoshuaÂ B. Tenenbaum, and Igor Mordatch.
    </span>
    <span class="ltx_bibblock">
     Improving factuality and reasoning in language models through multiagent debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      CoRR
     </em>
     , abs/2305.14325, 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2305.14325
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2305.14325" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2305.14325
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fehr &amp; GÃ¤chter (2000)
    </span>
    <span class="ltx_bibblock">
     Ernst Fehr and Simon GÃ¤chter.
    </span>
    <span class="ltx_bibblock">
     Cooperation and punishment in public goods experiments.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      American Economic Review
     </em>
     , 90(4):980â€“994, 2000.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.aeaweb.org/doi/pdf/10.1257/aer.90.4.980" target="_blank" title="">
      https://pubs.aeaweb.org/doi/pdf/10.1257/aer.90.4.980
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Goertzel &amp; Pennachin (2007)
    </span>
    <span class="ltx_bibblock">
     Ben Goertzel and Cassio Pennachin.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      Artificial general intelligence
     </em>
     , volumeÂ 2.
    </span>
    <span class="ltx_bibblock">
     Springer, 2007.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://link.springer.com/book/10.1007/978-3-540-68677-4" target="_blank" title="">
      https://link.springer.com/book/10.1007/978-3-540-68677-4
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     Guohao Li, Hasan Abed AlÂ Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.
    </span>
    <span class="ltx_bibblock">
     CAMEL: communicative agents for â€mindâ€ exploration of large scale language model society.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      CoRR
     </em>
     , abs/2303.17760, 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2303.17760
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2303.17760" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2303.17760
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, YuÂ Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, YuÂ Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang.
    </span>
    <span class="ltx_bibblock">
     Agentbench: Evaluating llms as agents, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Madaan etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, BodhisattwaÂ Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark.
    </span>
    <span class="ltx_bibblock">
     Self-refine: Iterative refinement with self-feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      CoRR
     </em>
     , abs/2303.17651, 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2303.17651
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2303.17651" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2303.17651
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mehri &amp; EskÃ©nazi (2020)
    </span>
    <span class="ltx_bibblock">
     Shikib Mehri and Maxine EskÃ©nazi.
    </span>
    <span class="ltx_bibblock">
     Unsupervised evaluation of interactive dialog with dialogpt.
    </span>
    <span class="ltx_bibblock">
     In Olivier Pietquin, Smaranda Muresan, Vivian Chen, Casey Kennington, David Vandyke, Nina Dethlefs, Koji Inoue, Erik Ekstedt, and Stefan Ultes (eds.),
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGdial 2020, 1st virtual meeting, July 1-3, 2020
     </em>
     , pp.Â  225â€“235. Association for Computational Linguistics, 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.sigdial-1.28/" target="_blank" title="">
      https://aclanthology.org/2020.sigdial-1.28/
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Minsky (1988)
    </span>
    <span class="ltx_bibblock">
     Marvin Minsky.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      The Society of Mind
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     Simon &amp; Schuster, 1988.
    </span>
    <span class="ltx_bibblock">
     ISBN 0671657135.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://jmvidal.cse.sc.edu/lib/minsky88a.html" target="_blank" title="">
      https://jmvidal.cse.sc.edu/lib/minsky88a.html
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mowen &amp; Sujan (2005)
    </span>
    <span class="ltx_bibblock">
     JohnÂ C Mowen and Harish Sujan.
    </span>
    <span class="ltx_bibblock">
     Volunteer behavior: A hierarchical model approach for investigating its trait and functional motive antecedents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      Journal of consumer psychology
     </em>
     , 15(2):170â€“182, 2005.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://myscp.onlinelibrary.wiley.com/doi/abs/10.1207/s15327663jcp1502_9" target="_blank" title="">
      https://myscp.onlinelibrary.wiley.com/doi/abs/10.1207/s15327663jcp1502_9
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nakajima (2023)
    </span>
    <span class="ltx_bibblock">
     Yohei Nakajima.
    </span>
    <span class="ltx_bibblock">
     Babyagi.
    </span>
    <span class="ltx_bibblock">
     2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/yoheinakajima/babyagi" target="_blank" title="">
      https://github.com/yoheinakajima/babyagi
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     [Software].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Omoto &amp; Snyder (1995)
    </span>
    <span class="ltx_bibblock">
     AllenÂ M Omoto and Mark Snyder.
    </span>
    <span class="ltx_bibblock">
     Sustained helping without obligation: motivation, longevity of service, and perceived attitude change among aids volunteers.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      Journal of personality and social psychology
     </em>
     , 68(4):671, 1995.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://psycnet.apa.org/record/1995-26640-001" target="_blank" title="">
      https://psycnet.apa.org/record/1995-26640-001
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI (2023a)
    </span>
    <span class="ltx_bibblock">
     OpenAI.
    </span>
    <span class="ltx_bibblock">
     GPT-4 technical report.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      CoRR
     </em>
     , abs/2303.08774, 2023a.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2303.08774
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2303.08774" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2303.08774
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI (2023b)
    </span>
    <span class="ltx_bibblock">
     OpenAI.
    </span>
    <span class="ltx_bibblock">
     Chatgpt can now see, hear, and speak, 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt-can-now-see-hear-and-speak" target="_blank" title="">
      https://openai.com/blog/chatgpt-can-now-see-hear-and-speak
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Park etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     JoonÂ Sung Park, JosephÂ C. Oâ€™Brien, CarrieÂ J. Cai, MeredithÂ Ringel Morris, Percy Liang, and MichaelÂ S. Bernstein.
    </span>
    <span class="ltx_bibblock">
     Generative agents: Interactive simulacra of human behavior.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      CoRR
     </em>
     , abs/2304.03442, 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2304.03442
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2304.03442" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2304.03442
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Phillips &amp; Oâ€™Reilly (1998)
    </span>
    <span class="ltx_bibblock">
     Katherine Phillips and Charles Oâ€™Reilly.
    </span>
    <span class="ltx_bibblock">
     Demography and diversity in organizations: A review of 40 years of research.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">
      Research in Organizational Behavior
     </em>
     , 20:77â€“140, 01 1998.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.researchgate.net/publication/234022034_Demography_and_Diversity_in_Organizations_A_Review_of_40_Years_of_Research" target="_blank" title="">
      https://www.researchgate.net/publication/234022034_Demography_and_Diversity_in_Organizations_A_Review_of_40_Years_of_Research
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qian etÂ al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun.
    </span>
    <span class="ltx_bibblock">
     Communicative agents for software development.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      CoRR
     </em>
     , abs/2307.07924, 2023a.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2307.07924
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2307.07924" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2307.07924
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qian etÂ al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Cheng Qian, Chi Han, YiÂ R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji.
    </span>
    <span class="ltx_bibblock">
     CREATOR: disentangling abstract and concrete reasonings of large language models through tool creation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      CoRR
     </em>
     , abs/2305.14318, 2023b.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2305.14318
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2305.14318" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2305.14318
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qin etÂ al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, YiÂ Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, XuÂ Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun.
    </span>
    <span class="ltx_bibblock">
     Tool learning with foundation models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      CoRR
     </em>
     , abs/2304.08354, 2023a.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2304.08354
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2304.08354" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2304.08354
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qin etÂ al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, etÂ al.
    </span>
    <span class="ltx_bibblock">
     Toolllm: Facilitating large language models to master 16000+ real-world apis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      arXiv preprint arXiv:2307.16789
     </em>
     , 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.16789" target="_blank" title="">
      https://arxiv.org/abs/2307.16789
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Reworkd (2023)
    </span>
    <span class="ltx_bibblock">
     Reworkd.
    </span>
    <span class="ltx_bibblock">
     Agentgpt, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/reworkd/AgentGPT" target="_blank" title="">
      https://github.com/reworkd/AgentGPT
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     [Software].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Richards &amp; etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     ToranÂ Bruce Richards and etÂ al.
    </span>
    <span class="ltx_bibblock">
     Auto-gpt: An autonomous gpt-4 experiment, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Significant-Gravitas/Auto-GPT" target="_blank" title="">
      https://github.com/Significant-Gravitas/Auto-GPT
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     [Software].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Salewski etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, and Zeynep Akata.
    </span>
    <span class="ltx_bibblock">
     In-context impersonation reveals large language modelsâ€™ strengths and biases.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      CoRR
     </em>
     , abs/2305.14930, 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2305.14930
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2305.14930" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2305.14930
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schick etÂ al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    </span>
    <span class="ltx_bibblock">
     Toolformer: Language models can teach themselves to use tools.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      CoRR
     </em>
     , abs/2302.04761, 2023a.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2302.04761
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2302.04761" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2302.04761
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schick etÂ al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    </span>
    <span class="ltx_bibblock">
     Toolformer: Language models can teach themselves to use tools.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">
      CoRR
     </em>
     , abs/2302.04761, 2023b.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2302.04761
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2302.04761" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2302.04761
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shi etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, HyungÂ Won Chung, YiÂ Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei.
    </span>
    <span class="ltx_bibblock">
     Language models are multilingual chain-of-thought reasoners.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023
     </em>
     . OpenReview.net, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/pdf?id=fR3wGCk-IXp" target="_blank" title="">
      https://openreview.net/pdf?id=fR3wGCk-IXp
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shinn etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.
    </span>
    <span class="ltx_bibblock">
     Reflexion: Language agents with verbal reinforcement learning, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2303.11366" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2303.11366
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Srivastava etÂ al. (2022)
    </span>
    <span class="ltx_bibblock">
     Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu AwalÂ Md Shoeb, Abubakar Abid, Adam Fisch, AdamÂ R. Brown, Adam Santoro, Aditya Gupta, AdriÃ  Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, AlexanderÂ W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, AnantharamanÂ S. Iyer, Anders Andreassen, Andrea Santilli, Andreas StuhlmÃ¼ller, AndrewÂ M. Dai, Andrew La, AndrewÂ K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and etÂ al.
    </span>
    <span class="ltx_bibblock">
     Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      CoRR
     </em>
     , abs/2206.04615, 2022.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2206.04615
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2206.04615" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2206.04615
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Stiennon etÂ al. (2020)
    </span>
    <span class="ltx_bibblock">
     Nisan Stiennon, Long Ouyang, Jeffrey Wu, DanielÂ M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and PaulÂ F. Christiano.
    </span>
    <span class="ltx_bibblock">
     Learning to summarize with human feedback.
    </span>
    <span class="ltx_bibblock">
     In Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual
     </em>
     , 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html" target="_blank" title="">
      https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Stone &amp; Veloso (2000)
    </span>
    <span class="ltx_bibblock">
     Peter Stone and Manuela Veloso.
    </span>
    <span class="ltx_bibblock">
     Multiagent systems: A survey from a machine learning perspective.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      Auton. Robots
     </em>
     , 8(3):345â€“383, jun 2000.
    </span>
    <span class="ltx_bibblock">
     ISSN 0929-5593.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.1023/A:1008942012299
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1023/A:1008942012299" target="_blank" title="">
      https://doi.org/10.1023/A:1008942012299
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron etÂ al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, AurÃ©lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
    </span>
    <span class="ltx_bibblock">
     Llama: Open and efficient foundation language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">
      CoRR
     </em>
     , abs/2302.13971, 2023a.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2302.13971
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2302.13971" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2302.13971
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron etÂ al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, PunitÂ Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, EricÂ Michael Smith, Ranjan Subramanian, XiaoqingÂ Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, JianÂ Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, AurÃ©lien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom.
    </span>
    <span class="ltx_bibblock">
     Llama 2: Open foundation and fine-tuned chat models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">
      CoRR
     </em>
     , abs/2307.09288, 2023b.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2307.09288
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2307.09288" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2307.09288
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang etÂ al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.
    </span>
    <span class="ltx_bibblock">
     Voyager: An open-ended embodied agent with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">
      CoRR
     </em>
     , abs/2305.16291, 2023a.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2305.16291
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2305.16291" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2305.16291
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang etÂ al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji.
    </span>
    <span class="ltx_bibblock">
     Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">
      CoRR
     </em>
     , abs/2307.05300, 2023b.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2307.05300
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2307.05300" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2307.05300
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei etÂ al. (2022a)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Maarten Bosma, VincentÂ Y. Zhao, Kelvin Guu, AdamsÂ Wei Yu, Brian Lester, Nan Du, AndrewÂ M. Dai, and QuocÂ V. Le.
    </span>
    <span class="ltx_bibblock">
     Finetuned language models are zero-shot learners.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">
      The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022
     </em>
     . OpenReview.net, 2022a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=gEZrGCozdqR" target="_blank" title="">
      https://openreview.net/forum?id=gEZrGCozdqR
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei etÂ al. (2022b)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, EdÂ H. Chi, QuocÂ V. Le, and Denny Zhou.
    </span>
    <span class="ltx_bibblock">
     Chain-of-thought prompting elicits reasoning in large language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">
      NeurIPS
     </em>
     , 2022b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html" target="_blank" title="">
      http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     Jimmy Wei, Kurt Shuster, Arthur Szlam, Jason Weston, Jack Urbanek, and Mojtaba Komeili.
    </span>
    <span class="ltx_bibblock">
     Multi-party chat: Conversational agents in group settings with humans and models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">
      CoRR
     </em>
     , abs/2304.13835, 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2304.13835
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2304.13835" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2304.13835
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wooldridge &amp; Jennings (1995)
    </span>
    <span class="ltx_bibblock">
     MichaelÂ J. Wooldridge and NicholasÂ R. Jennings.
    </span>
    <span class="ltx_bibblock">
     Intelligent agents: theory and practice.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">
      Knowl. Eng. Rev.
     </em>
     , 10(2):115â€“152, 1995.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.1017/S0269888900008122
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1017/S0269888900008122" target="_blank" title="">
      https://doi.org/10.1017/S0269888900008122
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Woolley etÂ al. (2010)
    </span>
    <span class="ltx_bibblock">
     AnitaÂ Williams Woolley, ChristopherÂ F. Chabris, Alex Pentland, Nada Hashmi, and ThomasÂ W. Malone.
    </span>
    <span class="ltx_bibblock">
     Evidence for a collective intelligence factor in the performance of human groups.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">
      Science
     </em>
     , 330(6004):686â€“688, 2010.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.1126/science.1193147
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.science.org/doi/abs/10.1126/science.1193147" target="_blank" title="">
      https://www.science.org/doi/abs/10.1126/science.1193147
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib51">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Woolley etÂ al. (2015)
    </span>
    <span class="ltx_bibblock">
     AnitaÂ Williams Woolley, Ishani Aggarwal, and ThomasÂ W. Malone.
    </span>
    <span class="ltx_bibblock">
     Collective intelligence and group performance.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">
      Current Directions in Psychological Science
     </em>
     , 24(6):420â€“424, 2015.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.1177/0963721415599543
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1177/0963721415599543" target="_blank" title="">
      https://doi.org/10.1177/0963721415599543
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib52">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, LiÂ Jiang, Xiaoyun Zhang, and Chi Wang.
    </span>
    <span class="ltx_bibblock">
     Autogen: Enabling next-gen llm applications via multi-agent conversation framework, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2308.08155" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2308.08155
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib53">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao etÂ al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, ThomasÂ L. Griffiths, Yuan Cao, and Karthik Narasimhan.
    </span>
    <span class="ltx_bibblock">
     Tree of thoughts: Deliberate problem solving with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">
      CoRR
     </em>
     , abs/2305.10601, 2023a.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2305.10601
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2305.10601" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2305.10601
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib54">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao etÂ al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, KarthikÂ R. Narasimhan, and Yuan Cao.
    </span>
    <span class="ltx_bibblock">
     React: Synergizing reasoning and acting in language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">
      The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023
     </em>
     . OpenReview.net, 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/pdf?id=WE_vluYUL-X" target="_blank" title="">
      https://openreview.net/pdf?id=WE_vluYUL-X
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib55">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang etÂ al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, JoshuaÂ B. Tenenbaum, Tianmin Shu, and Chuang Gan.
    </span>
    <span class="ltx_bibblock">
     Building cooperative embodied agents modularly with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">
      CoRR
     </em>
     , abs/2307.02485, 2023a.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2307.02485
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2307.02485" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2307.02485
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib56">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang etÂ al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li.
    </span>
    <span class="ltx_bibblock">
     Wider and deeper llm networks are fairer llm evaluators.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">
      arXiv preprint arXiv:2308.01862
     </em>
     , 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2308.01862" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2308.01862
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib57">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhou etÂ al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shuyan Zhou, FrankÂ F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig.
    </span>
    <span class="ltx_bibblock">
     Webarena: A realistic web environment for building autonomous agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">
      CoRR
     </em>
     , abs/2307.13854, 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.48550/arXiv.2307.13854
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2307.13854" target="_blank" title="">
      https://doi.org/10.48550/arXiv.2307.13854
     </a>
     .
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Configurations of the Experiments
  </h2>
  <section class="ltx_paragraph" id="A1.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Datasets and Evaluation Metrics
   </h4>
   <div class="ltx_para ltx_noindent" id="A1.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="A1.SS0.SSS0.Px1.p1.1">
     Our evaluation assesses different aspects of agents, including general understanding and reasoning capabilities, coding capabilities and tool utilization capabilities.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="A1.SS0.SSS0.Px1.p2">
    <ul class="ltx_itemize" id="A1.I1">
     <li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       â€¢
      </span>
      <div class="ltx_para ltx_noindent" id="A1.I1.i1.p1">
       <p class="ltx_p" id="A1.I1.i1.p1.1">
        <span class="ltx_text ltx_font_bold" id="A1.I1.i1.p1.1.1">
         General Understanding Capabilities
        </span>
        : We utilize two datasets. The first one is a Dialogue response dataset, FED
        <cite class="ltx_cite ltx_citemacro_citep">
         (Mehri &amp; EskÃ©nazi,
         <a class="ltx_ref" href="#bib.bib19" title="">
          2020
         </a>
         )
        </cite>
        , where given a multi-round chat history, the agent or agent group is required to generate the next chat. Following previous work
        <cite class="ltx_cite ltx_citemacro_citep">
         (Madaan etÂ al.,
         <a class="ltx_ref" href="#bib.bib18" title="">
          2023
         </a>
         )
        </cite>
        , we utilize GPT-4 as the evaluator to score the agent-generated response against the human-written ones, and report the agentâ€™s win rate. The second dataset is Commongen-Challenge
        <cite class="ltx_cite ltx_citemacro_citep">
         (Madaan etÂ al.,
         <a class="ltx_ref" href="#bib.bib18" title="">
          2023
         </a>
         )
        </cite>
        , which is a constrained generation dataset where given 20 concepts, the agent is required to generate a coherent and grammatically correct paragraph containing as many concepts as possible. We report the average percentage of the covered concepts.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       â€¢
      </span>
      <div class="ltx_para ltx_noindent" id="A1.I1.i2.p1">
       <p class="ltx_p" id="A1.I1.i2.p1.1">
        <span class="ltx_text ltx_font_bold" id="A1.I1.i2.p1.1.1">
         General Reasoning Capabilities
        </span>
        : We utilize the English subset of MGSM
        <cite class="ltx_cite ltx_citemacro_citep">
         (Shi etÂ al.,
         <a class="ltx_ref" href="#bib.bib37" title="">
          2023
         </a>
         )
        </cite>
        , which is a subset of GSM-8k
        <cite class="ltx_cite ltx_citemacro_citep">
         (Cobbe etÂ al.,
         <a class="ltx_ref" href="#bib.bib11" title="">
          2021
         </a>
         )
        </cite>
        , to evaluate the agentsâ€™ mathematical reasoning capabilities. It is a dataset containing grade school math problems. We report the percentage of the correct answers. And we use the logic grid puzzles task from BigBench
        <cite class="ltx_cite ltx_citemacro_citep">
         (Srivastava etÂ al.,
         <a class="ltx_ref" href="#bib.bib39" title="">
          2022
         </a>
         )
        </cite>
        , which contains logic problems that requires multi-step logic reasoning, to assess the agentsâ€™ logical reasoning capabilities. We report the accuracy.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       â€¢
      </span>
      <div class="ltx_para ltx_noindent" id="A1.I1.i3.p1">
       <p class="ltx_p" id="A1.I1.i3.p1.1">
        <span class="ltx_text ltx_font_bold" id="A1.I1.i3.p1.1.1">
         Coding Capabilities
        </span>
        : We utilize Humaneval
        <cite class="ltx_cite ltx_citemacro_citep">
         (Chen etÂ al.,
         <a class="ltx_ref" href="#bib.bib7" title="">
          2021
         </a>
         )
        </cite>
        , which is a code completion dataset, and report Pass@1 metric
        <span class="ltx_note ltx_role_footnote" id="footnote1">
         <sup class="ltx_note_mark">
          1
         </sup>
         <span class="ltx_note_outer">
          <span class="ltx_note_content">
           <sup class="ltx_note_mark">
            1
           </sup>
           <span class="ltx_tag ltx_tag_note">
            1
           </span>
           The method for calculating Pass@1 differs from the approach in
           <cite class="ltx_cite ltx_citemacro_citet">
            Chen etÂ al. (
            <a class="ltx_ref" href="#bib.bib7" title="">
             2021
            </a>
            )
           </cite>
           . Instead of generating multiple responses and calculating an unbiased estimator, we directly employ the first response to compute the Pass@1.
          </span>
         </span>
        </span>
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       â€¢
      </span>
      <div class="ltx_para ltx_noindent" id="A1.I1.i4.p1">
       <p class="ltx_p" id="A1.I1.i4.p1.1">
        <span class="ltx_text ltx_font_bold" id="A1.I1.i4.p1.1.1">
         Tool Utilization Capabilities
        </span>
        : Since automatic evaluation on the performance of tool utilization is difficult, and there is currently no relevant benchmark, we craft 10 complex instructions and manually assess the performance. The instructions are listed in
        <a class="ltx_ref" href="#A2" title="Appendix B Experiment Details for Multi-Agent Tool Using â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
         <span class="ltx_text ltx_ref_tag">
          Appendix
         </span>
         <span class="ltx_text ltx_ref_tag">
          B
         </span>
        </a>
        .
       </p>
      </div>
     </li>
    </ul>
   </div>
  </section>
  <section class="ltx_paragraph" id="A1.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    Expert Recruitment
   </h4>
   <div class="ltx_para ltx_noindent" id="A1.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="A1.SS0.SSS0.Px2.p1.1">
     For tasks including dialogue response, code completion, and constrained generation, four agents is recruited into the system. For the task of mathematical reasoning, we limited the number to two agents. This decision was based on our observation that an increase in the number of reviewers for mathematical reasoning tasks correlates with a higher likelihood of them giving erroneous critiques, leading to incorrect solutions by the solver. We have a discussion on this topic in
     <a class="ltx_ref" href="#S3.SS1" title="3.1 General Understanding and Reasoning Capabilities â€£ 3 Experiments â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Section
      </span>
      <span class="ltx_text ltx_ref_tag">
       3.1
      </span>
     </a>
     . For tool utilization, we recruit two or three agents to engage in collaborative decision-making and action execution depending on the specific task. The detailed setups are listed at
     <a class="ltx_ref" href="#A2" title="Appendix B Experiment Details for Multi-Agent Tool Using â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Appendix
      </span>
      <span class="ltx_text ltx_ref_tag">
       B
      </span>
     </a>
     . Currently the number of experts is pre-defined by us for each task. We are seeking a way to automate this decision as well.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="A1.SS0.SSS0.Px3">
   <h4 class="ltx_title ltx_title_paragraph">
    Collaborative Decision-Making
   </h4>
   <div class="ltx_para ltx_noindent" id="A1.SS0.SSS0.Px3.p1">
    <p class="ltx_p" id="A1.SS0.SSS0.Px3.p1.1">
     For tasks in coding and general understanding and reasoning, we use the vertical structure because all these tasks require only one response as the answer, and the solver in the vertical structure can be responsible for answering. For tool utilization, we use the horizontal structure because the agents should clarify their own sub-tasks in the discussion.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="A1.SS0.SSS0.Px4">
   <h4 class="ltx_title ltx_title_paragraph">
    Action Execution
   </h4>
   <div class="ltx_para ltx_noindent" id="A1.SS0.SSS0.Px4.p1">
    <p class="ltx_p" id="A1.SS0.SSS0.Px4.p1.1">
     For the Humaneval code completion dataset benchmarked with GPT-4, we incorporate an additional agent during the action execution stage to craft unit testing code (in an zero-shot manner). Subsequently, the generated code is subjected to unit testing, and the testing results are conveyed as the environment state to the evaluation module.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="A1.SS0.SSS0.Px4.p2">
    <p class="ltx_p" id="A1.SS0.SSS0.Px4.p2.1">
     Regarding the constrained generation dataset, Commongen-Challenge, the agent-generated response undergoes a concept coverage check. Any missing concepts are then passed to the evaluation module as the environment state.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="A1.SS0.SSS0.Px4.p3">
    <p class="ltx_p" id="A1.SS0.SSS0.Px4.p3.1">
     In the context of tool utilization, each agent iteratively calls the tool in the ReAct manner, up to a maximum of 10 iterations. Upon reaching the final iteration, the agent is forced to draw a conclusion regarding the result, labeling the taskâ€™s status as either â€pendingâ€ or â€finishedâ€. These conclusions are then forwarded to the evaluator for assessment.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="A1.SS0.SSS0.Px5">
   <h4 class="ltx_title ltx_title_paragraph">
    Evaluation
   </h4>
   <div class="ltx_para ltx_noindent" id="A1.SS0.SSS0.Px5.p1">
    <p class="ltx_p" id="A1.SS0.SSS0.Px5.p1.3">
     To facilitate a feedback loop, an agent was tasked with the role of evaluator. This agent, provided with the initial problem
     <math alttext="p" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px5.p1.1.m1.1">
      <semantics id="A1.SS0.SSS0.Px5.p1.1.m1.1a">
       <mi id="A1.SS0.SSS0.Px5.p1.1.m1.1.1" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1.cmml">
        p
       </mi>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px5.p1.1.m1.1b">
        <ci id="A1.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1">
         ğ‘
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px5.p1.1.m1.1c">
        p
       </annotation>
      </semantics>
     </math>
     and the decisions
     <math alttext="A" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px5.p1.2.m2.1">
      <semantics id="A1.SS0.SSS0.Px5.p1.2.m2.1a">
       <mi id="A1.SS0.SSS0.Px5.p1.2.m2.1.1" xref="A1.SS0.SSS0.Px5.p1.2.m2.1.1.cmml">
        A
       </mi>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px5.p1.2.m2.1b">
        <ci id="A1.SS0.SSS0.Px5.p1.2.m2.1.1.cmml" xref="A1.SS0.SSS0.Px5.p1.2.m2.1.1">
         ğ´
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px5.p1.2.m2.1c">
        A
       </annotation>
      </semantics>
     </math>
     made during the collaborative decision-making stage, is charged with determining the correctness of those decisions. In cases where the decision is identified as erroneous, feedback is channeled back to the expert recruitment stage. If the decision meets the accuracy criteria, it is determined as the final answer to
     <math alttext="p" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px5.p1.3.m3.1">
      <semantics id="A1.SS0.SSS0.Px5.p1.3.m3.1a">
       <mi id="A1.SS0.SSS0.Px5.p1.3.m3.1.1" xref="A1.SS0.SSS0.Px5.p1.3.m3.1.1.cmml">
        p
       </mi>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px5.p1.3.m3.1b">
        <ci id="A1.SS0.SSS0.Px5.p1.3.m3.1.1.cmml" xref="A1.SS0.SSS0.Px5.p1.3.m3.1.1">
         ğ‘
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px5.p1.3.m3.1c">
        p
       </annotation>
      </semantics>
     </math>
     . While our current configuration employs an agent for evaluation, we acknowledge the potential of human evaluators and intend to incorporate such experiments in future endeavors.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Experiment Details for Multi-Agent Tool Using
  </h2>
  <section class="ltx_subsection" id="A2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.1
    </span>
    Setups
   </h3>
   <div class="ltx_para ltx_noindent" id="A2.SS1.p1">
    <p class="ltx_p" id="A2.SS1.p1.1">
     This section provides specific implementation details for enabling multiple agents in
     <span class="ltx_text ltx_font_smallcaps" id="A2.SS1.p1.1.1">
      AgentVerse
     </span>
     to collaboratively utilize tools to accomplish userâ€™s query. Unless specified herein, the implementation adheres to the standard procedures defined in the other experiments.
    </p>
   </div>
   <section class="ltx_paragraph" id="A2.SS1.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Collaborative Decision-Making
    </h4>
    <div class="ltx_para ltx_noindent" id="A2.SS1.SSS0.Px1.p1">
     <p class="ltx_p" id="A2.SS1.SSS0.Px1.p1.1">
      Agents recruited during the Expert Recruitment stage engage in collaborative discussions regarding the assigned task using a horizontal communication structure. In this configuration, agents communicate in a predetermined sequential order. At the conclusion of their discussions, an additional agent is designated as the â€summarizerâ€ and is responsible for consolidating the discussed sub-tasks for each participant.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="A2.SS1.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Action Execution
    </h4>
    <div class="ltx_para ltx_noindent" id="A2.SS1.SSS0.Px2.p1">
     <p class="ltx_p" id="A2.SS1.SSS0.Px2.p1.1">
      During action execution, we follow the ReAct prompting technique to let the agents write down their thought, and then call the tool, and we return the tool response to the agent. The agent again write down the thought and call the tools. The process iterates until the agent decide to submit its conclusion, or a maximum iteration number, which we set to be 10, is reached. In practice, we use OpenAIâ€™s function call to handle the tools.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="A2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.2
    </span>
    Results
   </h3>
   <div class="ltx_para ltx_noindent" id="A2.SS2.p1">
    <p class="ltx_p" id="A2.SS2.p1.1">
     For action execution, the ReAct prompting technique is employed. Agents articulate their thought process, invoke the respective tool, and subsequently receive feedback from the tool. This cycle of reflection and tool interaction continues iteratively. The process concludes either when the agent finalizes its execution with its conclusion or after a pre-set maximum number of iterationsâ€”set (10 in our experiments). We utilize OpenAIâ€™s function call to handle the tool calls.
    </p>
   </div>
   <section class="ltx_paragraph" id="A2.SS2.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Instruction 1
    </h4>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px1.p1">
     <p class="ltx_p" id="A2.SS2.SSS0.Px1.p1.1">
      Recently, it has become popular in the AI field to verify the mathematical reasoning abilities of large language models by observing if they can solve the â€24-Point Game.â€ What is this game? Does it have a code-based solution? If it does, provide a Python code along with test cases and test its functionality. What are some other similar games that can be used to test the modelsâ€™ mathematical reasoning abilities?
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px1.p2">
     <p class="ltx_p" id="A2.SS2.SSS0.Px1.p2.1">
      <span class="ltx_text" id="A2.SS2.SSS0.Px1.p2.1.1" style="color:#0000FF;">
       Single ReAct Agent
      </span>
      : (The generated code is omitted) The â€™24-Point Gameâ€™ is a mathematical game where four numbers must be used with the basic arithmetic operations (addition, subtraction, multiplication, and division) to make the number 24. The Python code I developed generates all possible combinations of the four numbers and operations to find a solution that equals 24. The code was tested with different sets of numbers, and it correctly identified whether a solution existed and what it was. This task tested my ability to understand the rules of a game and to develop and test a program that solves it. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px1.p2.1.2" style="color:#008080;">
       game rule âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px1.p2.1.3" style="color:#008080;">
       code âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px1.p2.1.4" style="color:#FF0000;">
       similar games âœ–
      </span>
      )
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px1.p3">
     <p class="ltx_p" id="A2.SS2.SSS0.Px1.p3.1">
      <span class="ltx_text ltx_font_smallcaps" id="A2.SS2.SSS0.Px1.p3.1.1" style="color:#FF00FF;">
       AgentVerse
       <span class="ltx_text ltx_font_upright" id="A2.SS2.SSS0.Px1.p3.1.1.1">
        (2 agents)
       </span>
      </span>
      : (The generated code is omitted) The â€24-Point Gameâ€ is a mathematical game where four numbers must be used with the basic arithmetic operations (addition, subtraction, multiplication, and division) to make the number 24. A Python code was developed and tested that generates all possible combinations of the four numbers and operations to find a solution that equals 24. The code was tested with different sets of numbers, and it correctly identified whether a solution existed and what it was. Similar games to the â€™24-Point Gameâ€™ that can be used to test mathematical reasoning abilities include â€™Make a Numberâ€™ and â€™Math Safeâ€™. â€™Make a Numberâ€™ is a game where players are given 4 numbers and 4 operations and they have to use these to create a target number. â€™Math Safeâ€™ is a game where players use four given single-digit numbers and any of the four operations to reach a target number and open a safe. Both these games require the use of operations and logical thinking to reach a target number, similar to the â€™24-Point Gameâ€™. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px1.p3.1.2" style="color:#008080;">
       game rule âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px1.p3.1.3" style="color:#008080;">
       code âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px1.p3.1.4" style="color:#008080;">
       similar games âœ”
      </span>
      )
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="A2.SS2.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Instruction 2 (Additional API: Weather)
    </h4>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px2.p1">
     <p class="ltx_p" id="A2.SS2.SSS0.Px2.p1.1">
      I want to hold a party at somewhere around Tsinghua University tomorrow. I need you to look for some best places for holding a party nearby, and tell me whether the weather is good for holding a party tomorrow. Also, I want to know what activities can be considered in my party. Help me search the web.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px2.p2">
     <p class="ltx_p" id="A2.SS2.SSS0.Px2.p2.1">
      <span class="ltx_text" id="A2.SS2.SSS0.Px2.p2.1.1" style="color:#0000FF;">
       Single ReAct Agent
      </span>
      : There are several restaurants near Tsinghua University that could be good places for a party, including Lan Yang ShuFang, YiLan ShaoKao, Guan Shi JiChi, Dong Lai Shun, and Xibu Mahua Beef Noodle. The weather in Beijing tomorrow is expected to be overcast with a temperature of 26 degrees Celsius, which might be suitable for a party. Some activities that can be considered for the party include the Saran Wrap Game, Post It Note Game, Most Likely To, Never Have I Ever, Would You Rather, and many more. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px2.p2.1.2" style="color:#008080;">
       âœ”
      </span>
      )
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px2.p3">
     <p class="ltx_p" id="A2.SS2.SSS0.Px2.p3.1">
      <span class="ltx_text ltx_font_smallcaps" id="A2.SS2.SSS0.Px2.p3.1.1" style="color:#FF00FF;">
       AgentVerse
       <span class="ltx_text ltx_font_upright" id="A2.SS2.SSS0.Px2.p3.1.1.1">
        (3 agents)
       </span>
      </span>
      : The weather in the Tsinghua University area tomorrow is expected to be overcast with a temperature of 26 degrees Celsius, which is suitable for holding a party. The best places for a party near Tsinghua University include the Summer Palace, Wudaokou, Zhongguancun, and Peking University, as well as several restaurants nearby. For party activities, you could consider games such as the Saran Wrap Game, the Post It Note Game, Howâ€™s Yours?, Most Likely To, Never Have I Ever, Would You Rather?, Heads Up!, Iâ€™m Hosting a Partyâ€¦, Two Truths and a Lie, Sticker Stalker, Great Minds Think Alike, murder mystery games, Who Am I?, the Newlywed Game, Truth or Dare, Spoons, Pictionary, Kiss Marry Kill, the Donâ€™t Say Yes game, Cards Against Humanity, What Do You Meme, Drawing Without Dignity, and Thatâ€™s What She Said. These games are suitable for various party occasions and there are also suggestions for icebreakers for guests who donâ€™t know each other well. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px2.p3.1.2" style="color:#008080;">
       âœ”provide more details
      </span>
      )
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="A2.SS2.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Instruction 3 (Additional API: Billboard)
    </h4>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px3.p1">
     <p class="ltx_p" id="A2.SS2.SSS0.Px3.p1.1">
      Iâ€™m currently analyzing what is popular on the website. Can you help me find the recent trending stuff. It could be anything, like trending news, products, books, movies, music, etc. Give a summarization for me.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px3.p2">
     <p class="ltx_p" id="A2.SS2.SSS0.Px3.p2.1">
      <span class="ltx_text" id="A2.SS2.SSS0.Px3.p2.1.1" style="color:#0000FF;">
       Single ReAct Agent
      </span>
      : The recent trending products on the web include Apple AirPods Pro, Diamond Dazzle Cleaner Pen, Ugg Slippers, Squishmallows, Winix Air Purifier, ThisWorx Car Vacuum Cleaner, All-Clad Cookware, Eufy Robot Vacuum, Renpho Eye Massager, Crocs, Throw blankets, Chuckit! Sport Dog Ball Launcher, Apple AirTags, Revlon Hair Dryer Brush, Lego Sets, KitchenAid Stand Mixer, Winc Wine Subscription, Saatva Mattress, Home Chef Subscription, and MasterClass Subscription. Unfortunately, we were unable to find specific information on recent trending books. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px3.p2.1.2" style="color:#008080;">
       trending product âœ”
      </span>
      )
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px3.p3">
     <p class="ltx_p" id="A2.SS2.SSS0.Px3.p3.1">
      <span class="ltx_text ltx_font_smallcaps" id="A2.SS2.SSS0.Px3.p3.1.1" style="color:#FF00FF;">
       AgentVerse
       <span class="ltx_text ltx_font_upright" id="A2.SS2.SSS0.Px3.p3.1.1.1">
        (3 agents)
       </span>
      </span>
      : Here is a summary of the trending topics on the website:
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px3.p4">
     <p class="ltx_p" id="A2.SS2.SSS0.Px3.p4.1">
      Music:
     </p>
     <ol class="ltx_enumerate" id="A2.I1">
      <li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        1.
       </span>
       <div class="ltx_para" id="A2.I1.i1.p1">
        <p class="ltx_p" id="A2.I1.i1.p1.1">
         â€™Heat Wavesâ€™ by Glass Animals
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        2.
       </span>
       <div class="ltx_para" id="A2.I1.i2.p1">
        <p class="ltx_p" id="A2.I1.i2.p1.1">
         â€™We Donâ€™t Talk About Brunoâ€™ by Carolina Gaitan, Mauro Castillo, Adassa, Rhenzy Feliz, Diane Guerrero, Stephanie Beatriz &amp; Encanto Cast
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I1.i3" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        3.
       </span>
       <div class="ltx_para" id="A2.I1.i3.p1">
        <p class="ltx_p" id="A2.I1.i3.p1.1">
         â€™Super Gremlinâ€™ by Kodak Black
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I1.i4" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        4.
       </span>
       <div class="ltx_para" id="A2.I1.i4.p1">
        <p class="ltx_p" id="A2.I1.i4.p1.1">
         â€™Slime You Outâ€™ by Drake Featuring SZA
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I1.i5" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        5.
       </span>
       <div class="ltx_para" id="A2.I1.i5.p1">
        <p class="ltx_p" id="A2.I1.i5.p1.1">
         â€™Paint The Town Redâ€™ by Doja Cat
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I1.i6" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        6.
       </span>
       <div class="ltx_para ltx_noindent" id="A2.I1.i6.p1">
        <p class="ltx_p" id="A2.I1.i6.p1.1">
         â€™Snoozeâ€™ by SZA
        </p>
       </div>
      </li>
     </ol>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px3.p5">
     <p class="ltx_p" id="A2.SS2.SSS0.Px3.p5.1">
      Movies:
     </p>
     <ol class="ltx_enumerate" id="A2.I2">
      <li class="ltx_item" id="A2.I2.i1" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        1.
       </span>
       <div class="ltx_para" id="A2.I2.i1.p1">
        <p class="ltx_p" id="A2.I2.i1.p1.1">
         â€™No One Will Save Youâ€™
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I2.i2" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        2.
       </span>
       <div class="ltx_para" id="A2.I2.i2.p1">
        <p class="ltx_p" id="A2.I2.i2.p1.1">
         â€™A Haunting in Veniceâ€™
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I2.i3" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        3.
       </span>
       <div class="ltx_para ltx_noindent" id="A2.I2.i3.p1">
        <p class="ltx_p" id="A2.I2.i3.p1.1">
         â€™Expend4blesâ€™
        </p>
       </div>
      </li>
     </ol>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px3.p6">
     <p class="ltx_p" id="A2.SS2.SSS0.Px3.p6.1">
      Books:
     </p>
     <ol class="ltx_enumerate" id="A2.I3">
      <li class="ltx_item" id="A2.I3.i1" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        1.
       </span>
       <div class="ltx_para" id="A2.I3.i1.p1">
        <p class="ltx_p" id="A2.I3.i1.p1.1">
         â€™The Midnight Libraryâ€™
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I3.i2" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        2.
       </span>
       <div class="ltx_para" id="A2.I3.i2.p1">
        <p class="ltx_p" id="A2.I3.i2.p1.1">
         â€™The Invisible Life of Addie LaRueâ€™
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I3.i3" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        3.
       </span>
       <div class="ltx_para ltx_noindent" id="A2.I3.i3.p1">
        <p class="ltx_p" id="A2.I3.i3.p1.1">
         â€™The Seven Husbands of Evelyn Hugoâ€™
        </p>
       </div>
      </li>
     </ol>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px3.p7">
     <p class="ltx_p" id="A2.SS2.SSS0.Px3.p7.1">
      Products:
     </p>
     <ol class="ltx_enumerate" id="A2.I4">
      <li class="ltx_item" id="A2.I4.i1" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        1.
       </span>
       <div class="ltx_para" id="A2.I4.i1.p1">
        <p class="ltx_p" id="A2.I4.i1.p1.1">
         Laser hair removal
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i2" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        2.
       </span>
       <div class="ltx_para" id="A2.I4.i2.p1">
        <p class="ltx_p" id="A2.I4.i2.p1.1">
         Head scarfs
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i3" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        3.
       </span>
       <div class="ltx_para" id="A2.I4.i3.p1">
        <p class="ltx_p" id="A2.I4.i3.p1.1">
         Detangling hair brushes
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i4" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        4.
       </span>
       <div class="ltx_para" id="A2.I4.i4.p1">
        <p class="ltx_p" id="A2.I4.i4.p1.1">
         Hair wigs
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i5" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        5.
       </span>
       <div class="ltx_para" id="A2.I4.i5.p1">
        <p class="ltx_p" id="A2.I4.i5.p1.1">
         Straight-leg jeans
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i6" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        6.
       </span>
       <div class="ltx_para" id="A2.I4.i6.p1">
        <p class="ltx_p" id="A2.I4.i6.p1.1">
         Shapewear
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i7" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        7.
       </span>
       <div class="ltx_para" id="A2.I4.i7.p1">
        <p class="ltx_p" id="A2.I4.i7.p1.1">
         Athleisure
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i8" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        8.
       </span>
       <div class="ltx_para" id="A2.I4.i8.p1">
        <p class="ltx_p" id="A2.I4.i8.p1.1">
         Abdominal crunch straps
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i9" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        9.
       </span>
       <div class="ltx_para" id="A2.I4.i9.p1">
        <p class="ltx_p" id="A2.I4.i9.p1.1">
         Swimsuits
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i10" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        10.
       </span>
       <div class="ltx_para" id="A2.I4.i10.p1">
        <p class="ltx_p" id="A2.I4.i10.p1.1">
         Power tool accessories
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i11" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        11.
       </span>
       <div class="ltx_para" id="A2.I4.i11.p1">
        <p class="ltx_p" id="A2.I4.i11.p1.1">
         Dog toys
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i12" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        12.
       </span>
       <div class="ltx_para" id="A2.I4.i12.p1">
        <p class="ltx_p" id="A2.I4.i12.p1.1">
         Shot glasses
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i13" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        13.
       </span>
       <div class="ltx_para" id="A2.I4.i13.p1">
        <p class="ltx_p" id="A2.I4.i13.p1.1">
         Costume shoes
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i14" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        14.
       </span>
       <div class="ltx_para" id="A2.I4.i14.p1">
        <p class="ltx_p" id="A2.I4.i14.p1.1">
         Slipcovers
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i15" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        15.
       </span>
       <div class="ltx_para" id="A2.I4.i15.p1">
        <p class="ltx_p" id="A2.I4.i15.p1.1">
         Wall plates and covers
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i16" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        16.
       </span>
       <div class="ltx_para" id="A2.I4.i16.p1">
        <p class="ltx_p" id="A2.I4.i16.p1.1">
         Baby shirts
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i17" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        17.
       </span>
       <div class="ltx_para" id="A2.I4.i17.p1">
        <p class="ltx_p" id="A2.I4.i17.p1.1">
         Curtain and drape rings
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i18" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        18.
       </span>
       <div class="ltx_para" id="A2.I4.i18.p1">
        <p class="ltx_p" id="A2.I4.i18.p1.1">
         Cabinet knobs and handles
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i19" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        19.
       </span>
       <div class="ltx_para" id="A2.I4.i19.p1">
        <p class="ltx_p" id="A2.I4.i19.p1.1">
         Watch accessories
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i20" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        20.
       </span>
       <div class="ltx_para" id="A2.I4.i20.p1">
        <p class="ltx_p" id="A2.I4.i20.p1.1">
         Tumblers
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i21" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        21.
       </span>
       <div class="ltx_para" id="A2.I4.i21.p1">
        <p class="ltx_p" id="A2.I4.i21.p1.1">
         Model train sets
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i22" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        22.
       </span>
       <div class="ltx_para" id="A2.I4.i22.p1">
        <p class="ltx_p" id="A2.I4.i22.p1.1">
         Signage
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i23" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        23.
       </span>
       <div class="ltx_para" id="A2.I4.i23.p1">
        <p class="ltx_p" id="A2.I4.i23.p1.1">
         Wedding dresses
        </p>
       </div>
      </li>
      <li class="ltx_item" id="A2.I4.i24" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        24.
       </span>
       <div class="ltx_para ltx_noindent" id="A2.I4.i24.p1">
        <p class="ltx_p" id="A2.I4.i24.p1.1">
         Table runners
        </p>
       </div>
      </li>
     </ol>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px3.p8">
     <p class="ltx_p" id="A2.SS2.SSS0.Px3.p8.1">
      Please note that the popularity of these items can vary based on the region and demographic of the consumers. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px3.p8.1.1" style="color:#008080;">
       music âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px3.p8.1.2" style="color:#008080;">
       movie âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px3.p8.1.3" style="color:#008080;">
       books âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px3.p8.1.4" style="color:#008080;">
       products âœ”
      </span>
      )
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="A2.SS2.SSS0.Px4">
    <h4 class="ltx_title ltx_title_paragraph">
     Instruction 4
    </h4>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px4.p1">
     <p class="ltx_p" id="A2.SS2.SSS0.Px4.p1.1">
      Iâ€™m planning a two-week vacation to Japan next month. Help me plan my itinerary. I want to visit Tokyo, Kyoto, and Osaka. Look for the top tourist attractions in each city, and also suggest the best mode of travel between these cities. Additionally, find out the weather forecast for the month Iâ€™ll be visiting.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px4.p2">
     <p class="ltx_p" id="A2.SS2.SSS0.Px4.p2.1">
      <span class="ltx_text" id="A2.SS2.SSS0.Px4.p2.1.1" style="color:#0000FF;">
       Single ReAct Agent
      </span>
      : The top tourist attractions in Tokyo are the Tokyo Skytree, Senso-ji Temple, Meiji Shrine, Tokyo Disneyland, Ueno Park, and Odaiba. In Kyoto, the top attractions are Nijo Castle, Fushimi-Inari Taisha Shrine, Kinkaku-ji: The Golden Pavilion, Kiyomizu-dera Temple, SanjÅ«sangen-dÅ Temple, and Kyoto Imperial Palace. In Osaka, tourists can visit Osaka Castle, ShitennÅ-ji Temple, Osaka Aquarium Kaiyukan, Tempozan Ferris Wheel and Harbor Village, and Universal Studios Japan. These attractions offer a mix of historical, cultural, and modern experiences for visitors. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px4.p2.1.2" style="color:#008080;">
       tourist attractions âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px4.p2.1.3" style="color:#FF0000;">
       best mode of travel between cities âœ–
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px4.p2.1.4" style="color:#FF0000;">
       weather forecast âœ–
      </span>
      )
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px4.p3">
     <p class="ltx_p" id="A2.SS2.SSS0.Px4.p3.1">
      <span class="ltx_text ltx_font_smallcaps" id="A2.SS2.SSS0.Px4.p3.1.1" style="color:#FF00FF;">
       AgentVerse
       <span class="ltx_text ltx_font_upright" id="A2.SS2.SSS0.Px4.p3.1.1.1">
        (3 agents)
       </span>
      </span>
      : Your two-week vacation to Japan is shaping up to be quite exciting. In Tokyo, you can visit the Tokyo Skytree, Senso-ji Temple, Meiji Shrine, Tokyo Disneyland and DisneySea, and the Odaiba area. In Kyoto, the top attractions are Nijo Castle, Fushimi-Inari Taisha Shrine, Kinkaku-ji (Golden Pavilion), Kiyomizu-dera Temple, and SanjÅ«sangen-dÅ Temple. While in Osaka, you should visit Osaka Castle, ShitennÅ-ji Temple, Osaka Aquarium Kaiyukan, and Universal Studios Japan.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px4.p4">
     <p class="ltx_p" id="A2.SS2.SSS0.Px4.p4.1">
      As for transportation, the best way to travel between these cities is by train, specifically the Shinkansen bullet train. You can also consider flights with ANA or JAL, or if youâ€™re on a budget, the night bus or LCC flights by Jetstar and Peach.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px4.p5">
     <p class="ltx_p" id="A2.SS2.SSS0.Px4.p5.1">
      Regarding the weather, itâ€™s difficult to provide an exact forecast for the next month, but weâ€™ve gathered the average weather data for these cities based on previous years. Please remember to check the weather forecast closer to your departure date for a more accurate prediction. Enjoy your trip! (
      <span class="ltx_text" id="A2.SS2.SSS0.Px4.p5.1.1" style="color:#008080;">
       tourist attractions âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px4.p5.1.2" style="color:#008080;">
       best mode of travel between cities âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px4.p5.1.3" style="color:#008080;">
       weather forecast âœ”
      </span>
      )
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="A2.SS2.SSS0.Px5">
    <h4 class="ltx_title ltx_title_paragraph">
     Instruction 5
    </h4>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px5.p1">
     <p class="ltx_p" id="A2.SS2.SSS0.Px5.p1.1">
      I am planning a date with my girlfriend this week, please search for a good movie theater and a restaurant near Tsinghua University in Beijing and recommend a good movie to watch. Please search the web.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px5.p2">
     <p class="ltx_p" id="A2.SS2.SSS0.Px5.p2.1">
      <span class="ltx_text" id="A2.SS2.SSS0.Px5.p2.1.1" style="color:#0000FF;">
       Single ReAct Agent
      </span>
      : Your date near Tsinghua University in Beijing is all set. For dining, you can choose from Lan Yang ShuFang, YiLan ShaoKao, Guan Shi JiChi (Zhongguancun), or Dong Lai Shun (Zhongguancun HaiTu). As for the movie, there are several highly-rated ones showing in 2023. Some of these include M3GAN, Plane, Missing, Infinity Pool, John Wick: Chapter 4, Creed III, Scream VI, Air, Suzume, Rise, Are You There God? Itâ€™s Me, Margaret, Showing Up, and The Covenant. Enjoy your date! (
      <span class="ltx_text" id="A2.SS2.SSS0.Px5.p2.1.2" style="color:#008080;">
       restaurant âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px5.p2.1.3" style="color:#008080;">
       movie âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px5.p2.1.4" style="color:#FF0000;">
       theater âœ–
      </span>
      )
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px5.p3">
     <p class="ltx_p" id="A2.SS2.SSS0.Px5.p3.1">
      <span class="ltx_text ltx_font_smallcaps" id="A2.SS2.SSS0.Px5.p3.1.1" style="color:#FF00FF;">
       AgentVerse
       <span class="ltx_text ltx_font_upright" id="A2.SS2.SSS0.Px5.p3.1.1.1">
        (3 agents)
       </span>
      </span>
      : Your date is all set! For dining, we recommend either Jiasuo Restaurant or Zijingyuan, both located conveniently on the Tsinghua University campus. For your movie, we suggest â€™No Bearsâ€™, which is currently popular and highly rated. You can watch it at the Maple Garden Drive-In Cinema, one of the top-rated movie theaters near Tsinghua University. Enjoy your date! (
      <span class="ltx_text" id="A2.SS2.SSS0.Px5.p3.1.2" style="color:#008080;">
       restaurant âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px5.p3.1.3" style="color:#008080;">
       movie âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px5.p3.1.4" style="color:#008080;">
       theater âœ”
      </span>
      )
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="A2.SS2.SSS0.Px6">
    <h4 class="ltx_title ltx_title_paragraph">
     Instruction 6
    </h4>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px6.p1">
     <p class="ltx_p" id="A2.SS2.SSS0.Px6.p1.1">
      I want to kick off a book club with my friends. Can you tell me the top 5 bestselling books this month, gather the content summary for each, and find online platforms where we can buy or borrow them?
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px6.p2">
     <p class="ltx_p" id="A2.SS2.SSS0.Px6.p2.1">
      <span class="ltx_text" id="A2.SS2.SSS0.Px6.p2.1.1" style="color:#0000FF;">
       Single ReAct Agent
      </span>
      : I was unable to find the top 5 bestselling books for October 2023. The websites I visited did not provide the required information. Therefore, I am unable to provide the content summary for each of the top 5 bestselling books and find online platforms where they can be bought or borrowed. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px6.p2.1.2" style="color:#FF0000;">
       âœ–
      </span>
      )
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px6.p3">
     <p class="ltx_p" id="A2.SS2.SSS0.Px6.p3.1">
      <span class="ltx_text ltx_font_smallcaps" id="A2.SS2.SSS0.Px6.p3.1.1" style="color:#FF00FF;">
       AgentVerse
       <span class="ltx_text ltx_font_upright" id="A2.SS2.SSS0.Px6.p3.1.1.1">
        (2 agents)
       </span>
      </span>
      :The top 5 bestsellers this month can be bought or borrowed from several online platforms. These include OverDrive, which allows you to borrow books for free through your local library or school. Other platforms include the Amazon Kindle Store, Barnes &amp; Nobleâ€™s Nook Book Store, Fictionwise, eBooks.com, eReader.com, Google Play Book Store, Powellâ€™s Books, as well as eBook lending and borrowing services like eBookFling, Lendle, and BookLending.com. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px6.p3.1.2" style="color:#008080;">
       online platforms âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px6.p3.1.3" style="color:#FF0000;">
       top 5 bestselling books âœ–
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px6.p3.1.4" style="color:#FF0000;">
       content summary âœ–
      </span>
      )
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="A2.SS2.SSS0.Px7">
    <h4 class="ltx_title ltx_title_paragraph">
     Instruction 7
    </h4>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px7.p1">
     <p class="ltx_p" id="A2.SS2.SSS0.Px7.p1.1">
      I am planning to buy a new car. Could you help me compare the features and prices of the latest models of Tesla, Ford, and Toyota? Include details about range, charging time, safety features, and after-sales service. Also, provide a brief analysis of the pros and cons of each car.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px7.p2">
     <p class="ltx_p" id="A2.SS2.SSS0.Px7.p2.1">
      <span class="ltx_text" id="A2.SS2.SSS0.Px7.p2.1.1" style="color:#0000FF;">
       Single ReAct Agent
      </span>
      : The 2023 Tesla Model 3 starts at $41,880 and goes up to $54,880. It comes with standard features such as heated front seats, navigation, and Teslaâ€™s Autopilot semi-autonomous driving system. In terms of safety features, the Model 3 includes standard automated emergency braking, lane-departure warning, and adaptive cruise control. Teslaâ€™s after-sales service is characterized by a vertically integrated format that eliminates third-party intermediaries and relies on over-the-air (OTA) update capabilities and a mobile service fleet. The companyâ€™s OTA update capabilities will open up new subscription-based aftersales revenue streams, while its closed-loop system allows access to vehicle data for driving-related feedback loops and predictive maintenance. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px7.p2.1.2" style="color:#008080;">
       Tesla âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px7.p2.1.3" style="color:#FF0000;">
       Other brands âœ–
      </span>
      )
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px7.p3">
     <p class="ltx_p" id="A2.SS2.SSS0.Px7.p3.1">
      <span class="ltx_text ltx_font_smallcaps" id="A2.SS2.SSS0.Px7.p3.1.1" style="color:#FF00FF;">
       AgentVerse
       <span class="ltx_text ltx_font_upright" id="A2.SS2.SSS0.Px7.p3.1.1.1">
        (3 agents)
       </span>
      </span>
      : The starting price for the 2023 Ford Mustang Mach-E is around $43,895 and it takes approximately 38 hours to fully charge using a 120V outlet, 14 hours using a 240V outlet, and around 45 minutes to 80% capacity with DC fast charging. The starting price for the 2023 Toyota bZ4X is approximately $42,000. The charging time varies depending on the method: 52 hours for Level 1 charging, 26 hours or 18.75 hours for Level 2 charging, and 30-45 minutes to 80% capacity for Level 3 DC fast charging. Unfortunately, the price for the 2023 Tesla Model 3 was not found, but the charging time can be as quick as 15 minutes with a 250 kW Tesla Supercharger. At home, using a standard 120-volt outlet takes around three to four days for a full charge, while a 240-volt outlet with a NEMA 14-50 adapter reduces charging time to 10 or 11 hours. A Tesla Wall Connector can fully charge the car in about five hours.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px7.p4">
     <p class="ltx_p" id="A2.SS2.SSS0.Px7.p4.1">
      In terms of safety, all three models come equipped with a variety of features such as anti-lock brakes, stability control, front-impact airbags, side impact airbags, overhead airbags, knee airbags, pretensioners, and a security system. The Tesla Model 3 also includes autopilot and collision warning features.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px7.p5">
     <p class="ltx_p" id="A2.SS2.SSS0.Px7.p5.1">
      Regarding after-sales service, Tesla offers a comprehensive warranty and roadside assistance with service centers across the globe and mobile service. Toyotaâ€™s after-sales service is known for being top-notch and continually improving with expert technicians and quality genuine parts. However, Fordâ€™s after-sales service has received negative reviews, with customers reporting poor customer support and inefficient handling of issues.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px7.p6">
     <p class="ltx_p" id="A2.SS2.SSS0.Px7.p6.1">
      Please consider these factors when making your decision. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px7.p6.1.1" style="color:#008080;">
       Prices âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px7.p6.1.2" style="color:#008080;">
       charging time
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px7.p6.1.3" style="color:#008080;">
       safety features
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px7.p6.1.4" style="color:#008080;">
       after-sales service
      </span>
      )
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="A2.SS2.SSS0.Px8">
    <h4 class="ltx_title ltx_title_paragraph">
     Instruction 8
    </h4>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px8.p1">
     <p class="ltx_p" id="A2.SS2.SSS0.Px8.p1.1">
      Iâ€™ve recently taken an interest in DIY home projects. Search for beginner-friendly DIY projects that can be completed over the weekend. Also, provide a list of materials required and a step-by-step guide for each project.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px8.p2">
     <p class="ltx_p" id="A2.SS2.SSS0.Px8.p2.1">
      <span class="ltx_text" id="A2.SS2.SSS0.Px8.p2.1.1" style="color:#0000FF;">
       Single ReAct Agent
      </span>
      : I found a variety of beginner-friendly DIY home projects from three different websites. These projects include painting a room, replacing cabinet pulls, repairing torn window screens, fixing interior doors, balancing furnace dampers, repairing leaky faucets, refreshing caulk in the bathroom, building garage storage shelves, constructing a fire pit, hanging a ceiling fan, installing a wall-mounted television, and cleaning out sink traps. These projects are suitable for beginners and can help improve DIY skills while boosting confidence. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px8.p2.1.2" style="color:#008080;">
       DIY projects âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px8.p2.1.3" style="color:#FF0000;">
       list of materials âœ–
      </span>
      )
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px8.p3">
     <p class="ltx_p" id="A2.SS2.SSS0.Px8.p3.1">
      <span class="ltx_text ltx_font_smallcaps" id="A2.SS2.SSS0.Px8.p3.1.1" style="color:#FF00FF;">
       AgentVerse
       <span class="ltx_text ltx_font_upright" id="A2.SS2.SSS0.Px8.p3.1.1.1">
        (2 agents)
       </span>
      </span>
      : The team has successfully found a variety of beginner-friendly DIY home projects that can be completed over the weekend. These projects range from painting a room, replacing cabinet pulls, installing a wall-mounted television, building a fire pit, to more complex tasks like kitchen remodels and furniture updates. The materials required for these projects are basic and easy to source, including tools like a roller and pan for painting, a cordless drill for replacing cabinet pulls, a stud finder for installing a wall-mounted television, and concrete work tools for building a fire pit. Detailed, step-by-step guides for each project have been provided to ensure a smooth DIY experience. Happy DIY-ing! (
      <span class="ltx_text" id="A2.SS2.SSS0.Px8.p3.1.2" style="color:#008080;">
       DIY projects âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px8.p3.1.3" style="color:#008080;">
       list of materials âœ”
      </span>
      )
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="A2.SS2.SSS0.Px9">
    <h4 class="ltx_title ltx_title_paragraph">
     Instruction 9
    </h4>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px9.p1">
     <p class="ltx_p" id="A2.SS2.SSS0.Px9.p1.1">
      Winter is coming, and I want to update my wardrobe. Search for the latest winter fashion trends for men and women. Also, suggest some online stores where I can buy trendy winter clothing.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px9.p2">
     <p class="ltx_p" id="A2.SS2.SSS0.Px9.p2.1">
      <span class="ltx_text" id="A2.SS2.SSS0.Px9.p2.1.1" style="color:#0000FF;">
       Single ReAct Agent
      </span>
      : The latest winter fashion trends for both men and women this year include suiting, bold shoulders, western touches, maxis, electric hues, bombers, mismatched maximalism, corseting, cut-outs, Y2K fashion, and chunky boots. You can purchase these trendy winter clothes at online stores like PrettyLittleThing and Windsor. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px9.p2.1.2" style="color:#008080;">
       fashion trends âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px9.p2.1.3" style="color:#008080;">
       online stores âœ”
      </span>
      )
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px9.p3">
     <p class="ltx_p" id="A2.SS2.SSS0.Px9.p3.1">
      <span class="ltx_text ltx_font_smallcaps" id="A2.SS2.SSS0.Px9.p3.1.1" style="color:#FF00FF;">
       AgentVerse
       <span class="ltx_text ltx_font_upright" id="A2.SS2.SSS0.Px9.p3.1.1.1">
        (3 agents)
       </span>
      </span>
      : The latest winter fashion trends for men include a reinvention of classic staples, misery chic, pops of pink, throwback jackets, showstopping trousers, and Mac coats. For women, the trends include corseting, cowboy boots, Y2K styles, oversized outerwear, pleated skirts, and balaclavas. You can shop for these trendy winter clothes at popular online stores such as Uniqlo, Everlane, and Target. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px9.p3.1.2" style="color:#008080;">
       fashion trends âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px9.p3.1.3" style="color:#008080;">
       online stores âœ”
      </span>
      )
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="A2.SS2.SSS0.Px10">
    <h4 class="ltx_title ltx_title_paragraph">
     Instruction 10
    </h4>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px10.p1">
     <p class="ltx_p" id="A2.SS2.SSS0.Px10.p1.1">
      I want to lose 5kg in the next 2 months. I weigh 70kg, am 170cm tall, and my age is 25. Calculate my BMI and based on that, suggest a workout routine and daily calorie intake to help me achieve my goal.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px10.p2">
     <p class="ltx_p" id="A2.SS2.SSS0.Px10.p2.1">
      <span class="ltx_text" id="A2.SS2.SSS0.Px10.p2.1.1" style="color:#0000FF;">
       Single ReAct Agent
      </span>
      : Your BMI is 24.22, which is within the normal range. To lose 5kg in 2 months, you should aim for a daily calorie deficit of 500-1000 calories. This can be achieved through a combination of a balanced diet and regular exercise. Your diet should focus on whole foods, lean proteins, fruits, and vegetables. For exercise, a combination of cardio and strength training is recommended. Cardio exercises like running, cycling, swimming can help burn calories, while strength training exercises like weight lifting can help build muscle, which can increase your metabolic rate and aid in weight loss. Donâ€™t forget to get enough sleep and stay hydrated. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px10.p2.1.2" style="color:#008080;">
       bmi âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px10.p2.1.3" style="color:#008080;">
       workout routine âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px10.p2.1.4" style="color:#008080;">
       daily calorie intake âœ”
      </span>
      )
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="A2.SS2.SSS0.Px10.p3">
     <p class="ltx_p" id="A2.SS2.SSS0.Px10.p3.1">
      <span class="ltx_text ltx_font_smallcaps" id="A2.SS2.SSS0.Px10.p3.1.1" style="color:#FF00FF;">
       AgentVerse
       <span class="ltx_text ltx_font_upright" id="A2.SS2.SSS0.Px10.p3.1.1.1">
        (2 agents)
       </span>
      </span>
      : Your Body Mass Index (BMI) is approximately 24.22, which is within the healthy weight range. To achieve your weight loss goal, you should aim to consume between 1400 and 1600 calories per day. This will create a calorie deficit necessary for weight loss. In terms of physical activity, you should start with a 4-week program that includes upper and lower body strength training, interval training/conditioning, and low intensity cardio exercises. After completing this, transition to a 12-week program that includes cardio, strength training, and flexibility workouts. Each workout should take approximately 20-22 minutes. Remember to maintain healthy habits such as eating a nutritious diet, staying hydrated, getting enough sleep, and managing stress. (
      <span class="ltx_text" id="A2.SS2.SSS0.Px10.p3.1.2" style="color:#008080;">
       bmi âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px10.p3.1.3" style="color:#008080;">
       workout routine âœ”
      </span>
      ,
      <span class="ltx_text" id="A2.SS2.SSS0.Px10.p3.1.4" style="color:#008080;">
       daily calorie intake âœ”
      </span>
      )
     </p>
    </div>
   </section>
  </section>
 </section>
 <section class="ltx_appendix" id="A3">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix C
   </span>
   Details of the Experiments on Minecraft
  </h2>
  <div class="ltx_para ltx_noindent" id="A3.p1">
   <p class="ltx_p" id="A3.p1.1">
    In this section, we explain some implementation details of the experiments that we conduct on Minecraft (
    <a class="ltx_ref" href="#S4" title="4 Emergent Behaviors within a Multi-agent Group â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      Section
     </span>
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    ).
   </p>
  </div>
  <section class="ltx_paragraph" id="A3.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Expert Recruitment
   </h4>
   <div class="ltx_para ltx_noindent" id="A3.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="A3.SS0.SSS0.Px1.p1.1">
     As noted in
     <a class="ltx_ref" href="#S4" title="4 Emergent Behaviors within a Multi-agent Group â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
      <span class="ltx_text ltx_ref_tag">
       Section
      </span>
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     , real-world gaming scenarios requires intricate communication and coordination across multiple rounds, there is often a consistent set of team members. Therefore when using
     <span class="ltx_text ltx_font_smallcaps" id="A3.SS0.SSS0.Px1.p1.1.1">
      AgentVerse
     </span>
     to simulate the game playing, we bypass the automated expert recruitment stage, and manually assign each agent as
     <span class="ltx_text ltx_font_italic" id="A3.SS0.SSS0.Px1.p1.1.2">
      â€an experienced Minecraft playerâ€
     </span>
     .
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="A3.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    Collaborative Decision-Making
   </h4>
   <div class="ltx_para ltx_noindent" id="A3.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="A3.SS0.SSS0.Px2.p1.1">
     For multi-player gameplay, the horizontal communication paradigm is favored. It lends itself to an environment where each agent independently formulates plans, diverging from traditional benchmark tasks which demand a singular solution. Agents are set to communicate in a predetermined sequential order, continuing until consensus is perceived. We let the agent to append a special token â€[END]â€ at the end of its response if it finds that the group have reached consensus on the task assignment.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="A3.SS0.SSS0.Px2.p2">
    <p class="ltx_p" id="A3.SS0.SSS0.Px2.p2.1">
     Subsequent to achieving consensus, an auxiliary agent is tasked to deduce the specific assignment for each agent from the entire communication record. This distilled information is then given as the input to the Voyager agent to inform it the assigned task.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="A3.SS0.SSS0.Px3">
   <h4 class="ltx_title ltx_title_paragraph">
    Action Execution
   </h4>
   <div class="ltx_para ltx_noindent" id="A3.SS0.SSS0.Px3.p1">
    <p class="ltx_p" id="A3.SS0.SSS0.Px3.p1.1">
     We instantiate several Voyager agents within a shared Minecraft environment. A brief introduction of the Voyager agent is provided here, and we refer the interested readers to
     <cite class="ltx_cite ltx_citemacro_citet">
      Wang etÂ al. (
      <a class="ltx_ref" href="#bib.bib44" title="">
       2023a
      </a>
      )
     </cite>
     for a more detailed exposition.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="A3.SS0.SSS0.Px3.p2">
    <p class="ltx_p" id="A3.SS0.SSS0.Px3.p2.1">
     A Voyager agent is adept at navigating Minecraft. On receiving a task, it first decomposes it into a set of manageable sub-tasks. For instance, if assigned the task â€Kill 3 cowsâ€, the agent might decompose it into sequential sub-goals like: [punch 2 trees, Craft 4 wooden planks, Craft 1 stick, Craft 1 crafting table, Craft 1 wooden sword, Kill 3 cows]. The agent then sequentially attempt to complete each sub-task.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="A3.SS0.SSS0.Px3.p3">
    <p class="ltx_p" id="A3.SS0.SSS0.Px3.p3.1">
     We employ the checkpoint available in the official repository
     <span class="ltx_note ltx_role_footnote" id="footnote2">
      <sup class="ltx_note_mark">
       2
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         2
        </sup>
        <span class="ltx_tag ltx_tag_note">
         2
        </span>
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/MineDojo/Voyager/tree/main/skill_library/trial1/skill" target="_blank" title="">
         https://github.com/MineDojo/Voyager/tree/main/skill_library/trial1/skill
        </a>
       </span>
      </span>
     </span>
     , and use
     <span class="ltx_text ltx_font_typewriter" id="A3.SS0.SSS0.Px3.p3.1.1">
      GPT-4-0314
     </span>
     as the backbone LLM for Voyager agent to be consistent with
     <cite class="ltx_cite ltx_citemacro_citet">
      Wang etÂ al. (
      <a class="ltx_ref" href="#bib.bib44" title="">
       2023a
      </a>
      )
     </cite>
     . Once an agent accomplish its own task, or all agents hit the cap of five attempts, the task execution stage terminates and evaluation stage starts.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="A3.SS0.SSS0.Px4">
   <h4 class="ltx_title ltx_title_paragraph">
    Evaluation
   </h4>
   <div class="ltx_para ltx_noindent" id="A3.SS0.SSS0.Px4.p1">
    <p class="ltx_p" id="A3.SS0.SSS0.Px4.p1.1">
     We directly exploit the inventory and the completed or failed sub-tasks of each agent as the feedback.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A4">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix D
   </span>
   Prompts
  </h2>
  <div class="ltx_para ltx_noindent" id="A4.p1">
   <p class="ltx_p" id="A4.p1.1">
    We list the prompts used in
    <a class="ltx_ref" href="#S3" title="3 Experiments â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      Section
     </span>
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    at
    <a class="ltx_ref" href="#A5.F7" title="In Appendix E Limitation and Future Work â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      Figures
     </span>
     <span class="ltx_text ltx_ref_tag">
      7
     </span>
    </a>
    ,
    <a class="ltx_ref" href="#A5.F8" title="Figure 8 â€£ Appendix E Limitation and Future Work â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      8
     </span>
    </a>
    ,
    <a class="ltx_ref" href="#A5.F9" title="Figure 9 â€£ Appendix E Limitation and Future Work â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      9
     </span>
    </a>
    ,
    <a class="ltx_ref" href="#A5.F10" title="Figure 10 â€£ Appendix E Limitation and Future Work â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      10
     </span>
    </a>
    and
    <a class="ltx_ref" href="#A5.F11" title="Figure 11 â€£ Appendix E Limitation and Future Work â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      11
     </span>
    </a>
    .
   </p>
   <ul class="ltx_itemize" id="A4.I1">
    <li class="ltx_item" id="A4.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      â€¢
     </span>
     <div class="ltx_para" id="A4.I1.i1.p1">
      <p class="ltx_p" id="A4.I1.i1.p1.1">
       <span class="ltx_text ltx_font_bold" id="A4.I1.i1.p1.1.1">
        FED
       </span>
       :
       <a class="ltx_ref" href="#A5.F7" title="In Appendix E Limitation and Future Work â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
        <span class="ltx_text ltx_ref_tag">
         Figure
        </span>
        <span class="ltx_text ltx_ref_tag">
         7
        </span>
       </a>
      </p>
     </div>
    </li>
    <li class="ltx_item" id="A4.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      â€¢
     </span>
     <div class="ltx_para" id="A4.I1.i2.p1">
      <p class="ltx_p" id="A4.I1.i2.p1.1">
       <span class="ltx_text ltx_font_bold" id="A4.I1.i2.p1.1.1">
        MGSM
       </span>
       :
       <a class="ltx_ref" href="#A5.F8" title="In Appendix E Limitation and Future Work â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
        <span class="ltx_text ltx_ref_tag">
         Figure
        </span>
        <span class="ltx_text ltx_ref_tag">
         8
        </span>
       </a>
      </p>
     </div>
    </li>
    <li class="ltx_item" id="A4.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      â€¢
     </span>
     <div class="ltx_para" id="A4.I1.i3.p1">
      <p class="ltx_p" id="A4.I1.i3.p1.1">
       <span class="ltx_text ltx_font_bold" id="A4.I1.i3.p1.1.1">
        Humaneval
       </span>
       :
       <a class="ltx_ref" href="#A5.F9" title="In Appendix E Limitation and Future Work â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
        <span class="ltx_text ltx_ref_tag">
         Figure
        </span>
        <span class="ltx_text ltx_ref_tag">
         9
        </span>
       </a>
      </p>
     </div>
    </li>
    <li class="ltx_item" id="A4.I1.i4" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      â€¢
     </span>
     <div class="ltx_para" id="A4.I1.i4.p1">
      <p class="ltx_p" id="A4.I1.i4.p1.1">
       <span class="ltx_text ltx_font_bold" id="A4.I1.i4.p1.1.1">
        Commongen-Challenge
       </span>
       :
       <a class="ltx_ref" href="#A5.F10" title="In Appendix E Limitation and Future Work â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
        <span class="ltx_text ltx_ref_tag">
         Figure
        </span>
        <span class="ltx_text ltx_ref_tag">
         10
        </span>
       </a>
      </p>
     </div>
    </li>
    <li class="ltx_item" id="A4.I1.i5" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      â€¢
     </span>
     <div class="ltx_para ltx_noindent" id="A4.I1.i5.p1">
      <p class="ltx_p" id="A4.I1.i5.p1.1">
       <span class="ltx_text ltx_font_bold" id="A4.I1.i5.p1.1.1">
        Tool
       </span>
       :
       <a class="ltx_ref" href="#A5.F11" title="In Appendix E Limitation and Future Work â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
        <span class="ltx_text ltx_ref_tag">
         Figure
        </span>
        <span class="ltx_text ltx_ref_tag">
         11
        </span>
       </a>
      </p>
     </div>
    </li>
   </ul>
  </div>
 </section>
 <section class="ltx_appendix" id="A5">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix E
   </span>
   Limitation and Future Work
  </h2>
  <div class="ltx_para ltx_noindent" id="A5.p1">
   <p class="ltx_p" id="A5.p1.1">
    In this work, we introduce
    <span class="ltx_text ltx_font_smallcaps" id="A5.p1.1.1">
     AgentVerse
    </span>
    that facilitates multiple autonomous agents to simulate human groups to accomplish tasks, and discuss the emergent social behaviors of agents during this process.
    <span class="ltx_text ltx_font_smallcaps" id="A5.p1.1.2">
     AgentVerse
    </span>
    is an advanced attempt; thus, there are some techniques within
    <span class="ltx_text ltx_font_smallcaps" id="A5.p1.1.3">
     AgentVerse
    </span>
    that still have room for improvement and are worthy of exploration. In this section, we delve into these aspects for further illustration.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A5.p2">
   <p class="ltx_p" id="A5.p2.1">
    <span class="ltx_text ltx_font_bold" id="A5.p2.1.1">
     More Capable Agents and More Challenging Scenarios.
    </span>
    The
    <span class="ltx_text ltx_font_smallcaps" id="A5.p2.1.2">
     AgentVerse
    </span>
    is designed to enable various multiple LLM-based agents to collaboratively accomplish tasks. In the current research, we have utilized state-of-the-art agents based on GPT-4. With the advancements in LLMs, such as the newly released version of ChatGPT that incorporates voice and image capabilities
    <cite class="ltx_cite ltx_citemacro_citep">
     (OpenAI,
     <a class="ltx_ref" href="#bib.bib25" title="">
      2023b
     </a>
     )
    </cite>
    , LLM-based agents have more perceptual capabilities, including seeing, hearing, and speaking. These enhancements may increase the potential of agents and allow them to accomplish more complex real-world tasks based on the
    <span class="ltx_text ltx_font_smallcaps" id="A5.p2.1.3">
     AgentVerse
    </span>
    framework.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A5.p3">
   <p class="ltx_p" id="A5.p3.1">
    <span class="ltx_text ltx_font_bold" id="A5.p3.1.1">
     Multi-party Communication Among Agents.
    </span>
    The currently proposed autonomous agents
    <cite class="ltx_cite ltx_citemacro_citep">
     (Richards &amp; etÂ al.,
     <a class="ltx_ref" href="#bib.bib33" title="">
      2023
     </a>
     ; Nakajima,
     <a class="ltx_ref" href="#bib.bib22" title="">
      2023
     </a>
     ; Reworkd,
     <a class="ltx_ref" href="#bib.bib32" title="">
      2023
     </a>
     ; Wang etÂ al.,
     <a class="ltx_ref" href="#bib.bib44" title="">
      2023a
     </a>
     )
    </cite>
    LLMs possess excellent instruction comprehension capabilities
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wei etÂ al.,
     <a class="ltx_ref" href="#bib.bib46" title="">
      2022a
     </a>
     ; Stiennon etÂ al.,
     <a class="ltx_ref" href="#bib.bib40" title="">
      2020
     </a>
     )
    </cite>
    . This enables them to follow given human instructions and accomplish tasks within a one-on-one (human-to-AI) scenario. However, multi-agent collaboration involves a
    <span class="ltx_text ltx_font_italic" id="A5.p3.1.2">
     multi-party communication
    </span>
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wei etÂ al.,
     <a class="ltx_ref" href="#bib.bib48" title="">
      2023
     </a>
     )
    </cite>
    scenario that requires the capability to autonomously determine
    <span class="ltx_text ltx_font_italic" id="A5.p3.1.3">
     when to speak
    </span>
    and
    <span class="ltx_text ltx_font_italic" id="A5.p3.1.4">
     whom to speak
    </span>
    . This leads to difficulties in communication among the agents during the collaborative decision-making step within the
    <span class="ltx_text ltx_font_smallcaps" id="A5.p3.1.5">
     AgentVerse
    </span>
    framework. Hence, there are two directions worth exploring. Firstly, akin to the aforementioned, we can explore more effective mechanisms for managing agent communication. Additionally, we can design more advanced perceptual-aware LLMs
    <cite class="ltx_cite ltx_citemacro_citep">
     (OpenAI,
     <a class="ltx_ref" href="#bib.bib25" title="">
      2023b
     </a>
     )
    </cite>
    that can autonomously interact with their environments
    <span class="ltx_note ltx_role_footnote" id="footnote3">
     <sup class="ltx_note_mark">
      3
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        3
       </sup>
       <span class="ltx_tag ltx_tag_note">
        3
       </span>
       This kind of perceptual-aware agent has long been a goal of embodied AI
       <cite class="ltx_cite ltx_citemacro_citep">
        (Ahn etÂ al.,
        <a class="ltx_ref" href="#bib.bib1" title="">
         2022
        </a>
        ; Driess etÂ al.,
        <a class="ltx_ref" href="#bib.bib12" title="">
         2023
        </a>
        )
       </cite>
       , which is a promising direction to explore.
      </span>
     </span>
    </span>
    , including other agents.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A5.p4">
   <p class="ltx_p" id="A5.p4.1">
    <span class="ltx_text ltx_font_bold" id="A5.p4.1.1">
     Leverage Emergent Behaviors and Mitigate Safety Issues.
    </span>
    In Section
    <a class="ltx_ref" href="#S4" title="4 Emergent Behaviors within a Multi-agent Group â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    , we identified both emergent positive and harmful behaviors. Exploring ways to leverage positive behaviors for improving work efficiency and effectiveness, as well as mitigating harmful behaviors, are promising directions.
   </p>
  </div>
  <figure class="ltx_figure" id="A5.F7">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="711" id="A5.F7.g1" src="/html/2308.10848/assets/x9.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 7:
    </span>
    Prompt of FED dataset.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A5.F8">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="565" id="A5.F8.g1" src="/html/2308.10848/assets/x10.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 8:
    </span>
    Prompt for MGSM dataset.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A5.F9">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="596" id="A5.F9.g1" src="/html/2308.10848/assets/x11.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 9:
    </span>
    Prompt for Humaneval dataset.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A5.F10">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="577" id="A5.F10.g1" src="/html/2308.10848/assets/x12.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 10:
    </span>
    Prompt for Commongen-Challenge dataset.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A5.F11">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="547" id="A5.F11.g1" src="/html/2308.10848/assets/x13.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 11:
    </span>
    Prompt of Tool utilization.
   </figcaption>
  </figure>
 </section>
 <section class="ltx_appendix" id="A6">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix F
   </span>
   Examples of the Case Studies
  </h2>
  <div class="ltx_para ltx_noindent" id="A6.p1">
   <p class="ltx_p" id="A6.p1.1">
    In this section, we delve into specific examples to illustrate the experimental processes discussed in our paper. For each instance, we juxtapose the single-agent approach with the multi-agent method. Specifically:
   </p>
   <ul class="ltx_itemize" id="A6.I1">
    <li class="ltx_item" id="A6.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      â€¢
     </span>
     <div class="ltx_para" id="A6.I1.i1.p1">
      <p class="ltx_p" id="A6.I1.i1.p1.1">
       <span class="ltx_text ltx_font_bold" id="A6.I1.i1.p1.1.1">
        Software Development
       </span>
       :
       <a class="ltx_ref" href="#A6.F12" title="In Appendix F Examples of the Case Studies â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
        <span class="ltx_text ltx_ref_tag">
         Figure
        </span>
        <span class="ltx_text ltx_ref_tag">
         12
        </span>
       </a>
       depicts the process for developing a calculator.
       <a class="ltx_ref" href="#A6.F13" title="In Appendix F Examples of the Case Studies â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
        <span class="ltx_text ltx_ref_tag">
         Figures
        </span>
        <span class="ltx_text ltx_ref_tag">
         13
        </span>
       </a>
       and
       <a class="ltx_ref" href="#A6.F14" title="Figure 14 â€£ Appendix F Examples of the Case Studies â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
        <span class="ltx_text ltx_ref_tag">
         14
        </span>
       </a>
       show the code generated by single agent and multi-agent group respectively.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="A6.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      â€¢
     </span>
     <div class="ltx_para" id="A6.I1.i2.p1">
      <p class="ltx_p" id="A6.I1.i2.p1.1">
       <span class="ltx_text ltx_font_bold" id="A6.I1.i2.p1.1.1">
        Consulting in Horizontal Structure
       </span>
       : For consulting, we present single-agent and multi-agent approaches using horizontal structure. These can be seen in
       <a class="ltx_ref" href="#A6.F15" title="In Appendix F Examples of the Case Studies â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
        <span class="ltx_text ltx_ref_tag">
         Figures
        </span>
        <span class="ltx_text ltx_ref_tag">
         15
        </span>
       </a>
       and
       <a class="ltx_ref" href="#A6.F17" title="Figure 17 â€£ Appendix F Examples of the Case Studies â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
        <span class="ltx_text ltx_ref_tag">
         17
        </span>
       </a>
       .
      </p>
     </div>
    </li>
    <li class="ltx_item" id="A6.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      â€¢
     </span>
     <div class="ltx_para" id="A6.I1.i3.p1">
      <p class="ltx_p" id="A6.I1.i3.p1.1">
       <span class="ltx_text ltx_font_bold" id="A6.I1.i3.p1.1.1">
        Consulting in Vertical Structure
       </span>
       Similarly,
       <a class="ltx_ref" href="#A6.F18" title="In Appendix F Examples of the Case Studies â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
        <span class="ltx_text ltx_ref_tag">
         Figures
        </span>
        <span class="ltx_text ltx_ref_tag">
         18
        </span>
       </a>
       and
       <a class="ltx_ref" href="#A6.F20" title="Figure 20 â€£ Appendix F Examples of the Case Studies â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
        <span class="ltx_text ltx_ref_tag">
         20
        </span>
       </a>
       showcase single-agent and multi-agent project consulting, but employing a vertical structure structure for multi-agent.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="A6.I1.i4" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      â€¢
     </span>
     <div class="ltx_para" id="A6.I1.i4.p1">
      <p class="ltx_p" id="A6.I1.i4.p1.1">
       <span class="ltx_text ltx_font_bold" id="A6.I1.i4.p1.1.1">
        Tool Utilization
       </span>
       :
       <a class="ltx_ref" href="#A6.F21" title="In Appendix F Examples of the Case Studies â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
        <span class="ltx_text ltx_ref_tag">
         Figure
        </span>
        <span class="ltx_text ltx_ref_tag">
         21
        </span>
       </a>
       presents how two agents effectively decompose the given query into different sub-tasks, and use different tools to collaboratively resolve the query.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="A6.I1.i5" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      â€¢
     </span>
     <div class="ltx_para ltx_noindent" id="A6.I1.i5.p1">
      <p class="ltx_p" id="A6.I1.i5.p1.1">
       <span class="ltx_text ltx_font_bold" id="A6.I1.i5.p1.1.1">
        Minecraft
       </span>
       : Lastly,
       <a class="ltx_ref" href="#A6.F22" title="In Appendix F Examples of the Case Studies â€£ AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors">
        <span class="ltx_text ltx_ref_tag">
         Figure
        </span>
        <span class="ltx_text ltx_ref_tag">
         22
        </span>
       </a>
       provides an insight into a process where three agents collaborate to craft a bookshelf in Minecraft.
      </p>
     </div>
    </li>
   </ul>
  </div>
  <figure class="ltx_figure" id="A6.F12">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="480" id="A6.F12.g1" src="/html/2308.10848/assets/x14.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 12:
    </span>
    An example of the process of software development.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A6.F13">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="487" id="A6.F13.g1" src="/html/2308.10848/assets/x15.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 13:
    </span>
    The code generated by single agent.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A6.F14">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="687" id="A6.F14.g1" src="/html/2308.10848/assets/x16.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 14:
    </span>
    The code generated by the multi-agent group.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A6.F15">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="673" id="A6.F15.g1" src="/html/2308.10848/assets/x17.png" width="392"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 15:
    </span>
    (Page 1) An example process of project consulting with Group setup in horizontal decision-making structure. The agents are providing suggestions on the problem â€Give me some suggestions if I want to build a compressed hydrogen storage station in Ohioâ€
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A6.F16">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="661" id="A6.F16.g1" src="/html/2308.10848/assets/x18.png" width="392"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 16:
    </span>
    (Page 2) An example process of project consulting with Group setup in horizontal decision-making structure. The agents are providing suggestions on the problem â€Give me some suggestions if I want to build a compressed hydrogen storage station in Ohioâ€
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A6.F17">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="702" id="A6.F17.g1" src="/html/2308.10848/assets/x19.png" width="392"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 17:
    </span>
    An example process of project consulting in Solo setup. The agent is required to provide suggestions on the problem â€Give me some suggestions if I want to build a compressed hydrogen storage station in Ohioâ€.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A6.F18">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="658" id="A6.F18.g1" src="/html/2308.10848/assets/x20.png" width="392"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 18:
    </span>
    (Page 1) An example process of project consulting with Group setup in vertical decision-making structure. The agents are providing suggestions on the problem â€Generate a proposal about 3-day employee orientation for newly hired engineers at AgentVerse. AgentVerse is a open-source team devoted to developing a LLM multi-agent platform for accomplishingâ€.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A6.F19">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="662" id="A6.F19.g1" src="/html/2308.10848/assets/x21.png" width="392"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 19:
    </span>
    (Page 2) An example process of project consulting with Group setup in vertical decision-making structure. The agents are providing suggestions on the problem â€Generate a proposal about 3-day employee orientation for newly hired engineers at AgentVerse. AgentVerse is a open-source team devoted to developing a LLM multi-agent platform for accomplishingâ€.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A6.F20">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="514" id="A6.F20.g1" src="/html/2308.10848/assets/x22.png" width="392"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 20:
    </span>
    An example process of project consulting with Solo setup. The agent is required to provide suggestions on the problem â€Generate a proposal about 3-day employee orientation for newly hired engineers at AgentVerse. AgentVerse is a open-source team devoted to developing a LLM multi-agent platform for accomplishingâ€.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A6.F21">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="414" id="A6.F21.g1" src="/html/2308.10848/assets/x23.png" width="392"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 21:
    </span>
    An example process of
    <span class="ltx_text ltx_font_smallcaps" id="A6.F21.2.1">
     AgentVerse
    </span>
    with Group setup solving user query with three different tools.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A6.F22">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="599" id="A6.F22.g1" src="/html/2308.10848/assets/x24.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 22:
    </span>
    An example process of three agents crafting a bookshelf in Minecraft.
   </figcaption>
  </figure>
 </section>
</article>

<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Jiaxi Cui
    <br class="ltx_break"/>
    Peking University
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id2.2.id1">
     jiaxicui@chatlaw.cloud
    </span>
    <br class="ltx_break"/>
    &amp;Zongjian Li
    <sup class="ltx_sup" id="id3.3.id2">
     âˆ—
    </sup>
    <br class="ltx_break"/>
    Peking University
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id4.4.id3">
     chestnutlzj@chatlaw.cloud
    </span>
    <br class="ltx_break"/>
    &amp;Yang Yan
    <br class="ltx_break"/>
    Peking University
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id5.5.id4">
     yyang@stu.pku.edu.cn
    </span>
    <br class="ltx_break"/>
    &amp;Bohua Chen
    <br class="ltx_break"/>
    Peking University
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id6.6.id5">
     bohua@chatlaw.cloud
    </span>
    <br class="ltx_break"/>
    &amp;Li Yuan
    <br class="ltx_break"/>
    Peking University
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id7.7.id6">
     yuanli-ece@pku.edu.cn
    </span>
    <br class="ltx_break"/>
   </span>
   <span class="ltx_author_notes">
    Equal Contribution.Corresponding Author
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id8.id1">
   Large Language Models (LLMs) have shown the potential to revolutionize natural language processing tasks in various domains, sparking great interest in vertical-specific large models. However, unlike proprietary models such as BloombergGPT and FinGPT, which have leveraged their unique data accumulations to make strides in the finance domain, there hasnâ€™t not many similar large language models in the Chinese legal domain to facilitate its digital transformation.
  </p>
  <p class="ltx_p" id="id9.id2">
   In this paper, we propose an open-source legal large language model named ChatLaw. Due to the importance of data quality, we carefully designed a legal domain fine-tuning dataset. Additionally, to overcome the problem of model hallucinations in legal data screening during reference data retrieval, we introduce a method that combines vector database retrieval with keyword retrieval to effectively reduce the inaccuracy of relying solely on vector database retrieval. Furthermore, we propose a self-attention method to enhance the ability of large models to overcome errors present in reference data, further optimizing the issue of model hallucinations at the model level and improving the problem-solving capabilities of large models. We also open-sourced our model and part of the data at
   <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/PKU-YuanGroup/ChatLaw" target="_blank" title="">
    https://github.com/PKU-YuanGroup/ChatLaw
   </a>
   .
  </p>
 </div>
 <figure class="ltx_figure" id="S0.F1">
  <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="291" id="S0.F1.g1" src="/html/2306.16092/assets/x1.png" width="461"/>
  <figcaption class="ltx_caption ltx_centering">
   <span class="ltx_tag ltx_tag_figure">
    <span class="ltx_text" id="S0.F1.2.1.1" style="font-size:90%;">
     Figure 1
    </span>
    :
   </span>
   <span class="ltx_text" id="S0.F1.3.2" style="font-size:90%;">
    ChatLaw Framework
   </span>
  </figcaption>
 </figure>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    The continuous expansion and development of artificial intelligence have provided a fertile ground for the proliferation of large-scale language models. Models such as ChatGPT, GPT4
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ]
    </cite>
    , LLaMA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ]
    </cite>
    , Falcon
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    , Vicuna
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib2" title="">
      2
     </a>
     ]
    </cite>
    , and ChatGLM
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ]
    </cite>
    have demonstrated remarkable performance in various conventional tasks, unleashing tremendous potential for the field of law. However, it is evident that acquiring high-quality, relevant, and up-to-date data is a crucial factor in the development of large language models. Therefore, the development of effective and efficient open-source legal language models has become of paramount importance.
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    In the realm of artificial intelligence, the development of large-scale models has permeated various domains such as healthcare, education, and finance: BloombergGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib9" title="">
      9
     </a>
     ]
    </cite>
    , FinGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ]
    </cite>
    , Huatuo
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib8" title="">
      8
     </a>
     ]
    </cite>
    , ChatMed
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib14" title="">
      14
     </a>
     ]
    </cite>
    , These models have demonstrated their utility and impact in tackling complex tasks and generating valuable insights. However, the field of law, with its inherent importance and demand for accuracy, stands as a domain that necessitates dedicated research and development of a specialized legal model.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    Law plays a pivotal role in shaping societies, governing human interactions, and upholding justice. Legal professionals rely on accurate and up-to-date information to make informed decisions, interpret laws, and provide legal counsel. The complexities of legal language, nuanced interpretations, and the ever-evolving nature of legislation present unique challenges that require tailored solutions.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    However, when it comes to legal issues, there is often a phenomenon of hallucination and nonsensical outputs, even with the most advanced model like GPT4. People tend to believe that fine-tuning a model with specific domain knowledge would yield satisfactory results. However, in reality, this is not the case with early legal LLM (LawGPT), as there are still many instances of hallucination and unreliable outputs.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    We initially recognized the need for a Chinese legal LLM. However, at the time, there were no commercially available Chinese models surpassing the scale of 13 billion parameters. Therefore, we built upon the foundation of OpenLLAMA, a commercially viable model, by expanding the Chinese vocabulary and incorporating training data from sources like MOSS. This allowed us to create a foundational Chinese language model. Subsequently, we incorporated legal-specific data to train our legal modelâ€”â€”ChatLaw.
   </p>
  </div>
  <div class="ltx_para" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    The key contributions of this paper are as follows:
   </p>
   <ol class="ltx_enumerate" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      1.
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       <span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">
        Effective Approach to Mitigate Hallucination:
       </span>
       We propose an approach to address hallucination by enhancing the modelâ€™s training process and incorporating four modules during inference: â€consult,â€ â€referenceâ€, â€self-suggestionâ€ and â€response.â€ By integrating vertical models and knowledge bases through the reference module, we inject domain-specific knowledge into the model and leverage accurate information from the knowledge base, reducing the occurrence of hallucinations.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      2.
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       <span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">
        Legal Feature Word Extraction Model based on LLM:
       </span>
       We train a model that extracts legal feature words from usersâ€™ everyday language. This model identifies words with legal significance, enabling efficient identification and analysis of legal contexts within user input.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      3.
     </span>
     <div class="ltx_para" id="S1.I1.i3.p1">
      <p class="ltx_p" id="S1.I1.i3.p1.1">
       <span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">
        Legal Text Similarity Calculation Model based on BERT:
       </span>
       We train a model to measure the similarity between usersâ€™ everyday language and a dataset consisting of 930,000 relevant legal case texts. This enables the creation of a vector database for efficient retrieval of similar legal texts, facilitating further analysis and reference.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      4.
     </span>
     <div class="ltx_para" id="S1.I1.i4.p1">
      <p class="ltx_p" id="S1.I1.i4.p1.1">
       <span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">
        Construction of a Chinese Legal Exam Testing Dataset:
       </span>
       We curate a dataset specifically designed for testing legal domain knowledge in Chinese. Additionally, we design an ELO arena scoring mechanism to compare the performance of different models in legal multiple-choice questions.
      </p>
     </div>
    </li>
   </ol>
  </div>
  <div class="ltx_para" id="S1.p7">
   <p class="ltx_p" id="S1.p7.1">
    Furthermore, we observed that a single general-purpose legal LLM may not perform optimally across all tasks in this domain. Therefore, we trained different models for various scenarios, such as multiple-choice questions, keyword extraction, and question-answering. To handle the selection and deployment of these models, we employed a big LLM as a controller using the methodology provided by HuggingGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib6" title="">
      6
     </a>
     ]
    </cite>
    . This controller model dynamically determines which specific model to invoke based on each userâ€™s request, ensuring the most suitable model is utilized for the given task.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Dataset
  </h2>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    In constructing the dataset, we employed several approaches to ensure its comprehensiveness and diversity. The dataset composition methods are as follows:
   </p>
  </div>
  <div class="ltx_para" id="S2.p2">
   <p class="ltx_p" id="S2.p2.1">
    <span class="ltx_text ltx_font_bold" id="S2.p2.1.1">
     Collection of a vast amount of original legal data:
    </span>
    This includes gathering legal news, social media content, and discussions from legal industry forums. These sources provide a diverse range of real-world legal text, offering insights into various legal topics and discussions.
   </p>
  </div>
  <div class="ltx_para" id="S2.p3">
   <blockquote class="ltx_quote" id="S2.p3.1">
    <p class="ltx_p" id="S2.p3.1.1">
     <span class="ltx_text ltx_font_bold" id="S2.p3.1.1.1" style="font-size:90%;">
      Question
     </span>
     :
     <span class="ltx_text" id="S2.p3.1.1.2" style="font-size:90%;">
      åœ¨ä¸€åå¤§å­¦ç”Ÿçš„æ—¥è®°é‡Œå†™æ»¡â€œä¸€å®šè¦å¹²æ‰ä¹™ï¼Œå¯ä»¥é‡‡ç”¨çš„æ–¹æ³•å¾ˆå¤šï¼Œæœ€å¥½æ˜¯æŠ•æ¯’â€çš„è¯è¯­ï¼Œä½†æ˜¯æœªè§è¡ŒåŠ¨ï¼ŒæŸæ—¥è¾…å¯¼å‘˜çœ‹åˆ°æ—¥è®°åæŠ¥æ¡ˆï¼Œè¯·é—®è¿™ç§è¡Œä¸ºæ˜¯å¦å±äºæ•…æ„æ€äººç½ªï¼Ÿ
     </span>
     <br class="ltx_break"/>
     <span class="ltx_text ltx_font_bold" id="S2.p3.1.1.3" style="font-size:90%;">
      Answer
     </span>
     :
     <span class="ltx_text" id="S2.p3.1.1.4" style="font-size:90%;">
      æ ¹æ®æ‚¨æä¾›çš„æƒ…å†µï¼Œä¸€åå¤§å­¦ç”Ÿåœ¨æ—¥è®°ä¸­å†™æ»¡â€œä¸€å®šè¦å¹²æ‰ä¹™ï¼Œå¯ä»¥é‡‡ç”¨çš„æ–¹æ³•å¾ˆå¤šï¼Œæœ€å¥½æ˜¯æŠ•æ¯’â€çš„è¯è¯­ï¼Œä½†æ˜¯ä¸€ç›´æœªè§è¡ŒåŠ¨ï¼ŒæŸæ—¥è¾…å¯¼å‘˜çœ‹åˆ°ç”²çš„æ—¥è®°ï¼Œå¾ˆæ‹…å¿ƒä¾¿æŠ¥æ¡ˆã€‚æ ¹æ®æˆ‘å›½ã€Šåˆ‘æ³•ã€‹è§„å®šï¼Œè¿™ç§è¡Œä¸ºå±äºçŠ¯æ„è¡¨ç¤ºï¼Œæ— ç½ªã€‚
      <br class="ltx_break"/>
     </span>
    </p>
   </blockquote>
  </div>
  <div class="ltx_para" id="S2.p4">
   <p class="ltx_p" id="S2.p4.1">
    <span class="ltx_text ltx_font_bold" id="S2.p4.1.1">
     Construction based on legal regulations and judicial interpretations:
    </span>
    To ensure comprehensive coverage of legal knowledge, we incorporate relevant legal regulations and judicial interpretations into the dataset. This ensures that the dataset reflects the legal framework and provides accurate and up-to-date information.
   </p>
  </div>
  <div class="ltx_para" id="S2.p5">
   <blockquote class="ltx_quote" id="S2.p5.1">
    <p class="ltx_p" id="S2.p5.1.1">
     <span class="ltx_text ltx_font_bold" id="S2.p5.1.1.1" style="font-size:90%;">
      Question
     </span>
     :
     <span class="ltx_text" id="S2.p5.1.1.2" style="font-size:90%;">
      å¦‚æœæŸä¸ªå·¥äººæ¯å¤©å·¥ä½œ10å°æ—¶ï¼Œé‚£ä¹ˆæ¯å‘¨å·¥ä½œæ—¶é—´è¶…è¿‡äº†è§„å®šå—ï¼Ÿ
     </span>
     <br class="ltx_break"/>
     <span class="ltx_text ltx_font_bold" id="S2.p5.1.1.3" style="font-size:90%;">
      Answer
     </span>
     :
     <span class="ltx_text" id="S2.p5.1.1.4" style="font-size:90%;">
      æ ¹æ®ã€ŠåŠ³åŠ¨æ³•ã€‹ç¬¬ä¸‰åå…­æ¡ï¼Œæ¯æ—¥å·¥ä½œæ—¶é—´ä¸å¾—è¶…è¿‡8å°æ—¶ï¼Œå› æ­¤æ¯å‘¨å·¥ä½œæ—¶é—´å°†è¶…è¿‡44å°æ—¶ï¼Œè¶…è¿‡äº†è§„å®šã€‚
      <br class="ltx_break"/>
      <span class="ltx_text ltx_font_bold" id="S2.p5.1.1.4.1">
       Question
      </span>
     </span>
     :
     <span class="ltx_text" id="S2.p5.1.1.5" style="font-size:90%;">
      å¦‚æœä¸€ä¸ªå¤–å›½äººåœ¨æˆ‘å›½æˆ˜æ—¶å‘å›½å¤–æ­¦è£…åŠ›é‡æä¾›æ­¦å™¨è£…å¤‡ï¼Œæ˜¯å¦æ„æˆèµ„æ•Œç½ªï¼Ÿ
     </span>
     <br class="ltx_break"/>
     <span class="ltx_text ltx_font_bold" id="S2.p5.1.1.6" style="font-size:90%;">
      Answer
     </span>
     :
     <span class="ltx_text" id="S2.p5.1.1.7" style="font-size:90%;">
      ä¸æ„æˆèµ„æ•Œç½ªã€‚æ ¹æ®ã€Šåˆ‘æ³•ã€‹ç¬¬ä¸€ç™¾ä¸€åäºŒæ¡è§„å®šï¼Œèµ„æ•Œç½ªä¸»ä½“åªèƒ½æ˜¯æˆ‘å›½å…¬æ°‘ï¼Œå¤–å›½äººã€æ— å›½ç±äººä¸èƒ½æˆä¸ºèµ„æ•Œç½ªä¸»ä½“ã€‚å› æ­¤ï¼Œå¤–å›½äººåœ¨æˆ‘å›½æˆ˜æ—¶å‘å›½å¤–æ­¦è£…åŠ›é‡æä¾›æ­¦å™¨è£…å¤‡ï¼Œä¸æ„æˆèµ„æ•Œç½ªã€‚
      <br class="ltx_break"/>
     </span>
    </p>
   </blockquote>
  </div>
  <div class="ltx_para" id="S2.p6">
   <p class="ltx_p" id="S2.p6.1">
    <span class="ltx_text ltx_font_bold" id="S2.p6.1.1">
     Crawling real legal consultation data:
    </span>
    We retrieve authentic legal consultation data, leveraging existing legal consultation datasets. This enables the inclusion of real-world legal scenarios and questions that users commonly encounter, enriching the dataset with practical legal examples.
   </p>
  </div>
  <div class="ltx_para" id="S2.p7">
   <blockquote class="ltx_quote" id="S2.p7.1">
    <p class="ltx_p" id="S2.p7.1.1">
     <span class="ltx_text ltx_font_bold" id="S2.p7.1.1.1" style="font-size:90%;">
      Question
     </span>
     :
     <span class="ltx_text" id="S2.p7.1.1.2" style="font-size:90%;">
      æˆ‘æƒ³é—®ä¸€ä¸‹ï¼Œå½“äº‹äººå¦‚æœæ— åŠ›æ”¯ä»˜å¾‹å¸ˆè´¹ç”¨ï¼Œå¾‹å¸ˆåº”è¯¥å‡æ”¶æˆ–å…æ”¶è´¹ç”¨çš„æƒ…å†µæœ‰å“ªäº›ï¼Ÿæ¯”å¦‚ï¼Œå› ä¸ºè¢«æ£€å¯Ÿé™¢é”™è¯¯é€®æ•è€Œè¯·æ±‚èµ”å¿çš„æ¡ˆä»¶ï¼Œåœ¨ä½œä¸šæœŸé—´å‘ç”Ÿä¼¤æ®‹å¹¶è¯·æ±‚å•ä½æ”¯ä»˜åŒ»ç–—è´¹å’Œç”Ÿæ´»è´¹çš„æ¡ˆä»¶ï¼Œä»¥åŠå› äº¤é€šè‚‡äº‹ç½ªè€Œè˜è¯·å¾‹å¸ˆä½œè¾©æŠ¤çš„æ¡ˆä»¶ï¼Ÿ
     </span>
     <br class="ltx_break"/>
     <span class="ltx_text ltx_font_bold" id="S2.p7.1.1.3" style="font-size:90%;">
      Answer
     </span>
     :
     <span class="ltx_text" id="S2.p7.1.1.4" style="font-size:90%;">
      æ˜¯çš„ï¼Œå¦‚æœå½“äº‹äººæ— åŠ›æ”¯ä»˜å¾‹å¸ˆè´¹ç”¨ï¼Œå¾‹å¸ˆåº”è¯¥å‡æ”¶æˆ–å…æ”¶è´¹ç”¨çš„æƒ…å†µåŒ…æ‹¬ï¼šå› ä¸ºè¢«æ£€å¯Ÿé™¢é”™è¯¯é€®æ•è€Œè¯·æ±‚èµ”å¿çš„æ¡ˆä»¶ã€åœ¨ä½œä¸šæœŸé—´å‘ç”Ÿä¼¤æ®‹å¹¶è¯·æ±‚å•ä½æ”¯ä»˜åŒ»ç–—è´¹å’Œç”Ÿæ´»è´¹çš„æ¡ˆä»¶ï¼Œä»¥åŠå› äº¤é€šè‚‡äº‹ç½ªè€Œè˜è¯·å¾‹å¸ˆä½œè¾©æŠ¤çš„æ¡ˆä»¶ã€‚è¿™äº›æ¡ˆä»¶éƒ½å±äºç‰¹æ®Šæƒ…å†µï¼Œå¾‹å¸ˆåº”è¯¥åŠæ—¶å¸®åŠ©å½“äº‹äººè§£å†³ç›¸å…³é—®é¢˜ï¼Œæ”¯æŒå½“äº‹äººå®ç°æ­£ä¹‰ï¼Œå› æ­¤ï¼Œå¾‹å¸ˆåº”æ ¹æ®å½“äº‹äººçš„å®é™…æƒ…å†µå‡æ”¶æˆ–å…æ”¶è´¹ç”¨ã€‚
      <br class="ltx_break"/>
     </span>
    </p>
   </blockquote>
  </div>
  <div class="ltx_para" id="S2.p8">
   <p class="ltx_p" id="S2.p8.1">
    <span class="ltx_text ltx_font_bold" id="S2.p8.1.1">
     Construction of multiple-choice questions for the bar exam:
    </span>
    We create a set of multiple-choice questions specifically designed for the bar exam. These questions cover various legal topics and test usersâ€™ understanding and application of legal principles.
   </p>
  </div>
  <div class="ltx_para" id="S2.p9">
   <blockquote class="ltx_quote" id="S2.p9.1">
    <p class="ltx_p" id="S2.p9.1.1">
     <span class="ltx_text ltx_font_bold" id="S2.p9.1.1.1" style="font-size:90%;">
      Question
     </span>
     :
     <span class="ltx_text" id="S2.p9.1.1.2" style="font-size:90%;">
      çº¢æ˜Ÿä¸­å­¦é‡‡ç”¨ä¼ªåŠ£äº§å“é“ºè®¾è¶³çƒåœºï¼Œè‡´ä½¿åˆºæ¿€æ€§æ°”å‘³å››å¤„æ•£å‘ï¼Œå¹¶ä¸¥é‡æ±¡æŸ“äº†åœºåœ°åº•ä¸‹åœŸå£¤ã€‚äºæ˜¯ï¼Œç”²ç¯ä¿åä¼šå‘å¸‚ä¸­çº§äººæ°‘æ³•é™¢æèµ·è¯‰è®¼ï¼Œè¯·æ±‚åˆ¤ä»¤çº¢æ˜Ÿä¸­å­¦æ‹†é™¤æ–°å»ºçš„è¶³çƒåœºï¼Œå¹¶å¯¹æ±¡æŸ“çš„åœŸå£¤é‡‡å–ä¿®å¤æªæ–½ã€‚æ³•é™¢åœ¨å—ç†åç¬¬7æ—¥ä¹¦é¢å‘ŠçŸ¥å¸‚ç¯ä¿å±€ã€‚æ­¤æ—¶ï¼Œå¸‚äººæ°‘æ£€å¯Ÿé™¢ä¹Ÿå°±æ­¤å‘æ³•é™¢æèµ·å…¬ç›Šè¯‰è®¼ï¼Œæ³•é™¢å°†å…¶åˆ—ä¸ºå…±åŒåŸå‘Šã€‚åŒæ–¹å½“äº‹äººç»åå•†è¾¾æˆçš„å’Œè§£åè®®ï¼Œæ³•é™¢æœªäºˆå®¡æŸ¥å³å‘å‡ºå…¬å‘Šã€‚å…¬å‘ŠæœŸæ»¡åï¼Œåº”åŒæ–¹å½“äº‹äººè¯·æ±‚ï¼Œæ³•é™¢æœªåˆ¶ä½œè°ƒè§£ä¹¦ã€‚å…³äºæœ¬æ¡ˆï¼Œå¸‚ä¸­çº§äººæ°‘æ³•é™¢çš„ä¸‹åˆ—å“ªäº›åšæ³•æ˜¯ä¸åˆæ³•çš„ï¼ŸA. å—ç†åç¬¬7æ—¥ä¹¦é¢å‘ŠçŸ¥å¸‚ç¯ä¿å±€B. å¯¹å’Œè§£åè®®æœªç»å®¡æŸ¥å³å‘å‡ºå…¬å‘ŠC. å°†å¸‚äººæ°‘æ£€å¯Ÿé™¢åˆ—ä¸ºå…±åŒåŸå‘ŠD. åº”åŒæ–¹å½“äº‹äººè¯·æ±‚æœªåˆ¶ä½œè°ƒè§£ä¹¦
     </span>
     <br class="ltx_break"/>
     <span class="ltx_text ltx_font_bold" id="S2.p9.1.1.3" style="font-size:90%;">
      Answer
     </span>
     :
     <span class="ltx_text" id="S2.p9.1.1.4" style="font-size:90%;">
      ã€Šæ°‘è¯‰è§£é‡Šã€‹ç¬¬286æ¡è§„å®šï¼Œäººæ°‘æ³•é™¢å—ç†å…¬ç›Šè¯‰è®¼æ¡ˆä»¶åï¼Œåº”å½“åœ¨10æ—¥å†…ä¹¦é¢å‘ŠçŸ¥ç›¸å…³è¡Œæ”¿ä¸»ç®¡éƒ¨é—¨ã€‚å¸‚ä¸­é™¢å—ç†åç¬¬7æ—¥å‘ŠçŸ¥ï¼Œç¬¦åˆæ³•å¾‹è§„å®šã€‚Aé€‰é¡¹åˆæ³•ï¼Œä¸å½“é€‰ã€‚ã€Šæ°‘è¯‰è§£é‡Šã€‹ç¬¬289æ¡ç¬¬2æ¬¾è§„å®šï¼Œå½“äº‹äººè¾¾æˆå’Œè§£æˆ–è€…è°ƒè§£åè®®åï¼Œäººæ°‘æ³•é™¢åº”å½“å°†å’Œè§£æˆ–è€…è°ƒè§£åè®®è¿›è¡Œå…¬å‘Šã€‚å…¬å‘ŠæœŸé—´ä¸å¾—å°‘äº30æ—¥ã€‚æ®æ­¤ï¼Œæ³•é™¢å…¬å‘Šå‰ä¸å®¡æŸ¥ï¼Œå¹¶ä¸è¿åæ³•å¾‹è§„å®šã€‚Bé€‰é¡¹åˆæ³•ï¼Œä¸å½“é€‰ã€‚ã€Šæ°‘äº‹è¯‰è®¼æ³•ã€‹ç¬¬55æ¡ç¬¬2æ¬¾è§„å®šï¼Œäººæ°‘æ£€å¯Ÿé™¢åœ¨å±¥è¡ŒèŒè´£ä¸­å‘ç°ç ´åç”Ÿæ€ç¯å¢ƒå’Œèµ„æºä¿æŠ¤ã€é£Ÿå“è¯å“å®‰å…¨é¢†åŸŸä¾µå®³ä¼—å¤šæ¶ˆè´¹è€…åˆæ³•æƒç›Šç­‰æŸå®³ç¤¾ä¼šå…¬å…±åˆ©ç›Šçš„è¡Œä¸ºï¼Œåœ¨æ²¡æœ‰å‰æ¬¾è§„å®šçš„æœºå…³å’Œç»„ç»‡æˆ–è€…å‰æ¬¾è§„å®šçš„æœºå…³å’Œç»„ç»‡ä¸æèµ·è¯‰è®¼çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥å‘äººæ°‘æ³•é™¢æèµ·è¯‰è®¼ï¼›å‰æ¬¾è§„å®šçš„æœºå…³æˆ–è€…ç»„ç»‡æèµ·è¯‰è®¼çš„ï¼Œäººæ°‘æ£€å¯Ÿé™¢å¯ä»¥æ”¯æŒèµ·è¯‰ã€‚æ®æ­¤ï¼Œæœ¬æ¡ˆå·²æœ‰ç¯ä¿åä¼šæèµ·å…¬ç›Šè¯‰è®¼ï¼Œæ£€å¯Ÿæœºå…³åªèƒ½ä½œä¸ºæ”¯æŒèµ·è¯‰äººå‚ä¸å…¬ç›Šè¯‰è®¼ï¼Œè€Œä¸èƒ½æˆä¸ºå…±åŒåŸå‘Šã€‚Cé€‰é¡¹ä¸åˆæ³•ï¼Œå½“é€‰ã€‚ã€Šæ°‘è¯‰è§£é‡Šã€‹ç¬¬289æ¡ç¬¬3æ¬¾è§„å®šï¼Œå…¬å‘ŠæœŸæ»¡åï¼Œäººæ°‘æ³•é™¢ç»å®¡æŸ¥ï¼Œå’Œè§£æˆ–è€…è°ƒè§£åè®®ä¸è¿åç¤¾ä¼šå…¬å…±åˆ©ç›Šçš„ï¼Œåº”å½“å‡ºå…·è°ƒè§£ä¹¦ã€‚æ®æ­¤ï¼Œå…¬ç›Šè¯‰è®¼æ¡ˆä»¶æ³•é™¢å¿…é¡»åˆ¶ä½œè°ƒè§£ä¹¦ã€‚Dé€‰é¡¹ä¸åˆæ³•ï¼Œå½“é€‰ã€‚
      <br class="ltx_break"/>
     </span>
    </p>
   </blockquote>
  </div>
  <div class="ltx_para" id="S2.p10">
   <p class="ltx_p" id="S2.p10.1">
    By incorporating data from these diverse sources and construction methods, our dataset encompasses a wide range of legal contexts, ensuring that the developed model is capable of effectively understanding and addressing various legal scenarios.
   </p>
  </div>
  <div class="ltx_para" id="S2.p11">
   <p class="ltx_p" id="S2.p11.1">
    Once these data components are collected, the dataset undergoes a rigorous cleaning process. This involves filtering out short and incoherent responses, ensuring that only high-quality and meaningful text is included. Additionally, to enhance the dataset, we leverage the ChatGPT API for assisted construction, allowing us to generate supplementary data based on the existing dataset.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Training Process
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    The Keyword LLM is a language model that extracts keywords from abstract consulting problems raised by users. The Law LLM, on the other hand, extracts legal terminology that may be involved in user consultations. The ChatLaw LLM is the ultimate language model that outputs responses to users. It refers to relevant legal clauses and utilizes its own summarization and Q&amp;A function to generate advice for users in their consultations.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    ChatLaw LLM
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     To train ChatLAW, we fine-tuned it on the basis of Ziya-LLaMA-13B
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib11" title="">
       11
      </a>
      ]
     </cite>
     using Low-Rank Adaptation (LoRA)
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib3" title="">
       3
      </a>
      ]
     </cite>
     . Additionally, we introduced the self-suggestion role to further alleviate model hallucination issues. The training process was carried out on multiple A100 GPUs and the training costs were further reduced with the help of deepspeed.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Keyword LLM
   </h3>
   <figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_float">
      <span class="ltx_text ltx_font_bold" id="alg1.2.1.1">
       Algorithm 1
      </span>
     </span>
     Legal retrieval based on Large Langu Model keyword extraction
    </figcaption>
    <div class="ltx_listing ltx_listing" id="alg1.3">
     <div class="ltx_listingline" id="alg1.l1">
      <span class="ltx_tag ltx_tag_listingline">
       <span class="ltx_text" id="alg1.l1.1.1.1" style="font-size:80%;">
        1:
       </span>
      </span>
      Initialize the BERT model for embedding and keyword extraction model.
     </div>
     <div class="ltx_listingline" id="alg1.l2">
      <span class="ltx_tag ltx_tag_listingline">
       <span class="ltx_text" id="alg1.l2.1.1.1" style="font-size:80%;">
        2:
       </span>
      </span>
      Initialize the legal database as
      <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="alg1.l2.m1.1">
       <semantics id="alg1.l2.m1.1a">
        <mi class="ltx_font_mathcaligraphic" id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">
         â„’
        </mi>
        <annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b">
         <ci id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">
          â„’
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l2.m1.1c">
         \mathcal{L}
        </annotation>
       </semantics>
      </math>
      , where
      <math alttext="\mathbf{l}_{i}\in\mathcal{L}" class="ltx_Math" display="inline" id="alg1.l2.m2.1">
       <semantics id="alg1.l2.m2.1a">
        <mrow id="alg1.l2.m2.1.1" xref="alg1.l2.m2.1.1.cmml">
         <msub id="alg1.l2.m2.1.1.2" xref="alg1.l2.m2.1.1.2.cmml">
          <mi id="alg1.l2.m2.1.1.2.2" xref="alg1.l2.m2.1.1.2.2.cmml">
           ğ¥
          </mi>
          <mi id="alg1.l2.m2.1.1.2.3" xref="alg1.l2.m2.1.1.2.3.cmml">
           i
          </mi>
         </msub>
         <mo id="alg1.l2.m2.1.1.1" xref="alg1.l2.m2.1.1.1.cmml">
          âˆˆ
         </mo>
         <mi class="ltx_font_mathcaligraphic" id="alg1.l2.m2.1.1.3" xref="alg1.l2.m2.1.1.3.cmml">
          â„’
         </mi>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="alg1.l2.m2.1b">
         <apply id="alg1.l2.m2.1.1.cmml" xref="alg1.l2.m2.1.1">
          <in id="alg1.l2.m2.1.1.1.cmml" xref="alg1.l2.m2.1.1.1">
          </in>
          <apply id="alg1.l2.m2.1.1.2.cmml" xref="alg1.l2.m2.1.1.2">
           <csymbol cd="ambiguous" id="alg1.l2.m2.1.1.2.1.cmml" xref="alg1.l2.m2.1.1.2">
            subscript
           </csymbol>
           <ci id="alg1.l2.m2.1.1.2.2.cmml" xref="alg1.l2.m2.1.1.2.2">
            ğ¥
           </ci>
           <ci id="alg1.l2.m2.1.1.2.3.cmml" xref="alg1.l2.m2.1.1.2.3">
            ğ‘–
           </ci>
          </apply>
          <ci id="alg1.l2.m2.1.1.3.cmml" xref="alg1.l2.m2.1.1.3">
           â„’
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l2.m2.1c">
         \mathbf{l}_{i}\in\mathcal{L}
        </annotation>
       </semantics>
      </math>
      and i represents the i-th law. Let M be the number of laws in the legal database.
     </div>
     <div class="ltx_listingline" id="alg1.l3">
      <span class="ltx_tag ltx_tag_listingline">
       <span class="ltx_text" id="alg1.l3.1.1.1" style="font-size:80%;">
        3:
       </span>
      </span>
      Initialize the legal scores as
      <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="alg1.l3.m1.1">
       <semantics id="alg1.l3.m1.1a">
        <mi class="ltx_font_mathcaligraphic" id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml">
         ğ’®
        </mi>
        <annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b">
         <ci id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1">
          ğ’®
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l3.m1.1c">
         \mathcal{S}
        </annotation>
       </semantics>
      </math>
      , where
      <math alttext="s_{i}\in\mathcal{S}" class="ltx_Math" display="inline" id="alg1.l3.m2.1">
       <semantics id="alg1.l3.m2.1a">
        <mrow id="alg1.l3.m2.1.1" xref="alg1.l3.m2.1.1.cmml">
         <msub id="alg1.l3.m2.1.1.2" xref="alg1.l3.m2.1.1.2.cmml">
          <mi id="alg1.l3.m2.1.1.2.2" xref="alg1.l3.m2.1.1.2.2.cmml">
           s
          </mi>
          <mi id="alg1.l3.m2.1.1.2.3" xref="alg1.l3.m2.1.1.2.3.cmml">
           i
          </mi>
         </msub>
         <mo id="alg1.l3.m2.1.1.1" xref="alg1.l3.m2.1.1.1.cmml">
          âˆˆ
         </mo>
         <mi class="ltx_font_mathcaligraphic" id="alg1.l3.m2.1.1.3" xref="alg1.l3.m2.1.1.3.cmml">
          ğ’®
         </mi>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="alg1.l3.m2.1b">
         <apply id="alg1.l3.m2.1.1.cmml" xref="alg1.l3.m2.1.1">
          <in id="alg1.l3.m2.1.1.1.cmml" xref="alg1.l3.m2.1.1.1">
          </in>
          <apply id="alg1.l3.m2.1.1.2.cmml" xref="alg1.l3.m2.1.1.2">
           <csymbol cd="ambiguous" id="alg1.l3.m2.1.1.2.1.cmml" xref="alg1.l3.m2.1.1.2">
            subscript
           </csymbol>
           <ci id="alg1.l3.m2.1.1.2.2.cmml" xref="alg1.l3.m2.1.1.2.2">
            ğ‘ 
           </ci>
           <ci id="alg1.l3.m2.1.1.2.3.cmml" xref="alg1.l3.m2.1.1.2.3">
            ğ‘–
           </ci>
          </apply>
          <ci id="alg1.l3.m2.1.1.3.cmml" xref="alg1.l3.m2.1.1.3">
           ğ’®
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l3.m2.1c">
         s_{i}\in\mathcal{S}
        </annotation>
       </semantics>
      </math>
      represents the score corresponding to the
      <math alttext="i" class="ltx_Math" display="inline" id="alg1.l3.m3.1">
       <semantics id="alg1.l3.m3.1a">
        <mi id="alg1.l3.m3.1.1" xref="alg1.l3.m3.1.1.cmml">
         i
        </mi>
        <annotation-xml encoding="MathML-Content" id="alg1.l3.m3.1b">
         <ci id="alg1.l3.m3.1.1.cmml" xref="alg1.l3.m3.1.1">
          ğ‘–
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l3.m3.1c">
         i
        </annotation>
       </semantics>
      </math>
      -th law, all initialized to 0. The number of elements in
      <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="alg1.l3.m4.1">
       <semantics id="alg1.l3.m4.1a">
        <mi class="ltx_font_mathcaligraphic" id="alg1.l3.m4.1.1" xref="alg1.l3.m4.1.1.cmml">
         ğ’®
        </mi>
        <annotation-xml encoding="MathML-Content" id="alg1.l3.m4.1b">
         <ci id="alg1.l3.m4.1.1.cmml" xref="alg1.l3.m4.1.1">
          ğ’®
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l3.m4.1c">
         \mathcal{S}
        </annotation>
       </semantics>
      </math>
      is also
      <math alttext="M" class="ltx_Math" display="inline" id="alg1.l3.m5.1">
       <semantics id="alg1.l3.m5.1a">
        <mi id="alg1.l3.m5.1.1" xref="alg1.l3.m5.1.1.cmml">
         M
        </mi>
        <annotation-xml encoding="MathML-Content" id="alg1.l3.m5.1b">
         <ci id="alg1.l3.m5.1.1.cmml" xref="alg1.l3.m5.1.1">
          ğ‘€
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l3.m5.1c">
         M
        </annotation>
       </semantics>
      </math>
      .
     </div>
     <div class="ltx_listingline" id="alg1.l4">
      <span class="ltx_tag ltx_tag_listingline">
       <span class="ltx_text" id="alg1.l4.1.1.1" style="font-size:80%;">
        4:
       </span>
      </span>
      Extracting keywords from user queries using a keyword extraction model, and then inputting each keyword into a BERT model to obtain a collection of
      <math alttext="\mathcal{K}" class="ltx_Math" display="inline" id="alg1.l4.m1.1">
       <semantics id="alg1.l4.m1.1a">
        <mi class="ltx_font_mathcaligraphic" id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">
         ğ’¦
        </mi>
        <annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b">
         <ci id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1">
          ğ’¦
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l4.m1.1c">
         \mathcal{K}
        </annotation>
       </semantics>
      </math>
      keyword vectors, where
      <math alttext="\mathbf{k}_{i}" class="ltx_Math" display="inline" id="alg1.l4.m2.1">
       <semantics id="alg1.l4.m2.1a">
        <msub id="alg1.l4.m2.1.1" xref="alg1.l4.m2.1.1.cmml">
         <mi id="alg1.l4.m2.1.1.2" xref="alg1.l4.m2.1.1.2.cmml">
          ğ¤
         </mi>
         <mi id="alg1.l4.m2.1.1.3" xref="alg1.l4.m2.1.1.3.cmml">
          i
         </mi>
        </msub>
        <annotation-xml encoding="MathML-Content" id="alg1.l4.m2.1b">
         <apply id="alg1.l4.m2.1.1.cmml" xref="alg1.l4.m2.1.1">
          <csymbol cd="ambiguous" id="alg1.l4.m2.1.1.1.cmml" xref="alg1.l4.m2.1.1">
           subscript
          </csymbol>
          <ci id="alg1.l4.m2.1.1.2.cmml" xref="alg1.l4.m2.1.1.2">
           ğ¤
          </ci>
          <ci id="alg1.l4.m2.1.1.3.cmml" xref="alg1.l4.m2.1.1.3">
           ğ‘–
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l4.m2.1c">
         \mathbf{k}_{i}
        </annotation>
       </semantics>
      </math>
      represents the vector for the
      <math alttext="i" class="ltx_Math" display="inline" id="alg1.l4.m3.1">
       <semantics id="alg1.l4.m3.1a">
        <mi id="alg1.l4.m3.1.1" xref="alg1.l4.m3.1.1.cmml">
         i
        </mi>
        <annotation-xml encoding="MathML-Content" id="alg1.l4.m3.1b">
         <ci id="alg1.l4.m3.1.1.cmml" xref="alg1.l4.m3.1.1">
          ğ‘–
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l4.m3.1c">
         i
        </annotation>
       </semantics>
      </math>
      th keyword, with a total of
      <math alttext="N" class="ltx_Math" display="inline" id="alg1.l4.m4.1">
       <semantics id="alg1.l4.m4.1a">
        <mi id="alg1.l4.m4.1.1" xref="alg1.l4.m4.1.1.cmml">
         N
        </mi>
        <annotation-xml encoding="MathML-Content" id="alg1.l4.m4.1b">
         <ci id="alg1.l4.m4.1.1.cmml" xref="alg1.l4.m4.1.1">
          ğ‘
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l4.m4.1c">
         N
        </annotation>
       </semantics>
      </math>
      keywords. We obtain
      <math alttext="\mathbf{s}" class="ltx_Math" display="inline" id="alg1.l4.m5.1">
       <semantics id="alg1.l4.m5.1a">
        <mi id="alg1.l4.m5.1.1" xref="alg1.l4.m5.1.1.cmml">
         ğ¬
        </mi>
        <annotation-xml encoding="MathML-Content" id="alg1.l4.m5.1b">
         <ci id="alg1.l4.m5.1.1.cmml" xref="alg1.l4.m5.1.1">
          ğ¬
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l4.m5.1c">
         \mathbf{s}
        </annotation>
       </semantics>
      </math>
      by inputting the userâ€™s question into BERT.
     </div>
     <div class="ltx_listingline" id="alg1.l5">
      <span class="ltx_tag ltx_tag_listingline">
       <span class="ltx_text" id="alg1.l5.1.1.1" style="font-size:80%;">
        5:
       </span>
      </span>
      Initialize
      <math alttext="\alpha" class="ltx_Math" display="inline" id="alg1.l5.m1.1">
       <semantics id="alg1.l5.m1.1a">
        <mi id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml">
         Î±
        </mi>
        <annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b">
         <ci id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">
          ğ›¼
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l5.m1.1c">
         \alpha
        </annotation>
       </semantics>
      </math>
      for assigning weight to
      <math alttext="\mathbf{s}" class="ltx_Math" display="inline" id="alg1.l5.m2.1">
       <semantics id="alg1.l5.m2.1a">
        <mi id="alg1.l5.m2.1.1" xref="alg1.l5.m2.1.1.cmml">
         ğ¬
        </mi>
        <annotation-xml encoding="MathML-Content" id="alg1.l5.m2.1b">
         <ci id="alg1.l5.m2.1.1.cmml" xref="alg1.l5.m2.1.1">
          ğ¬
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l5.m2.1c">
         \mathbf{s}
        </annotation>
       </semantics>
      </math>
      .
     </div>
     <div class="ltx_listingline" id="alg1.l6">
      <span class="ltx_tag ltx_tag_listingline">
       <span class="ltx_text" id="alg1.l6.1.1.1" style="font-size:80%;">
        6:
       </span>
      </span>
      <span class="ltx_text ltx_font_bold" id="alg1.l6.2">
       for
      </span>
      <math alttext="i\ to\ N" class="ltx_Math" display="inline" id="alg1.l6.m1.1">
       <semantics id="alg1.l6.m1.1a">
        <mrow id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml">
         <mi id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml">
          i
         </mi>
         <mo id="alg1.l6.m1.1.1.1" lspace="0.500em" rspace="0em" xref="alg1.l6.m1.1.1.1.cmml">
          â€‹
         </mo>
         <mi id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml">
          t
         </mi>
         <mo id="alg1.l6.m1.1.1.1a" lspace="0em" rspace="0em" xref="alg1.l6.m1.1.1.1.cmml">
          â€‹
         </mo>
         <mi id="alg1.l6.m1.1.1.4" xref="alg1.l6.m1.1.1.4.cmml">
          o
         </mi>
         <mo id="alg1.l6.m1.1.1.1b" lspace="0.500em" rspace="0em" xref="alg1.l6.m1.1.1.1.cmml">
          â€‹
         </mo>
         <mi id="alg1.l6.m1.1.1.5" xref="alg1.l6.m1.1.1.5.cmml">
          N
         </mi>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b">
         <apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1">
          <times id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1">
          </times>
          <ci id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2">
           ğ‘–
          </ci>
          <ci id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3">
           ğ‘¡
          </ci>
          <ci id="alg1.l6.m1.1.1.4.cmml" xref="alg1.l6.m1.1.1.4">
           ğ‘œ
          </ci>
          <ci id="alg1.l6.m1.1.1.5.cmml" xref="alg1.l6.m1.1.1.5">
           ğ‘
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l6.m1.1c">
         i\ to\ N
        </annotation>
       </semantics>
      </math>
      <span class="ltx_text ltx_font_bold" id="alg1.l6.3">
       do
      </span>
     </div>
     <div class="ltx_listingline" id="alg1.l7">
      <span class="ltx_tag ltx_tag_listingline">
       <span class="ltx_text" id="alg1.l7.1.1.1" style="font-size:80%;">
        7:
       </span>
      </span>
      <math alttext="\mathbf{v}_{i}=\frac{\mathbf{k}_{i}}{||\mathbf{k}_{i}||}+\alpha\frac{\mathbf{s}}{||\mathbf{s}||}" class="ltx_Math" display="inline" id="alg1.l7.m1.2">
       <semantics id="alg1.l7.m1.2a">
        <mrow id="alg1.l7.m1.2.3" xref="alg1.l7.m1.2.3.cmml">
         <msub id="alg1.l7.m1.2.3.2" xref="alg1.l7.m1.2.3.2.cmml">
          <mi id="alg1.l7.m1.2.3.2.2" xref="alg1.l7.m1.2.3.2.2.cmml">
           ğ¯
          </mi>
          <mi id="alg1.l7.m1.2.3.2.3" xref="alg1.l7.m1.2.3.2.3.cmml">
           i
          </mi>
         </msub>
         <mo id="alg1.l7.m1.2.3.1" xref="alg1.l7.m1.2.3.1.cmml">
          =
         </mo>
         <mrow id="alg1.l7.m1.2.3.3" xref="alg1.l7.m1.2.3.3.cmml">
          <mfrac id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml">
           <msub id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3.cmml">
            <mi id="alg1.l7.m1.1.1.3.2" xref="alg1.l7.m1.1.1.3.2.cmml">
             ğ¤
            </mi>
            <mi id="alg1.l7.m1.1.1.3.3" xref="alg1.l7.m1.1.1.3.3.cmml">
             i
            </mi>
           </msub>
           <mrow id="alg1.l7.m1.1.1.1.1" xref="alg1.l7.m1.1.1.1.2.cmml">
            <mo id="alg1.l7.m1.1.1.1.1.2" maxsize="142%" minsize="142%" xref="alg1.l7.m1.1.1.1.2.1.cmml">
             â€–
            </mo>
            <msub id="alg1.l7.m1.1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.1.cmml">
             <mi id="alg1.l7.m1.1.1.1.1.1.2" xref="alg1.l7.m1.1.1.1.1.1.2.cmml">
              ğ¤
             </mi>
             <mi id="alg1.l7.m1.1.1.1.1.1.3" xref="alg1.l7.m1.1.1.1.1.1.3.cmml">
              i
             </mi>
            </msub>
            <mo id="alg1.l7.m1.1.1.1.1.3" maxsize="142%" minsize="142%" xref="alg1.l7.m1.1.1.1.2.1.cmml">
             â€–
            </mo>
           </mrow>
          </mfrac>
          <mo id="alg1.l7.m1.2.3.3.1" xref="alg1.l7.m1.2.3.3.1.cmml">
           +
          </mo>
          <mrow id="alg1.l7.m1.2.3.3.2" xref="alg1.l7.m1.2.3.3.2.cmml">
           <mi id="alg1.l7.m1.2.3.3.2.2" xref="alg1.l7.m1.2.3.3.2.2.cmml">
            Î±
           </mi>
           <mo id="alg1.l7.m1.2.3.3.2.1" lspace="0em" rspace="0em" xref="alg1.l7.m1.2.3.3.2.1.cmml">
            â€‹
           </mo>
           <mfrac id="alg1.l7.m1.2.2" xref="alg1.l7.m1.2.2.cmml">
            <mi id="alg1.l7.m1.2.2.3" xref="alg1.l7.m1.2.2.3.cmml">
             ğ¬
            </mi>
            <mrow id="alg1.l7.m1.2.2.1.3" xref="alg1.l7.m1.2.2.1.2.cmml">
             <mo id="alg1.l7.m1.2.2.1.3.1" maxsize="142%" minsize="142%" xref="alg1.l7.m1.2.2.1.2.1.cmml">
              â€–
             </mo>
             <mi id="alg1.l7.m1.2.2.1.1" xref="alg1.l7.m1.2.2.1.1.cmml">
              ğ¬
             </mi>
             <mo id="alg1.l7.m1.2.2.1.3.2" maxsize="142%" minsize="142%" xref="alg1.l7.m1.2.2.1.2.1.cmml">
              â€–
             </mo>
            </mrow>
           </mfrac>
          </mrow>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="alg1.l7.m1.2b">
         <apply id="alg1.l7.m1.2.3.cmml" xref="alg1.l7.m1.2.3">
          <eq id="alg1.l7.m1.2.3.1.cmml" xref="alg1.l7.m1.2.3.1">
          </eq>
          <apply id="alg1.l7.m1.2.3.2.cmml" xref="alg1.l7.m1.2.3.2">
           <csymbol cd="ambiguous" id="alg1.l7.m1.2.3.2.1.cmml" xref="alg1.l7.m1.2.3.2">
            subscript
           </csymbol>
           <ci id="alg1.l7.m1.2.3.2.2.cmml" xref="alg1.l7.m1.2.3.2.2">
            ğ¯
           </ci>
           <ci id="alg1.l7.m1.2.3.2.3.cmml" xref="alg1.l7.m1.2.3.2.3">
            ğ‘–
           </ci>
          </apply>
          <apply id="alg1.l7.m1.2.3.3.cmml" xref="alg1.l7.m1.2.3.3">
           <plus id="alg1.l7.m1.2.3.3.1.cmml" xref="alg1.l7.m1.2.3.3.1">
           </plus>
           <apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1">
            <divide id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1">
            </divide>
            <apply id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3">
             <csymbol cd="ambiguous" id="alg1.l7.m1.1.1.3.1.cmml" xref="alg1.l7.m1.1.1.3">
              subscript
             </csymbol>
             <ci id="alg1.l7.m1.1.1.3.2.cmml" xref="alg1.l7.m1.1.1.3.2">
              ğ¤
             </ci>
             <ci id="alg1.l7.m1.1.1.3.3.cmml" xref="alg1.l7.m1.1.1.3.3">
              ğ‘–
             </ci>
            </apply>
            <apply id="alg1.l7.m1.1.1.1.2.cmml" xref="alg1.l7.m1.1.1.1.1">
             <csymbol cd="latexml" id="alg1.l7.m1.1.1.1.2.1.cmml" xref="alg1.l7.m1.1.1.1.1.2">
              norm
             </csymbol>
             <apply id="alg1.l7.m1.1.1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1.1.1">
              <csymbol cd="ambiguous" id="alg1.l7.m1.1.1.1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1.1.1">
               subscript
              </csymbol>
              <ci id="alg1.l7.m1.1.1.1.1.1.2.cmml" xref="alg1.l7.m1.1.1.1.1.1.2">
               ğ¤
              </ci>
              <ci id="alg1.l7.m1.1.1.1.1.1.3.cmml" xref="alg1.l7.m1.1.1.1.1.1.3">
               ğ‘–
              </ci>
             </apply>
            </apply>
           </apply>
           <apply id="alg1.l7.m1.2.3.3.2.cmml" xref="alg1.l7.m1.2.3.3.2">
            <times id="alg1.l7.m1.2.3.3.2.1.cmml" xref="alg1.l7.m1.2.3.3.2.1">
            </times>
            <ci id="alg1.l7.m1.2.3.3.2.2.cmml" xref="alg1.l7.m1.2.3.3.2.2">
             ğ›¼
            </ci>
            <apply id="alg1.l7.m1.2.2.cmml" xref="alg1.l7.m1.2.2">
             <divide id="alg1.l7.m1.2.2.2.cmml" xref="alg1.l7.m1.2.2">
             </divide>
             <ci id="alg1.l7.m1.2.2.3.cmml" xref="alg1.l7.m1.2.2.3">
              ğ¬
             </ci>
             <apply id="alg1.l7.m1.2.2.1.2.cmml" xref="alg1.l7.m1.2.2.1.3">
              <csymbol cd="latexml" id="alg1.l7.m1.2.2.1.2.1.cmml" xref="alg1.l7.m1.2.2.1.3.1">
               norm
              </csymbol>
              <ci id="alg1.l7.m1.2.2.1.1.cmml" xref="alg1.l7.m1.2.2.1.1">
               ğ¬
              </ci>
             </apply>
            </apply>
           </apply>
          </apply>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l7.m1.2c">
         \mathbf{v}_{i}=\frac{\mathbf{k}_{i}}{||\mathbf{k}_{i}||}+\alpha\frac{\mathbf{s}}{||\mathbf{s}||}
        </annotation>
       </semantics>
      </math>
     </div>
     <div class="ltx_listingline" id="alg1.l8">
      <span class="ltx_tag ltx_tag_listingline">
       <span class="ltx_text" id="alg1.l8.1.1.1" style="font-size:80%;">
        8:
       </span>
      </span>
      <span class="ltx_text ltx_font_bold" id="alg1.l8.2">
       for
      </span>
      <math alttext="j\ to\ M" class="ltx_Math" display="inline" id="alg1.l8.m1.1">
       <semantics id="alg1.l8.m1.1a">
        <mrow id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml">
         <mi id="alg1.l8.m1.1.1.2" xref="alg1.l8.m1.1.1.2.cmml">
          j
         </mi>
         <mo id="alg1.l8.m1.1.1.1" lspace="0.500em" rspace="0em" xref="alg1.l8.m1.1.1.1.cmml">
          â€‹
         </mo>
         <mi id="alg1.l8.m1.1.1.3" xref="alg1.l8.m1.1.1.3.cmml">
          t
         </mi>
         <mo id="alg1.l8.m1.1.1.1a" lspace="0em" rspace="0em" xref="alg1.l8.m1.1.1.1.cmml">
          â€‹
         </mo>
         <mi id="alg1.l8.m1.1.1.4" xref="alg1.l8.m1.1.1.4.cmml">
          o
         </mi>
         <mo id="alg1.l8.m1.1.1.1b" lspace="0.500em" rspace="0em" xref="alg1.l8.m1.1.1.1.cmml">
          â€‹
         </mo>
         <mi id="alg1.l8.m1.1.1.5" xref="alg1.l8.m1.1.1.5.cmml">
          M
         </mi>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b">
         <apply id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1">
          <times id="alg1.l8.m1.1.1.1.cmml" xref="alg1.l8.m1.1.1.1">
          </times>
          <ci id="alg1.l8.m1.1.1.2.cmml" xref="alg1.l8.m1.1.1.2">
           ğ‘—
          </ci>
          <ci id="alg1.l8.m1.1.1.3.cmml" xref="alg1.l8.m1.1.1.3">
           ğ‘¡
          </ci>
          <ci id="alg1.l8.m1.1.1.4.cmml" xref="alg1.l8.m1.1.1.4">
           ğ‘œ
          </ci>
          <ci id="alg1.l8.m1.1.1.5.cmml" xref="alg1.l8.m1.1.1.5">
           ğ‘€
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l8.m1.1c">
         j\ to\ M
        </annotation>
       </semantics>
      </math>
      <span class="ltx_text ltx_font_bold" id="alg1.l8.3">
       do
      </span>
     </div>
     <div class="ltx_listingline" id="alg1.l9">
      <span class="ltx_tag ltx_tag_listingline">
       <span class="ltx_text" id="alg1.l9.1.1.1" style="font-size:80%;">
        9:
       </span>
      </span>
      <math alttext="s_{j}\xleftarrow{}s_{j}+cossim(\mathbf{v}_{i},\mathbf{l}_{j})" class="ltx_Math" display="inline" id="alg1.l9.m1.2">
       <semantics id="alg1.l9.m1.2a">
        <mrow id="alg1.l9.m1.2.2" xref="alg1.l9.m1.2.2.cmml">
         <msub id="alg1.l9.m1.2.2.4" xref="alg1.l9.m1.2.2.4.cmml">
          <mi id="alg1.l9.m1.2.2.4.2" xref="alg1.l9.m1.2.2.4.2.cmml">
           s
          </mi>
          <mi id="alg1.l9.m1.2.2.4.3" xref="alg1.l9.m1.2.2.4.3.cmml">
           j
          </mi>
         </msub>
         <mover accent="true" id="alg1.l9.m1.2.2.3" xref="alg1.l9.m1.2.2.3.cmml">
          <mo id="alg1.l9.m1.2.2.3.2" stretchy="false" xref="alg1.l9.m1.2.2.3.2.cmml">
           â†
          </mo>
          <mi id="alg1.l9.m1.2.2.3.1" xref="alg1.l9.m1.2.2.3.1.cmml">
          </mi>
         </mover>
         <mrow id="alg1.l9.m1.2.2.2" xref="alg1.l9.m1.2.2.2.cmml">
          <msub id="alg1.l9.m1.2.2.2.4" xref="alg1.l9.m1.2.2.2.4.cmml">
           <mi id="alg1.l9.m1.2.2.2.4.2" xref="alg1.l9.m1.2.2.2.4.2.cmml">
            s
           </mi>
           <mi id="alg1.l9.m1.2.2.2.4.3" xref="alg1.l9.m1.2.2.2.4.3.cmml">
            j
           </mi>
          </msub>
          <mo id="alg1.l9.m1.2.2.2.3" xref="alg1.l9.m1.2.2.2.3.cmml">
           +
          </mo>
          <mrow id="alg1.l9.m1.2.2.2.2" xref="alg1.l9.m1.2.2.2.2.cmml">
           <mi id="alg1.l9.m1.2.2.2.2.4" xref="alg1.l9.m1.2.2.2.2.4.cmml">
            c
           </mi>
           <mo id="alg1.l9.m1.2.2.2.2.3" lspace="0em" rspace="0em" xref="alg1.l9.m1.2.2.2.2.3.cmml">
            â€‹
           </mo>
           <mi id="alg1.l9.m1.2.2.2.2.5" xref="alg1.l9.m1.2.2.2.2.5.cmml">
            o
           </mi>
           <mo id="alg1.l9.m1.2.2.2.2.3a" lspace="0em" rspace="0em" xref="alg1.l9.m1.2.2.2.2.3.cmml">
            â€‹
           </mo>
           <mi id="alg1.l9.m1.2.2.2.2.6" xref="alg1.l9.m1.2.2.2.2.6.cmml">
            s
           </mi>
           <mo id="alg1.l9.m1.2.2.2.2.3b" lspace="0em" rspace="0em" xref="alg1.l9.m1.2.2.2.2.3.cmml">
            â€‹
           </mo>
           <mi id="alg1.l9.m1.2.2.2.2.7" xref="alg1.l9.m1.2.2.2.2.7.cmml">
            s
           </mi>
           <mo id="alg1.l9.m1.2.2.2.2.3c" lspace="0em" rspace="0em" xref="alg1.l9.m1.2.2.2.2.3.cmml">
            â€‹
           </mo>
           <mi id="alg1.l9.m1.2.2.2.2.8" xref="alg1.l9.m1.2.2.2.2.8.cmml">
            i
           </mi>
           <mo id="alg1.l9.m1.2.2.2.2.3d" lspace="0em" rspace="0em" xref="alg1.l9.m1.2.2.2.2.3.cmml">
            â€‹
           </mo>
           <mi id="alg1.l9.m1.2.2.2.2.9" xref="alg1.l9.m1.2.2.2.2.9.cmml">
            m
           </mi>
           <mo id="alg1.l9.m1.2.2.2.2.3e" lspace="0em" rspace="0em" xref="alg1.l9.m1.2.2.2.2.3.cmml">
            â€‹
           </mo>
           <mrow id="alg1.l9.m1.2.2.2.2.2.2" xref="alg1.l9.m1.2.2.2.2.2.3.cmml">
            <mo id="alg1.l9.m1.2.2.2.2.2.2.3" stretchy="false" xref="alg1.l9.m1.2.2.2.2.2.3.cmml">
             (
            </mo>
            <msub id="alg1.l9.m1.1.1.1.1.1.1.1" xref="alg1.l9.m1.1.1.1.1.1.1.1.cmml">
             <mi id="alg1.l9.m1.1.1.1.1.1.1.1.2" xref="alg1.l9.m1.1.1.1.1.1.1.1.2.cmml">
              ğ¯
             </mi>
             <mi id="alg1.l9.m1.1.1.1.1.1.1.1.3" xref="alg1.l9.m1.1.1.1.1.1.1.1.3.cmml">
              i
             </mi>
            </msub>
            <mo id="alg1.l9.m1.2.2.2.2.2.2.4" xref="alg1.l9.m1.2.2.2.2.2.3.cmml">
             ,
            </mo>
            <msub id="alg1.l9.m1.2.2.2.2.2.2.2" xref="alg1.l9.m1.2.2.2.2.2.2.2.cmml">
             <mi id="alg1.l9.m1.2.2.2.2.2.2.2.2" xref="alg1.l9.m1.2.2.2.2.2.2.2.2.cmml">
              ğ¥
             </mi>
             <mi id="alg1.l9.m1.2.2.2.2.2.2.2.3" xref="alg1.l9.m1.2.2.2.2.2.2.2.3.cmml">
              j
             </mi>
            </msub>
            <mo id="alg1.l9.m1.2.2.2.2.2.2.5" stretchy="false" xref="alg1.l9.m1.2.2.2.2.2.3.cmml">
             )
            </mo>
           </mrow>
          </mrow>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="alg1.l9.m1.2b">
         <apply id="alg1.l9.m1.2.2.cmml" xref="alg1.l9.m1.2.2">
          <apply id="alg1.l9.m1.2.2.3.cmml" xref="alg1.l9.m1.2.2.3">
           <csymbol cd="latexml" id="alg1.l9.m1.2.2.3.1.cmml" xref="alg1.l9.m1.2.2.3.1">
            absent
           </csymbol>
           <ci id="alg1.l9.m1.2.2.3.2.cmml" xref="alg1.l9.m1.2.2.3.2">
            â†
           </ci>
          </apply>
          <apply id="alg1.l9.m1.2.2.4.cmml" xref="alg1.l9.m1.2.2.4">
           <csymbol cd="ambiguous" id="alg1.l9.m1.2.2.4.1.cmml" xref="alg1.l9.m1.2.2.4">
            subscript
           </csymbol>
           <ci id="alg1.l9.m1.2.2.4.2.cmml" xref="alg1.l9.m1.2.2.4.2">
            ğ‘ 
           </ci>
           <ci id="alg1.l9.m1.2.2.4.3.cmml" xref="alg1.l9.m1.2.2.4.3">
            ğ‘—
           </ci>
          </apply>
          <apply id="alg1.l9.m1.2.2.2.cmml" xref="alg1.l9.m1.2.2.2">
           <plus id="alg1.l9.m1.2.2.2.3.cmml" xref="alg1.l9.m1.2.2.2.3">
           </plus>
           <apply id="alg1.l9.m1.2.2.2.4.cmml" xref="alg1.l9.m1.2.2.2.4">
            <csymbol cd="ambiguous" id="alg1.l9.m1.2.2.2.4.1.cmml" xref="alg1.l9.m1.2.2.2.4">
             subscript
            </csymbol>
            <ci id="alg1.l9.m1.2.2.2.4.2.cmml" xref="alg1.l9.m1.2.2.2.4.2">
             ğ‘ 
            </ci>
            <ci id="alg1.l9.m1.2.2.2.4.3.cmml" xref="alg1.l9.m1.2.2.2.4.3">
             ğ‘—
            </ci>
           </apply>
           <apply id="alg1.l9.m1.2.2.2.2.cmml" xref="alg1.l9.m1.2.2.2.2">
            <times id="alg1.l9.m1.2.2.2.2.3.cmml" xref="alg1.l9.m1.2.2.2.2.3">
            </times>
            <ci id="alg1.l9.m1.2.2.2.2.4.cmml" xref="alg1.l9.m1.2.2.2.2.4">
             ğ‘
            </ci>
            <ci id="alg1.l9.m1.2.2.2.2.5.cmml" xref="alg1.l9.m1.2.2.2.2.5">
             ğ‘œ
            </ci>
            <ci id="alg1.l9.m1.2.2.2.2.6.cmml" xref="alg1.l9.m1.2.2.2.2.6">
             ğ‘ 
            </ci>
            <ci id="alg1.l9.m1.2.2.2.2.7.cmml" xref="alg1.l9.m1.2.2.2.2.7">
             ğ‘ 
            </ci>
            <ci id="alg1.l9.m1.2.2.2.2.8.cmml" xref="alg1.l9.m1.2.2.2.2.8">
             ğ‘–
            </ci>
            <ci id="alg1.l9.m1.2.2.2.2.9.cmml" xref="alg1.l9.m1.2.2.2.2.9">
             ğ‘š
            </ci>
            <interval closure="open" id="alg1.l9.m1.2.2.2.2.2.3.cmml" xref="alg1.l9.m1.2.2.2.2.2.2">
             <apply id="alg1.l9.m1.1.1.1.1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1.1.1.1.1">
              <csymbol cd="ambiguous" id="alg1.l9.m1.1.1.1.1.1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1.1.1.1.1">
               subscript
              </csymbol>
              <ci id="alg1.l9.m1.1.1.1.1.1.1.1.2.cmml" xref="alg1.l9.m1.1.1.1.1.1.1.1.2">
               ğ¯
              </ci>
              <ci id="alg1.l9.m1.1.1.1.1.1.1.1.3.cmml" xref="alg1.l9.m1.1.1.1.1.1.1.1.3">
               ğ‘–
              </ci>
             </apply>
             <apply id="alg1.l9.m1.2.2.2.2.2.2.2.cmml" xref="alg1.l9.m1.2.2.2.2.2.2.2">
              <csymbol cd="ambiguous" id="alg1.l9.m1.2.2.2.2.2.2.2.1.cmml" xref="alg1.l9.m1.2.2.2.2.2.2.2">
               subscript
              </csymbol>
              <ci id="alg1.l9.m1.2.2.2.2.2.2.2.2.cmml" xref="alg1.l9.m1.2.2.2.2.2.2.2.2">
               ğ¥
              </ci>
              <ci id="alg1.l9.m1.2.2.2.2.2.2.2.3.cmml" xref="alg1.l9.m1.2.2.2.2.2.2.2.3">
               ğ‘—
              </ci>
             </apply>
            </interval>
           </apply>
          </apply>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l9.m1.2c">
         s_{j}\xleftarrow{}s_{j}+cossim(\mathbf{v}_{i},\mathbf{l}_{j})
        </annotation>
       </semantics>
      </math>
     </div>
     <div class="ltx_listingline" id="alg1.l10">
      <span class="ltx_tag ltx_tag_listingline">
       <span class="ltx_text" id="alg1.l10.1.1.1" style="font-size:80%;">
        10:
       </span>
      </span>
      <span class="ltx_text ltx_font_bold" id="alg1.l10.2">
       end
      </span>
      <span class="ltx_text ltx_font_bold" id="alg1.l10.3">
       for
      </span>
     </div>
     <div class="ltx_listingline" id="alg1.l11">
      <span class="ltx_tag ltx_tag_listingline">
       <span class="ltx_text" id="alg1.l11.1.1.1" style="font-size:80%;">
        11:
       </span>
      </span>
      <span class="ltx_text ltx_font_bold" id="alg1.l11.2">
       end
      </span>
      <span class="ltx_text ltx_font_bold" id="alg1.l11.3">
       for
      </span>
     </div>
     <div class="ltx_listingline" id="alg1.l12">
      <span class="ltx_tag ltx_tag_listingline">
       <span class="ltx_text" id="alg1.l12.1.1.1" style="font-size:80%;">
        12:
       </span>
      </span>
      <span class="ltx_text ltx_font_bold" id="alg1.l12.2">
       return
      </span>
      <math alttext="TopK(\mathcal{S})" class="ltx_Math" display="inline" id="alg1.l12.m1.1">
       <semantics id="alg1.l12.m1.1a">
        <mrow id="alg1.l12.m1.1.2" xref="alg1.l12.m1.1.2.cmml">
         <mi id="alg1.l12.m1.1.2.2" xref="alg1.l12.m1.1.2.2.cmml">
          T
         </mi>
         <mo id="alg1.l12.m1.1.2.1" lspace="0em" rspace="0em" xref="alg1.l12.m1.1.2.1.cmml">
          â€‹
         </mo>
         <mi id="alg1.l12.m1.1.2.3" xref="alg1.l12.m1.1.2.3.cmml">
          o
         </mi>
         <mo id="alg1.l12.m1.1.2.1a" lspace="0em" rspace="0em" xref="alg1.l12.m1.1.2.1.cmml">
          â€‹
         </mo>
         <mi id="alg1.l12.m1.1.2.4" xref="alg1.l12.m1.1.2.4.cmml">
          p
         </mi>
         <mo id="alg1.l12.m1.1.2.1b" lspace="0em" rspace="0em" xref="alg1.l12.m1.1.2.1.cmml">
          â€‹
         </mo>
         <mi id="alg1.l12.m1.1.2.5" xref="alg1.l12.m1.1.2.5.cmml">
          K
         </mi>
         <mo id="alg1.l12.m1.1.2.1c" lspace="0em" rspace="0em" xref="alg1.l12.m1.1.2.1.cmml">
          â€‹
         </mo>
         <mrow id="alg1.l12.m1.1.2.6.2" xref="alg1.l12.m1.1.2.cmml">
          <mo id="alg1.l12.m1.1.2.6.2.1" stretchy="false" xref="alg1.l12.m1.1.2.cmml">
           (
          </mo>
          <mi class="ltx_font_mathcaligraphic" id="alg1.l12.m1.1.1" xref="alg1.l12.m1.1.1.cmml">
           ğ’®
          </mi>
          <mo id="alg1.l12.m1.1.2.6.2.2" stretchy="false" xref="alg1.l12.m1.1.2.cmml">
           )
          </mo>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="alg1.l12.m1.1b">
         <apply id="alg1.l12.m1.1.2.cmml" xref="alg1.l12.m1.1.2">
          <times id="alg1.l12.m1.1.2.1.cmml" xref="alg1.l12.m1.1.2.1">
          </times>
          <ci id="alg1.l12.m1.1.2.2.cmml" xref="alg1.l12.m1.1.2.2">
           ğ‘‡
          </ci>
          <ci id="alg1.l12.m1.1.2.3.cmml" xref="alg1.l12.m1.1.2.3">
           ğ‘œ
          </ci>
          <ci id="alg1.l12.m1.1.2.4.cmml" xref="alg1.l12.m1.1.2.4">
           ğ‘
          </ci>
          <ci id="alg1.l12.m1.1.2.5.cmml" xref="alg1.l12.m1.1.2.5">
           ğ¾
          </ci>
          <ci id="alg1.l12.m1.1.1.cmml" xref="alg1.l12.m1.1.1">
           ğ’®
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="alg1.l12.m1.1c">
         TopK(\mathcal{S})
        </annotation>
       </semantics>
      </math>
     </div>
    </div>
   </figure>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     Creating ChatLaw product by combining vertical-specific LLM with a knowledge base, it is crucial to retrieve relevant information from the knowledge base based on user queries. We initially tried traditional software development methods such as MySQL and Elasticsearch for retrieval, but the results were unsatisfactory. Therefore, we attempted to use a pre-trained BERT model for embedding, followed by methods such as Faiss
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib4" title="">
       4
      </a>
      ]
     </cite>
     to calculate cosine similarity and extract the top k legal regulations related to user queries. However, this method often yields suboptimal results when the userâ€™s question is vague. Therefore, we aim to extract key information from user queries and use the vector embedding of this information to design an algorithm to improve matching accuracy.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.1">
     Due to the significant advantages of large models in understanding user queries, we fine-tuned an LLM to extract the keywords from user queries. After obtaining multiple keywords, we adopted
     <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">
      Algorithm 1
     </span>
     to retrieve relevant legal provisions.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F2">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="366" id="S3.F2.g1" src="/html/2306.16092/assets/images/keyword_law.jpg" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">
       Figure 2
      </span>
      :
     </span>
     <span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">
      Result of Keyword LLM and Law LLM
     </span>
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Law LLM
   </h3>
   <div class="ltx_para" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     We trained a BERT model using a dataset of 937k national case law examples to extract corresponding legal provisions and judicial interpretations from user queries. This Law LLM model forms an essential component of the ChatLaw product.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Experiment and Analysis
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    Evaluating the performance of the Large Language Model (LLM) has always been a challenge. For this purpose, we have collected national judicial examination questions over a decade and compiled a test dataset containing 2000 questions with their standard answers to measure the modelsâ€™ ability to handle legal multiple-choice questions.
   </p>
  </div>
  <div class="ltx_para" id="S4.p2">
   <p class="ltx_p" id="S4.p2.1">
    However, we found that the accuracy rates of the models are generally quite low. Under these circumstances, simply comparing accuracy rates seems to hold little significance. Therefore, we have established an evaluation mechanism for model competition for Elo points, inspired by the matchmaking mechanism in e-sports and the design of Chatbot Arena
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib13" title="">
      13
     </a>
     ]
    </cite>
    , to more effectively assess the modelsâ€™ abilities to handle legal multiple-choice questions.
   </p>
  </div>
  <figure class="ltx_figure" id="S4.F4">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_2">
     <figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S4.F4.1" style="width:108.4pt;">
      <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="820" id="S4.F4.1.g1" src="/html/2306.16092/assets/images/elo.jpg" width="598"/>
      <figcaption class="ltx_caption ltx_centering">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="S4.F4.1.1.1.1" style="font-size:90%;">
         Figure 3
        </span>
        :
       </span>
       <span class="ltx_text" id="S4.F4.1.2.2" style="font-size:90%;">
        ELO Ranking up until June 25
       </span>
      </figcaption>
     </figure>
    </div>
    <div class="ltx_flex_cell ltx_flex_size_2">
     <figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S4.F4.2" style="width:303.5pt;">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="529" id="S4.F4.2.g1" src="/html/2306.16092/assets/images/win_rate.png" width="598"/>
      <figcaption class="ltx_caption ltx_centering">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="S4.F4.2.1.1.1" style="font-size:90%;">
         Figure 4
        </span>
        :
       </span>
       <span class="ltx_text" id="S4.F4.2.2.2" style="font-size:90%;">
        LLM Win Rate
       </span>
      </figcaption>
     </figure>
    </div>
   </div>
  </figure>
  <div class="ltx_para" id="S4.p3">
   <p class="ltx_p" id="S4.p3.1">
    Through the analysis of the above experimental results, we can make the following observations:
   </p>
  </div>
  <div class="ltx_para" id="S4.p4">
   <p class="ltx_p" id="S4.p4.1">
    (1) The introduction of legal-related Q&amp;A and statute data can to some extent improve the modelâ€™s performance on multiple-choice questions;
   </p>
  </div>
  <div class="ltx_para" id="S4.p5">
   <p class="ltx_p" id="S4.p5.1">
    (2) The addition of specific task types for training significantly improves the modelâ€™s performance on such tasks. For example, the reason why the ChatLaw model outperforms GPT-4 is that we used a large number of multiple-choice questions as training data;
   </p>
  </div>
  <div class="ltx_para" id="S4.p6">
   <p class="ltx_p" id="S4.p6.1">
    (3) Legal multiple-choice questions require complex logical reasoning, so models with a larger number of parameters usually perform better.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Conclusions
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    In this paper, we proposed ChatLaw, a legal large language model(LLM) developed using legal domain knowledge. We propose a novel approach that combines LLM with vector knowledge databases, which significantly alleviates the hallucination problem commonly seen in LLM. Our stable model handling strategies enable the resolution of various legal domain problems. Additionally, we release a dataset for legal multiple-choice questions and design an ELO model ranking mechanism.
   </p>
  </div>
  <div class="ltx_para" id="S5.p2">
   <p class="ltx_p" id="S5.p2.1">
    However, our limitations arise due to the scale of the base model. Our performance in tasks such as logical reasoning and deduction is not optimal. Additionally, after incorporating a large amount of domain-specific data, further research is required to improve the generalization of ChatLaw for generic tasks. There are potential social risks on ChatLaw, and we advise users to make use of our method for proper purposes.
   </p>
  </div>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [1]
    </span>
    <span class="ltx_bibblock">
     Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien
Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme
Penedo.
    </span>
    <span class="ltx_bibblock">
     Falcon-40B: an open large language model with state-of-the-art
performance.
    </span>
    <span class="ltx_bibblock">
     2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [2]
    </span>
    <span class="ltx_bibblock">
     Wei-Lin Chiang, Zhuohan Li, ZiÂ Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
Zheng, Siyuan Zhuang, Yonghao Zhuang, JosephÂ E. Gonzalez, Ion Stoica, and
EricÂ P. Xing.
    </span>
    <span class="ltx_bibblock">
     Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
quality, March 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [3]
    </span>
    <span class="ltx_bibblock">
     EdwardÂ J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, LuÂ Wang, and Weizhu Chen.
    </span>
    <span class="ltx_bibblock">
     LoRA: Low-rank adaptation of large language models.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">
      International Conference on Learning Representations
     </span>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [4]
    </span>
    <span class="ltx_bibblock">
     Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou.
    </span>
    <span class="ltx_bibblock">
     Billion-scale similarity search with GPUs.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">
      IEEE Transactions on Big Data
     </span>
     , 7(3):535â€“547, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [5]
    </span>
    <span class="ltx_bibblock">
     OpenAI.
    </span>
    <span class="ltx_bibblock">
     Gpt-4 technical report, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [6]
    </span>
    <span class="ltx_bibblock">
     Yongliang Shen, Kaitao Song, XuÂ Tan, Dongsheng Li, Weiming Lu, and Yueting
Zhuang.
    </span>
    <span class="ltx_bibblock">
     Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging
face, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [7]
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric
Hambro, Faisal Azhar, etÂ al.
    </span>
    <span class="ltx_bibblock">
     Llama: Open and efficient foundation language models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">
      arXiv preprint arXiv:2302.13971
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [8]
    </span>
    <span class="ltx_bibblock">
     Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting
Liu.
    </span>
    <span class="ltx_bibblock">
     Huatuo: Tuning llama model with chinese medical knowledge, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [9]
    </span>
    <span class="ltx_bibblock">
     Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian
Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.
    </span>
    <span class="ltx_bibblock">
     Bloomberggpt: A large language model for finance, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [10]
    </span>
    <span class="ltx_bibblock">
     Hongyang Yang, Xiao-Yang Liu, and ChristinaÂ Dan Wang.
    </span>
    <span class="ltx_bibblock">
     Fingpt: Open-source financial large language models, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [11]
    </span>
    <span class="ltx_bibblock">
     Ping Yang, Junjie Wang, Ruyi Gan, Xinyu Zhu, Lin Zhang, Ziwei Wu, Xinyu Gao,
Jiaxing Zhang, and Tetsuya Sakai.
    </span>
    <span class="ltx_bibblock">
     Zero-shot learners for natural language understanding via a unified
multiple choice perspective, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [12]
    </span>
    <span class="ltx_bibblock">
     Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
Yang, Yifan Xu, Wendi Zheng, Xiao Xia, WengÂ Lam Tam, Zixuan Ma, Yufei Xue,
Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie
Tang.
    </span>
    <span class="ltx_bibblock">
     GLM-130b: An open bilingual pre-trained model.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">
      The Eleventh International Conference on Learning
Representations (ICLR)
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [13]
    </span>
    <span class="ltx_bibblock">
     Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang, ZiÂ Lin, Zhuohan Li, Dacheng Li, Eric.Â P Xing, Hao Zhang, JosephÂ E.
Gonzalez, and Ion Stoica.
    </span>
    <span class="ltx_bibblock">
     Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [14]
    </span>
    <span class="ltx_bibblock">
     Wei Zhu and Xiaoling Wang.
    </span>
    <span class="ltx_bibblock">
     Chatmed: A chinese medical large language model.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/michael-wzhu/ChatMed" target="_blank" title="">
      https://github.com/michael-wzhu/ChatMed
     </a>
     , 2023.
    </span>
   </li>
  </ul>
 </section>
</article>

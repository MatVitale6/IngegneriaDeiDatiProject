{
  "S5.T1": {
    "caption": "OpenAGI task-solving performances under different settings for three closed-source LLMs. Boldface denotes the highest score under each learning schema.",
    "table": "<table border=\"1\" style=\"border-collapse: collapse; text-align: center; width: 100%;\">  <thead>    <tr>      <th rowspan=\"2\">Metrics<\/th>      <th colspan=\"2\">GPT-3.5-turbo<\/th>      <th colspan=\"2\">Claude-2<\/th>      <th colspan=\"2\">GPT-4<\/th>    <\/tr>    <tr>      <th>Zero<\/th>      <th>Few<\/th>      <th>Zero<\/th>      <th>Few<\/th>      <th>Zero<\/th>      <th>Few<\/th>    <\/tr>  <\/thead>  <tbody>    <tr>      <td style=\"text-align: left;\">CLIP Score<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.2543<\/td>      <td>0.0<\/td>      <td>0.3055<\/td>    <\/tr>    <tr>      <td style=\"text-align: left;\">BERT Score<\/td>      <td>0.1914<\/td>      <td>0.3820<\/td>      <td>0.2111<\/td>      <td>0.5038<\/td>      <td>0.2076<\/td>      <td>0.6307<\/td>    <\/tr>    <tr>      <td style=\"text-align: left;\">ViT Score<\/td>      <td>0.2437<\/td>      <td>0.7497<\/td>      <td>0.4082<\/td>      <td>0.5416<\/td>      <td>0.5058<\/td>      <td>0.6480<\/td>    <\/tr>    <tr>      <td style=\"text-align: left;\">Overall<\/td>      <td>0.1450<\/td>      <td>0.3772<\/td>      <td>0.2064<\/td>      <td>0.4332<\/td>      <td><b>0.2378<\/b><\/td>      <td><b>0.5281<\/b><\/td>    <\/tr>  <\/tbody><\/table>",
    "footnotes": [],
    "references": [
      "The main experimental results are tabulated in Tab.                  1                and                  2                , showcasing the results for closed-source and open-source LLMs, respectively. The overall performance is calculated as the average of CLIP, BERT and ViT scores. Here, only the task descriptions of the benchmark tasks are fed into LLMs (additional information, such as the input prompt and LLMs\u2019 outputs, is provided in Fig.                  A.4                and                  A.5                in supplementary). Broadly speaking, closed-source LLMs demonstrate superior performance on OpenAGI tasks, with GPT-4 leading the pack under both zero- and few-shot scenarios. In the open-source category, LLaMA-2-13B takes the lead, consistently posting top results across various learning schema\u2014the performance possibly influenced by its larger model size. Notably, open-source LLMs significantly benefit from the tuning methods, particularly Fine-tuning and RLTF. These methods mark noticeable enhancements for Flan-T5-Large, Vicuna-7B, and LLaMA-2-13B when compared with zero-shot and few-shot learning schema. In fact, each of these open-source models hits its pinnacle under the RLTF approach. Conclusively, with RLTF tuning, the performance of LLaMA-2-13B approaches that of GPT-3.5, illustrating its potential.",
      "We design two types of prompts combined with different levels of model description to test LLMs\u2019 zero-shot performances. The first, Prompt-1, only combines the task description with the model names, while the second, Prompt-2, integrates the task description with comprehensive model descriptions, detailing model usage, input, and output types (additional information about these two prompts is provided in Fig.                  A.6                in supplementary).We analyze the results in Tab.                  3                and                  4                in conjunction with the previous zero-shot results in Tab.                  1                and                  2                . Compared to the original prompt that only uses task description to generate the results in Tab.                  1                and                  2                , it is evident that in most cases, the closed-source LLMs, such as GPT series and Claude-2, tend to outperform when provided with detailed model-related information as seen in Prompt-1 and Prompt-2. In contrast, open-source LLMs, whose understanding and reasoning capacity may be weaker than those huge closed-source models, appear to be misled by the ambiguous details in Prompt-1 and Prompt-2 during the model selection process.Overall, detailed prompts can assist in improving the zero-shot performance to a certain degree, depending on the specific model. However, they may not be as potent as other training scenarios for smaller size LLMs, such as fine-tuning or RLTF."
    ]
  },
  "S5.T2": {
    "caption": "OpenAGI task-solving performances under different settings for three open-source LLMs. Boldface denotes the highest score under each learning schema.",
    "table": "<table border=\"1\">  <thead>    <tr>      <th rowspan=\"2\">Metrics<\/th>      <th colspan=\"4\">Flan-T5-Large<\/th>      <th colspan=\"4\">Vicuna-7B<\/th>      <th colspan=\"4\">LLaMA-2-13B<\/th>    <\/tr>    <tr>      <th>Zero<\/th>      <th>Few<\/th>      <th>Fine-tuning<\/th>      <th>RLTF<\/th>      <th>Zero<\/th>      <th>Few<\/th>      <th>Fine-tuning<\/th>      <th>RLTF<\/th>      <th>Zero<\/th>      <th>Few<\/th>      <th>Fine-tuning<\/th>      <th>RLTF<\/th>    <\/tr>  <\/thead>  <tbody>    <tr>      <td>CLIP Score<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.0612<\/td>      <td>0.0608<\/td>      <td>0.1220<\/td>    <\/tr>    <tr>      <td>BERT Score<\/td>      <td>0.0<\/td>      <td>0.2488<\/td>      <td>0.0<\/td>      <td>0.0655<\/td>      <td>0.0513<\/td>      <td>0.0<\/td>      <td>0.1212<\/td>      <td>0.1756<\/td>      <td>0.0986<\/td>      <td>0.2281<\/td>      <td>0.1570<\/td>      <td>0.2401<\/td>    <\/tr>    <tr>      <td>ViT Score<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.6316<\/td>      <td>0.6978<\/td>      <td>0.1704<\/td>      <td>0.4285<\/td>      <td>0.5507<\/td>      <td>0.7300<\/td>      <td>0.3614<\/td>      <td>0.2558<\/td>      <td>0.6723<\/td>      <td>0.7584<\/td>    <\/tr>    <tr>      <td>Overall<\/td>      <td>0.0<\/td>      <td>0.0829<\/td>      <td>0.2105<\/td>      <td>0.2544<\/td>      <td>0.0739<\/td>      <td>0.1428<\/td>      <td>0.2239<\/td>      <td>0.3018<\/td>      <td>0.1533<\/td>      <td>0.1817<\/td>      <td>0.2967<\/td>      <td>0.3735<\/td>    <\/tr>  <\/tbody><\/table>",
    "footnotes": [],
    "references": [
      "The main experimental results are tabulated in Tab.                  1                and                  2                , showcasing the results for closed-source and open-source LLMs, respectively. The overall performance is calculated as the average of CLIP, BERT and ViT scores. Here, only the task descriptions of the benchmark tasks are fed into LLMs (additional information, such as the input prompt and LLMs\u2019 outputs, is provided in Fig.                  A.4                and                  A.5                in supplementary). Broadly speaking, closed-source LLMs demonstrate superior performance on OpenAGI tasks, with GPT-4 leading the pack under both zero- and few-shot scenarios. In the open-source category, LLaMA-2-13B takes the lead, consistently posting top results across various learning schema\u2014the performance possibly influenced by its larger model size. Notably, open-source LLMs significantly benefit from the tuning methods, particularly Fine-tuning and RLTF. These methods mark noticeable enhancements for Flan-T5-Large, Vicuna-7B, and LLaMA-2-13B when compared with zero-shot and few-shot learning schema. In fact, each of these open-source models hits its pinnacle under the RLTF approach. Conclusively, with RLTF tuning, the performance of LLaMA-2-13B approaches that of GPT-3.5, illustrating its potential.",
      "We design two types of prompts combined with different levels of model description to test LLMs\u2019 zero-shot performances. The first, Prompt-1, only combines the task description with the model names, while the second, Prompt-2, integrates the task description with comprehensive model descriptions, detailing model usage, input, and output types (additional information about these two prompts is provided in Fig.                  A.6                in supplementary).We analyze the results in Tab.                  3                and                  4                in conjunction with the previous zero-shot results in Tab.                  1                and                  2                . Compared to the original prompt that only uses task description to generate the results in Tab.                  1                and                  2                , it is evident that in most cases, the closed-source LLMs, such as GPT series and Claude-2, tend to outperform when provided with detailed model-related information as seen in Prompt-1 and Prompt-2. In contrast, open-source LLMs, whose understanding and reasoning capacity may be weaker than those huge closed-source models, appear to be misled by the ambiguous details in Prompt-1 and Prompt-2 during the model selection process.Overall, detailed prompts can assist in improving the zero-shot performance to a certain degree, depending on the specific model. However, they may not be as potent as other training scenarios for smaller size LLMs, such as fine-tuning or RLTF."
    ]
  },
  "S5.T3": {
    "caption": "Zero-shot task-solving performances under various prompts for three closed-source LLMs.",
    "table": "<table border=\"1\">  <thead>    <tr>      <th rowspan=\"2\">Metrics<\/th>      <th colspan=\"2\">GPT-3.5-turbo<\/th>      <th colspan=\"2\">Claude-2<\/th>      <th colspan=\"2\">GPT-4<\/th>    <\/tr>    <tr>      <th>Prompt-1<\/th>      <th>Prompt-2<\/th>      <th>Prompt-1<\/th>      <th>Prompt-2<\/th>      <th>Prompt-1<\/th>      <th>Prompt-2<\/th>    <\/tr>  <\/thead>  <tbody>    <tr>      <td>CLIP Score<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.0<\/td>      <td>0.0<\/td>    <\/tr>    <tr>      <td>BERT Score<\/td>      <td>0.2106<\/td>      <td>0.3013<\/td>      <td>0.4088<\/td>      <td>0.2333<\/td>      <td>0.4402<\/td>      <td>0.5595<\/td>    <\/tr>    <tr>      <td>ViT Score<\/td>      <td>0.0<\/td>      <td>0.2710<\/td>      <td>0.6816<\/td>      <td>0.7957<\/td>      <td>0.5497<\/td>      <td>0.5565<\/td>    <\/tr>    <tr>      <td>Overall<\/td>      <td>0.0702<\/td>      <td>0.1907<\/td>      <td>0.3635<\/td>      <td>0.3430<\/td>      <td>0.3299<\/td>      <td>0.3717<\/td>    <\/tr>  <\/tbody><\/table>",
    "footnotes": [],
    "references": [
      "We design two types of prompts combined with different levels of model description to test LLMs\u2019 zero-shot performances. The first, Prompt-1, only combines the task description with the model names, while the second, Prompt-2, integrates the task description with comprehensive model descriptions, detailing model usage, input, and output types (additional information about these two prompts is provided in Fig.                  A.6                in supplementary).We analyze the results in Tab.                  3                and                  4                in conjunction with the previous zero-shot results in Tab.                  1                and                  2                . Compared to the original prompt that only uses task description to generate the results in Tab.                  1                and                  2                , it is evident that in most cases, the closed-source LLMs, such as GPT series and Claude-2, tend to outperform when provided with detailed model-related information as seen in Prompt-1 and Prompt-2. In contrast, open-source LLMs, whose understanding and reasoning capacity may be weaker than those huge closed-source models, appear to be misled by the ambiguous details in Prompt-1 and Prompt-2 during the model selection process.Overall, detailed prompts can assist in improving the zero-shot performance to a certain degree, depending on the specific model. However, they may not be as potent as other training scenarios for smaller size LLMs, such as fine-tuning or RLTF."
    ]
  }
}
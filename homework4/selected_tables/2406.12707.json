{
  "S4.T1": {
    "caption": "Performance evaluation of PerceptiveAgent. BERTScore (%) measures the quality of cognitive empathy in linguistic contents, while accuracy (%) assesses the quality of affective empathy in acoustic responses.",
    "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\"><thead class=\"ltx_thead\"><tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\"><th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"><\/th><th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BERTScore<\/th><th id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Accuracy<\/th><\/tr><\/thead><tbody class=\"ltx_tbody\"><tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\"><th id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\">Speech-GPT3.5<\/th><td id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">53.03\u00B110.20<\/td><td id=\"S4.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.74<\/td><\/tr><tr id=\"S4.T1.1.3.2\" class=\"ltx_tr\"><th id=\"S4.T1.1.3.2.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">PerceptiveAgent<\/th><td id=\"S4.T1.1.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.3.2.2.1\" class=\"ltx_text ltx_font_bold\">54.36\u00B19.25<\/span><\/td><td id=\"S4.T1.1.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.3.2.3.1\" class=\"ltx_text ltx_font_bold\">21.89<\/span><\/td><\/tr><tr id=\"S4.T1.1.4.3\" class=\"ltx_tr\"><th id=\"S4.T1.1.4.3.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb\">-w\/o captions<\/th><td id=\"S4.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">-<\/td><td id=\"S4.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">16.53<\/td><\/tr><\/tbody><\/table>",
    "footnotes": [],
    "references": [
      "Table 1 presents the overall performance of PerceptiveAgent on cognitive empathy and affective empathy, evaluated on the generated content and audio, respectively. BERTScore measures the semantic similarity between the generated and real response contents, while accuracy assesses the similarity and diversity of emotions between the generated and real speeches. Overall, compared to Speech-GPT3.5, PerceptiveAgent demonstrates a strong ability in generating empathetic responses with a closer alignment to the dialogue context in terms of linguistic content and a higher expressiveness in acoustic information. Specifically, PerceptiveAgent achieves a slightly higher BERTScore than Speech-GPT3.5, primarily because our model can generate content that more accurately captures the speaker’s intentions and contains more emotionally intense words. Additionally, PerceptiveAgent notably outperforms Speech-GPT3.5 in terms of accuracy, as the latter doesn’t incorporate any emotion prompts during speech generation, thus maintaining a limited variety of prosody. Despite this, the accuracy of PerceptiveAgent still remains at a relatively moderate level. This is because the generated responses, while contextually appropriate, may not entirely align with the real responses in terms of semantics and emotions.",
      "Effectiveness of Captions. The last line in Table 1 demonstrates the effectiveness of captions in PerceptiveAgent. The system without captions synthesizes speech using randomly selected labels for all four speaking attributes (pitch, speed, energy, and emotion), while maintaining the same response contents as the PerceptiveAgent. It is evident that the PerceptiveAgent outperforms the system without captions, highlighting the effectiveness of captions in generating speech with affective empathy."
    ]
  },
  "S4.T2": {
    "caption": "Performance evaluation of the speech captioner. Precision, recall and F1-score (%) are utilized to measure its generalization ability on both the validation and test sets. Predicted labels are obtained through semantic classification on the generated captions, while the true labels are derived from the TextroSpeech dataset.",
    "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\"><thead class=\"ltx_thead\"><tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\"><th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"><\/th><th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BERTScore<\/th><th id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Accuracy<\/th><\/tr><\/thead><tbody class=\"ltx_tbody\"><tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\"><th id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\">Speech-GPT3.5<\/th><td id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">53.03\u00B110.20<\/td><td id=\"S4.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.74<\/td><\/tr><tr id=\"S4.T1.1.3.2\" class=\"ltx_tr\"><th id=\"S4.T1.1.3.2.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">PerceptiveAgent<\/th><td id=\"S4.T1.1.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.3.2.2.1\" class=\"ltx_text ltx_font_bold\">54.36\u00B19.25<\/span><\/td><td id=\"S4.T1.1.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.3.2.3.1\" class=\"ltx_text ltx_font_bold\">21.89<\/span><\/td><\/tr><tr id=\"S4.T1.1.4.3\" class=\"ltx_tr\"><th id=\"S4.T1.1.4.3.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb\">-w\/o captions<\/th><td id=\"S4.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">-<\/td><td id=\"S4.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">16.53<\/td><\/tr><\/tbody><\/table>",
    "footnotes": [],
    "references": [
      "Table 2 evaluates the speech captioner's generalization performance on both the validation and test sets. Overall, it is evident that that the model achieves the highest F1-score for gender, followed by pitch and emotion. This underscores the model’s proficiency in accurately discerning these attributes from input speech. Besides, both gender and emotion exhibit closely aligned precision and recall metrics, affirming the model's predictive prowess for these attributes. Meanwhile, there exists a notable disparity between precision and recall when predicting energy, indicating variable performance and a tendency towards confident predictions. Conversely, the model’s performance in predicting speed is unsatisfactory, which can be attributed to the imbalanced distribution of speed in the training dataset, with over 60% of samples labeled as “neutral”."
    ]
  },
  "S4.T3": {
    "caption": "Comparison of the speech captioner's performance across genders.",
    "table": "<table id=\"S4.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\"><thead class=\"ltx_thead\"><tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\"><th id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T3.1.1.1.1.1\" class=\"ltx_text\">Attribute<\/span><\/th><th id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">Male<\/th><th id=\"S4.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">Female<\/th><\/tr><tr id=\"S4.T3.1.2.2\" class=\"ltx_tr\"><th id=\"S4.T3.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Precision<\/th><th id=\"S4.T3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Recall<\/th><th id=\"S4.T3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">F1-score<\/th><th id=\"S4.T3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Precision<\/th><th id=\"S4.T3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Recall<\/th><th id=\"S4.T3.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">F1-score<\/th><\/tr><\/thead><tbody class=\"ltx_tbody\"><tr id=\"S4.T3.1.3.1\" class=\"ltx_tr\"><th id=\"S4.T3.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Emotion<\/th><td id=\"S4.T3.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">84.3<\/td><td id=\"S4.T3.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">85.4<\/td><td id=\"S4.T3.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">84.2<\/td><td id=\"S4.T3.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">87.4<\/td><td id=\"S4.T3.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">85.5<\/td><td id=\"S4.T3.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">86.0<\/td><\/tr><tr id=\"S4.T3.1.4.2\" class=\"ltx_tr\"><th id=\"S4.T3.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Pitch<\/th><td id=\"S4.T3.1.4.2.2\" class=\"ltx_td ltx_align_center\">88.2<\/td><td id=\"S4.T3.1.4.2.3\" class=\"ltx_td ltx_align_center\">82.8<\/td><td id=\"S4.T3.1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">85.3<\/td><td id=\"S4.T3.1.4.2.5\" class=\"ltx_td ltx_align_center\">84.8<\/td><td id=\"S4.T3.1.4.2.6\" class=\"ltx_td ltx_align_center\">71.0<\/td><td id=\"S4.T3.1.4.2.7\" class=\"ltx_td ltx_align_center\">75.9<\/td><\/tr><tr id=\"S4.T3.1.5.3\" class=\"ltx_tr\"><th id=\"S4.T3.1.5.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Energy<\/th><td id=\"S4.T3.1.5.3.2\" class=\"ltx_td ltx_align_center\">74.4<\/td><td id=\"S4.T3.1.5.3.3\" class=\"ltx_td ltx_align_center\">60.0<\/td><td id=\"S4.T3.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">65.0<\/td><td id=\"S4.T3.1.5.3.5\" class=\"ltx_td ltx_align_center\">71.2<\/td><td id=\"S4.T3.1.5.3.6\" class=\"ltx_td ltx_align_center\">54.9<\/td><td id=\"S4.T3.1.5.3.7\" class=\"ltx_td ltx_align_center\">60.9<\/td><\/tr><tr id=\"S4.T3.1.6.4\" class=\"ltx_tr\"><th id=\"S4.T3.1.6.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Speed<\/th><td id=\"S4.T3.1.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">46.4<\/td><td id=\"S4.T3.1.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">43.1<\/td><td id=\"S4.T3.1.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">44.6<\/td><td id=\"S4.T3.1.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">48.0<\/td><td id=\"S4.T3.1.6.4.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">30.6<\/td><td id=\"S4.T3.1.6.4.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">37.3<\/td><\/tr><\/tbody><\/table>",
    "footnotes": [],
    "references": [
      "We also discuss how errors in speech processing are affected by demographics of the speakers. Table  3 compares the performance of the speech captioner across genders, which represents the most prevalent factor. The F1-score on male speech surpasses that on female speech in terms of pitch, energy and speed, despite the comparable sample sizes for male and female groups (8634 VS. 8983). This demonstrates a variation in the model's performance depending on the gender of the speakers."
    ]
  }
}
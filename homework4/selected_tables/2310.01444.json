{
  "S4.T3.1": {
    "caption": "EM scores on HotpotQA with prompt and tuning methods. Methods that use fine-tuning are marked by “-Tuning”.",
    "table": "<table border=\"1\">  <thead>    <tr>      <th>Model<\/th>      <th>Method<\/th>      <th>EM score<\/th>    <\/tr>  <\/thead>  <tbody>    <tr>      <td rowspan=\"4\">PaLM-540B<\/td>      <td>CoT [61]<\/td>      <td>29.4<\/td>    <\/tr>    <tr>      <td>CoT-SC [55]<\/td>      <td>33.4<\/td>    <\/tr>    <tr>      <td>ReAct [65]<\/td>      <td>27.4<\/td>    <\/tr>    <tr>      <td>ReAct \u2192 CoT-SC<\/td>      <td>35.1<\/td>    <\/tr>    <tr>      <td>GPT3-175B<\/td>      <td>ReAct<\/td>      <td>30.8<\/td>    <\/tr>    <tr>      <td rowspan=\"2\">PaLM-62B<\/td>      <td>ReAct-Tuning<\/td>      <td>32.6<\/td>    <\/tr>    <tr>      <td>CoT-Tuning<\/td>      <td>25.2<\/td>    <\/tr>    <tr>      <td rowspan=\"2\">PaLM-8B<\/td>      <td>ReAct-Tuning<\/td>      <td>25.0<\/td>    <\/tr>    <tr>      <td>CoT-Tuning<\/td>      <td>14.1<\/td>    <\/tr>    <tr>      <td rowspan=\"2\">LLaMA-7B<\/td>      <td>ReAct-Tuning<\/td>      <td>28.1<\/td>    <\/tr>    <tr>      <td>LTC<\/td>      <td><b>33.2<\/b><\/td>    <\/tr>  <\/tbody><\/table>",
    "footnotes": [],
    "references": [
      "As shown in Table 3, LTC outperforms the instruction tuning baseline† by 5.1% on Exact Match (EM) score, and it even outperforms ReAct and CoT on their default settings. Note that ReAct and CoT use PaLM-540B and GPT3-175B as the pre-trained LM model, which is 77x and 25x larger than our the LLaMA-7B model we used. By sampling 21 CoT trajectories during inference and adopting the majority answer, CoT-SC is slightly better (0.2%) than LTC, and their combined method ReAct → CoT-SC surpasses LTC by 1.9%. Compared to other models with tuning, our LLaMA-7B based agent even obtains slightly better (0.6%) performance than the ReAct-Tuning baseline with 9* larger PaLM-62B model."
    ]
  },
  "S5.T4": {
    "caption": "Average number of tokens of the input prompts on test sets. LTC does not use any few shot examples in the prompt, hence uses only a fraction of tokens compared to ICL.",
    "table": "<table id=\"S5.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\"><thead class=\"ltx_thead\"><tr id=\"S5.T4.1.1.1\" class=\"ltx_tr\"><th id=\"S5.T4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T4.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Method<\/span><\/th><th id=\"S5.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T4.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">GSM8k<\/span><\/th><th id=\"S5.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T4.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Hotpot-QA<\/span><\/th><th id=\"S5.T4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T4.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Alfworld<\/span><\/th><\/tr><tr id=\"S5.T4.1.2.2\" class=\"ltx_tr\"><th id=\"S5.T4.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">(CoT)<\/th><th id=\"S5.T4.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">(ReAct)<\/th><th id=\"S5.T4.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">(ReAct)<\/th><\/tr><\/thead><tbody class=\"ltx_tbody\"><tr id=\"S5.T4.1.3.1\" class=\"ltx_tr\"><th id=\"S5.T4.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S5.T4.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">ICL<\/span><\/th><td id=\"S5.T4.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">836<\/td><td id=\"S5.T4.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1937<\/td><td id=\"S5.T4.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1744<\/td><\/tr><tr id=\"S5.T4.1.4.2\" class=\"ltx_tr\"><th id=\"S5.T4.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span id=\"S5.T4.1.4.2.1.1\" class=\"ltx_text ltx_font_bold\">LTC<\/span><\/th><td id=\"S5.T4.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T4.1.4.2.2.1\" class=\"ltx_text ltx_font_bold\">107<\/span><\/td><td id=\"S5.T4.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T4.1.4.2.3.1\" class=\"ltx_text ltx_font_bold\">167<\/span><\/td><td id=\"S5.T4.1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T4.1.4.2.4.1\" class=\"ltx_text ltx_font_bold\">189<\/span><\/td><\/tr><\/tbody><\/table>",
    "footnotes": [],
    "references": [
      "As mentioned above, prompting-based methods such as ReAct [65] and CoT [61] use a subset of exemplary trajectories from the given task as few-shot prompts during inference. However, these few-shot prompts are often long, which leads to increased inference cost and limited context length for user queries. As shown in Table 4, we compare the number of input tokens for each task. We compute the CoT prompts for GSM8k, and we use ReAct for the other two tasks. All the few-shot prompts are sourced from the original paper. As shown, our LTC agents used only 12.8%, 8.6%, and 10.8% of the input tokens required by the ICL methods on the three tasks, respectively."
    ]
  }
}
{
  "S4.T3.1": {
    "caption": "AlfWorld success rates (%) for 6 tasks. The results of the bottom block are obtained by fine-tuning LLaMA-7B model.",
    "table": "<table border=\"1\">  <thead>    <tr>      <th>Model<\/th>      <th>Method<\/th>      <th>EM score<\/th>    <\/tr>  <\/thead>  <tbody>    <tr>      <td rowspan=\"4\">PaLM-540B<\/td>      <td>CoT [61]<\/td>      <td>29.4<\/td>    <\/tr>    <tr>      <td>CoT-SC [55]<\/td>      <td>33.4<\/td>    <\/tr>    <tr>      <td>ReAct [65]<\/td>      <td>27.4<\/td>    <\/tr>    <tr>      <td>ReAct \u2192 CoT-SC<\/td>      <td>35.1<\/td>    <\/tr>    <tr>      <td>GPT3-175B<\/td>      <td>ReAct<\/td>      <td>30.8<\/td>    <\/tr>    <tr>      <td rowspan=\"2\">PaLM-62B<\/td>      <td>ReAct-Tuning<\/td>      <td>32.6<\/td>    <\/tr>    <tr>      <td>CoT-Tuning<\/td>      <td>25.2<\/td>    <\/tr>    <tr>      <td rowspan=\"2\">PaLM-8B<\/td>      <td>ReAct-Tuning<\/td>      <td>25.0<\/td>    <\/tr>    <tr>      <td>CoT-Tuning<\/td>      <td>14.1<\/td>    <\/tr>    <tr>      <td rowspan=\"2\">LLaMA-7B<\/td>      <td>ReAct-Tuning<\/td>      <td>28.1<\/td>    <\/tr>    <tr>      <td>LTC<\/td>      <td><b>33.2<\/b><\/td>    <\/tr>  <\/tbody><\/table>",
    "footnotes": [],
    "references": [
      "As shown in Table 1, LTC outperforms the previous best methods * * * For ALFWorld, ReAct and ReAct-IM results are from Table 3 of [65]. BUTLER and BUTLERg results are from Table 4 of [46], and they are trained with DAgger                   [                     38                    ]                  .                           on all of tasks of             ALFWorld            . We can see that Instruction Fine-tuning is already a strong baseline outperforming others, yet our LTC achieves a success rate of 91%, remarkably outperforming the best Instruction Tuning baseline (78%).Notably, on both Cool and Look tasks, LTC obtains a 100% success rate.Even on the hardest Pick Two & Place task (e.g., \u201cput two pencils in the drawer\u201d), it achieves a decent 76% success rate.The Pick Two task requires the agent to perform two sequences of \u201dpick and place\u201d actions in one task, while keeping track of the desired type and the location.The combined sequences and the need to remember the previous location make this task challenging.This may be the reason why baselines achieve lower success rates on this task.In contrast, our LTC agent, which further trains the agent with self-exploration significantly outperforms other agents. This underscores the effectiveness of the communication mechanism in LTC."
    ]
  },
  "S5.T4": {
    "caption": "EM scores on HotpotQA with prompt and tuning methods. Methods that use fine-tuning are marked by \u201c-Tuning\u201d.Accuracy on GSM8k. The results of the bottom block are obtained by fine-tuning LLaMA-7B model, while the others are prompting methods without fine-tuning.",
    "table": "<table id=\"S5.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\"><thead class=\"ltx_thead\"><tr id=\"S5.T4.1.1.1\" class=\"ltx_tr\"><th id=\"S5.T4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T4.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Method<\/span><\/th><th id=\"S5.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T4.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">GSM8k<\/span><\/th><th id=\"S5.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T4.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Hotpot-QA<\/span><\/th><th id=\"S5.T4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T4.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Alfworld<\/span><\/th><\/tr><tr id=\"S5.T4.1.2.2\" class=\"ltx_tr\"><th id=\"S5.T4.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">(CoT)<\/th><th id=\"S5.T4.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">(ReAct)<\/th><th id=\"S5.T4.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">(ReAct)<\/th><\/tr><\/thead><tbody class=\"ltx_tbody\"><tr id=\"S5.T4.1.3.1\" class=\"ltx_tr\"><th id=\"S5.T4.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S5.T4.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">ICL<\/span><\/th><td id=\"S5.T4.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">836<\/td><td id=\"S5.T4.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1937<\/td><td id=\"S5.T4.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1744<\/td><\/tr><tr id=\"S5.T4.1.4.2\" class=\"ltx_tr\"><th id=\"S5.T4.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span id=\"S5.T4.1.4.2.1.1\" class=\"ltx_text ltx_font_bold\">LTC<\/span><\/th><td id=\"S5.T4.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T4.1.4.2.2.1\" class=\"ltx_text ltx_font_bold\">107<\/span><\/td><td id=\"S5.T4.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T4.1.4.2.3.1\" class=\"ltx_text ltx_font_bold\">167<\/span><\/td><td id=\"S5.T4.1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T4.1.4.2.4.1\" class=\"ltx_text ltx_font_bold\">189<\/span><\/td><\/tr><\/tbody><\/table>",
    "footnotes": [],
    "references": [
      "As shown in Table 3, LTC outperforms the instruction tuning baseline \u2020 \u2020 \u2020 For HotPotQA, Prompting method results without fine-tuning are from Table 1&5 of                   [                     65                    ]                  . PaLM-8B and PaLM-62B scores are estimates from Figure 3 of                   [                     65                    ]                  .                           by 5.1% on Exact Match (EM) score, and it even outperforms ReAct and CoT on their default settings. Note that ReAct and CoT use PaLM-540B and GPT3-175B as the pre-trained LM model, which is 77x and 25x larger than our the LLaMA-7B model we used. By sampling 21 CoT trajectories during inference and adopting the majority answer, CoT-SC is slightly better (0.2%) than LTC, and their combined method ReAct                              \u2192                                   \u2192                                  \\to                           CoT-SC surpasses LTC by 1.9%. Compared to other models with tuning, our LLaMA-7B based agent even obtains slightly better (0.6%) performance than the ReAct-Tuning baseline with 9                              \u00d7                                                           \\times                           larger PaLM-62B model.",
      "As shown in Table 3, LTC outperforms the instruction fine-tuning baseline by 3.6% on accuracy.It also outperforms CoT and CoT-SC baselines with the same LLaMA-7B model.However, LTC underperforms CoT and CoT-SC with the much larger models (PaLM-540B and GPT3-175B).This phenomenon is because numerical reasoning requires a larger model size and sufficient pretraining data, as observed in             [               28              ]            .Unfortunately, due to computational resource limitations, we can only train the relatively small LLaMA-7B model but were unable to train larger-scale models.Nevertheless, we believe that exploring LTC with larger models is promising for future research."
    ]
  }
}
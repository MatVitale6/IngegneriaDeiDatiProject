
Consider the claims being in the form |{Specification, Specification, …}, Measure, Outcome|

Claims must be extracted according to the following format:
|{Specification, Specification, …}, Measure, Outcome|
Specification: |name, value| pair describing the details of an experiment
E.g.: |dataset, Spider|, or |LLM, Llama27b|
Measure: metric or measure used to evaluate the experiment
E.g.: F1-measure
Outcome: outcome value related to metric 
E.g.: 0.89

The format have to follows this examples:
Claim 0: |{|Model Type, General LLM|, |Model Name, ChatGPT-3.5-turbo|, |Parameter Size, 175B|, |Dataset, Spider dev|, |Difficulty Level, 1|}, Execution Match , 0.760|
Claim 1:| .... |

Extract all the claims, even with empty value for the measure in that exact form from the following html table:

<table border="1"> <thead> <tr> <th rowspan="2">Metrics</th> <th colspan="2">GPT-3.5-turbo</th> <th colspan="2">Claude-2</th> <th colspan="2">GPT-4</th> </tr> <tr> <th>Prompt-1</th> <th>Prompt-2</th> <th>Prompt-1</th> <th>Prompt-2</th> <th>Prompt-1</th> <th>Prompt-2</th> </tr> </thead> <tbody> <tr> <td>CLIP Score</td> <td>0.0</td> <td>0.0</td> <td>0.0</td> <td>0.0</td> <td>0.0</td> <td>0.0</td> </tr> <tr> <td>BERT Score</td> <td>0.2106</td> <td>0.3013</td> <td>0.4088</td> <td>0.2333</td> <td>0.4402</td> <td>0.5595</td> </tr> <tr> <td>ViT Score</td> <td>0.0</td> <td>0.2710</td> <td>0.6816</td> <td>0.7957</td> <td>0.5497</td> <td>0.5565</td> </tr> <tr> <td>Overall</td> <td>0.0702</td> <td>0.1907</td> <td>0.3635</td> <td>0.3430</td> <td>0.3299</td> <td>0.3717</td> </tr> </tbody></table>


Know that the context where the table was mentioned is the following:

Zero-shot task-solving performances under various prompts for three closed-source LLMs.

We design two types of prompts combined with different levels of model description to test LLMs’ zero-shot performances. The first, Prompt-1, only combines the task description with the model names, while the second, Prompt-2, integrates the task description with comprehensive model descriptions, detailing model usage, input, and output types (additional information about these two prompts is provided in Fig. A.6 in supplementary).We analyze the results in Tab. 3 and 4 in conjunction with the previous zero-shot results in Tab. 1 and 2 . Compared to the original prompt that only uses task description to generate the results in Tab. 1 and 2 , it is evident that in most cases, the closed-source LLMs, such as GPT series and Claude-2, tend to outperform when provided with detailed model-related information as seen in Prompt-1 and Prompt-2. In contrast, open-source LLMs, whose understanding and reasoning capacity may be weaker than those huge closed-source models, appear to be misled by the ambiguous details in Prompt-1 and Prompt-2 during the model selection process.Overall, detailed prompts can assist in improving the zero-shot performance to a certain degree, depending on the specific model. However, they may not be as potent as other training scenarios for smaller size LLMs, such as fine-tuning or RLTF.
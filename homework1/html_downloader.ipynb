{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOI Scouting (article id) and HTML Download\n",
    "Questa sezione esegue una query su arxiv.org cercando articoli che parlano di **LLM multi agent** e attraverso *bs4* ottiene i vari [DOI](https://en.wikipedia.org/wiki/Digital_object_identifier).\n",
    "Con questi DOI è dunque possibile eseguire richieste specifiche per scaricare i documenti HTML del topic scelto.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Costants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import time\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)\n",
    "\n",
    "SOURCES_DIR = 'sources/'\n",
    "\n",
    "if not os.path.exists(SOURCES_DIR):\n",
    "    os.makedirs(f'./{SOURCES_DIR}', exist_ok=True)\n",
    "\n",
    "article_ids = set()\n",
    "TOPIC = \"LLM multi agent\"\n",
    "found_urls = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(article, article_id):\n",
    "    file_path = os.path.join(SOURCES_DIR, f\"ar5iv_article_{article_id}.html\") \n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(article.prettify())\n",
    "    logging.info(f\"Article {article_id} saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ar5iv_html(article_id):\n",
    "    urls = [ f\"https://ar5iv.labs.arxiv.org/html/{article_id}\",\n",
    "             f\"https://arxiv.org/html/{article_id}\" ]               # sometimes html is provided on this url when it's not showing on ar5ive\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            logging.info(f\"Request URL: {response.url}\")\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                article = soup.find('article')\n",
    "\n",
    "                if article:\n",
    "                    found_urls.add(url)\n",
    "                    return article\n",
    "            else:\n",
    "                logging.error(f\"Error fetching article {article_id}, status code: {response.status_code}\")\n",
    "                return None\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error downloading article {article_id}: {e}\")\n",
    "            return None\n",
    "    with open('read.me', 'w') as f:\n",
    "        f.writelines([url + '\\n' for url in found_urls])\n",
    "        logging.info(f\"Successfully wrote {len(found_urls)} urls to read.me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_article_id(url, params):\n",
    "    response = requests.get(url, params=params)\n",
    "    logging.info(f\"Request URL: {response.url}\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        ol_tag = soup.find('ol')\n",
    "        if ol_tag:\n",
    "            li_tags = ol_tag.find_all('li')\n",
    "            local_article_counter = 0\n",
    "\n",
    "            for li in li_tags:\n",
    "                a_tag = li.find('a')\n",
    "                if a_tag:\n",
    "                    article_id = a_tag.text.split(':')[-1]\n",
    "                    logging.info(f\"Processing article number: {len(article_ids)}: {article_id}\")\n",
    "                    article = get_ar5iv_html(article_id)\n",
    "                    if article:\n",
    "                        logging.info(f\"Article found: {article_id}\")\n",
    "                        local_article_counter += 1\n",
    "                        article_ids.add(article_id)\n",
    "                        save_to_file(article, article_id)\n",
    "                    else:\n",
    "                        logging.warning(f\"Article {article_id} not found, skipping...\")\n",
    "                else:\n",
    "                    logging.warning(\"No <a> element found in the <li> element.\")\n",
    "\n",
    "            logging.info(f\"Number of local articles found: {local_article_counter} over 200\")  # inside a single page\n",
    "            time.sleep(2)\n",
    "            return local_article_counter\n",
    "        else:\n",
    "            logging.warning(\"No <ol> element found.\")\n",
    "            return None\n",
    "    else:\n",
    "        logging.error(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punto di lancio del codice, la query è composta da `url + params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL for the advanced search on arXiv\n",
    "url = \"https://arxiv.org/search/advanced\"\n",
    "\n",
    "# Define the search parameters\n",
    "params = {\n",
    "    \"advanced\": \"\",\n",
    "    \"terms-0-term\": TOPIC,              # Search term\n",
    "    \"terms-0-operator\": \"AND\",          # Operator\n",
    "    \"terms-0-field\": \"all\",             # Search field (all)\n",
    "    \"classification-computer_science\": \"y\",  # Limit to computer science\n",
    "    \"classification-physics_archives\": \"all\",  # Include all physics archives\n",
    "    \"classification-include_cross_list\": \"include\",  # Include cross-list\n",
    "    \"date-filter_by\": \"all_dates\",      # Filter by date (all dates)\n",
    "    \"date-year\": \"\",                    # No specific year\n",
    "    \"date-from_date\": \"\",               # No start date\n",
    "    \"date-to_date\": \"\",                 # No end date\n",
    "    \"date-date_type\": \"submitted_date\", # Search by submission date\n",
    "    \"abstracts\": \"hide\",                # Show abstracts\n",
    "    \"size\": 200,                        # Number of results to display\n",
    "    \"order\": \"submitted_date\"           # Order by submission date (oldest first) #older documents are more likely to have html version\n",
    "}\n",
    "\n",
    "doi_retrieved = 0\n",
    "while doi_retrieved <= 300:\n",
    "    doi_retrieved += retrieve_article_id(url, params)\n",
    "    params[\"start\"] = doi_retrieved\n",
    "\n",
    "logging.info(f\"Number of articles retrieved: {len(article_ids)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  Making LLaMA SEE and Draw with SEED Tokenizer
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text ltx_font_bold" id="id4.4.4">
     Yuying Ge
     <sup class="ltx_sup" id="id4.4.4.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id4.4.4.1.1">
       1‚àó
      </span>
     </sup>
     Sijie Zhao
     <sup class="ltx_sup" id="id4.4.4.2">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id4.4.4.2.1">
       1‚àó
      </span>
     </sup>
     Ziyun Zeng
     <sup class="ltx_sup" id="id4.4.4.3">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id4.4.4.3.1">
       2
      </span>
     </sup>
     Yixiao Ge
     <sup class="ltx_sup" id="id4.4.4.4">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id4.4.4.4.1">
       1,2‚Ä†
      </span>
     </sup>
     <br class="ltx_break"/>
    </span>
    <span class="ltx_text ltx_font_bold" id="id7.7.7">
     Chen Li
     <sup class="ltx_sup" id="id7.7.7.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.7.1.1">
       2
      </span>
     </sup>
     Xintao Wang
     <sup class="ltx_sup" id="id7.7.7.2">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.7.2.1">
       1,2
      </span>
     </sup>
     Ying Shan
     <sup class="ltx_sup" id="id7.7.7.3">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.7.3.1">
       1,2
      </span>
     </sup>
    </span>
    <sup class="ltx_sup" id="id10.10.id1">
     <span class="ltx_text ltx_font_italic" id="id10.10.id1.1">
      1
     </span>
    </sup>
    Tencent AI Lab
    <sup class="ltx_sup" id="id11.11.id2">
     <span class="ltx_text ltx_font_italic" id="id11.11.id2.1">
      2
     </span>
    </sup>
    ARC Lab
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Tencent PCG
    <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/AILab-CVC/SEED" target="_blank" title="">
     https://github.com/AILab-CVC/SEED
    </a>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id12.id1">
   The great success of Large Language Models (LLMs) has expanded the potential of multimodality, contributing to the gradual evolution of General Artificial Intelligence (AGI). A true AGI agent should not only possess the capability to perform predefined multi-tasks but also exhibit emergent abilities in an open-world context. However, despite the considerable advancements made by recent multimodal LLMs, they still fall short in effectively unifying comprehension and generation tasks, let alone open-world emergent abilities. We contend that the key to overcoming the present impasse lies in enabling text and images to be represented and processed interchangeably within a unified autoregressive Transformer. To this end, we introduce
   <span class="ltx_text ltx_font_bold" id="id12.id1.1">
    SEED
   </span>
   , an elaborate image tokenizer that empowers LLMs with the ability to
   <span class="ltx_text ltx_font_bold" id="id12.id1.2">
    SEE
   </span>
   and
   <span class="ltx_text ltx_font_bold" id="id12.id1.3">
    D
   </span>
   raw at the same time. We identify two crucial design principles:
(1) Image tokens should be independent of 2D physical patch positions and instead be produced with a
   <span class="ltx_text ltx_font_italic" id="id12.id1.4">
    1D causal dependency
   </span>
   , exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture
   <span class="ltx_text ltx_font_italic" id="id12.id1.5">
    high-level semantics
   </span>
   consistent with the degree of semantic abstraction in words, and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. With SEED tokens, LLM is able to perform scalable multimodal autoregression under its original training recipe, i.e., next-word prediction. SEED-LLaMA
   <span class="ltx_note ltx_role_footnote" id="footnote1">
    <sup class="ltx_note_mark">
     1
    </sup>
    <span class="ltx_note_outer">
     <span class="ltx_note_content">
      <sup class="ltx_note_mark">
       1
      </sup>
      <span class="ltx_tag ltx_tag_note">
       1
      </span>
      This work is a follow-up of SEED
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib1" title="">
        1
       </a>
       ]
      </cite>
      , where we update the visual tokenizer and present SEED-LLaMA.
     </span>
    </span>
   </span>
   is therefore produced by large-scale pretraining and instruction tuning on the interleaved textual and visual data, demonstrating impressive performance on a broad range of multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has exhibited compositional emergent abilities such as multi-turn in-context multimodal generation, acting like your AI assistant.
  </p>
 </div>
 <figure class="ltx_figure" id="S0.F1">
  <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="188" id="S0.F1.g1" src="/html/2310.01218/assets/x1.png" width="424"/>
  <figcaption class="ltx_caption ltx_centering">
   <span class="ltx_tag ltx_tag_figure">
    Figure 1:
   </span>
   The introduced SEED-LLaMA, a multimodal AI assistant, demonstrates
   <span class="ltx_text ltx_font_bold" id="S0.F1.2.1">
    emergent ability
   </span>
   in the multi-turn in-context image and text generation given multimodal instructions.
  </figcaption>
 </figure>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    In recent years, Large Language Models
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib2" title="">
      2
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib3" title="">
      3
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib4" title="">
      4
     </a>
     ]
    </cite>
    (LLMs) pre-trained on massive text corpus with straightforward training objectives such as next-word prediction have exhibited remarkable abilities to understand, reason, and generate texts across a variety of open-ended tasks. Recent studies further exploit the strong generality of LLMs to improve visual understanding or generation tasks, collectively referred to as Multimodal LLM (MLLM).
While these studies have contributed to technological advancements, MLLMs have yet to achieve the remarkable success of LLMs in terms of emergent capabilities.
We have made a bold assumption that the premise for the emergence of multimodal capabilities is that text and images can be represented and processed
    <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">
     interchangeably
    </span>
    in a unified autoregressive Transformer.
   </p>
  </div>
  <span class="ltx_note ltx_role_footnotetext" id="footnotex1">
   <sup class="ltx_note_mark">
    1
   </sup>
   <span class="ltx_note_outer">
    <span class="ltx_note_content">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_type">
      footnotetext:
     </span>
     Equal Contribution.
    </span>
   </span>
  </span>
  <span class="ltx_note ltx_role_footnotetext" id="footnotex2">
   <sup class="ltx_note_mark">
    2
   </sup>
   <span class="ltx_note_outer">
    <span class="ltx_note_content">
     <sup class="ltx_note_mark">
      2
     </sup>
     <span class="ltx_note_type">
      footnotetext:
     </span>
     Correspondence to
     <span class="ltx_text ltx_font_typewriter" id="footnotex2.1">
      yixiaoge@tencent.com
     </span>
     .
    </span>
   </span>
  </span>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    We posit that a proper visual tokenizer is the key as it can facilitate the follow-up multimodal training by (i) easing the semantic alignment between visual and word tokens, and (ii) enabling LLM‚Äôs original training recipe (i.e., next-word prediction) for multimodal data without specific adaptation for visual tokens.
Representing images as a sequence of discrete IDs is naturally compatible with the autoregressive training objective of LLMs.
But unfortunately, works
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib6" title="">
      6
     </a>
     ]
    </cite>
    that utilize discretized visual tokens for multimodal tasks have receded from prominence, as such models generally rely on super-scale training to converge, leading to substantial training costs.
Moreover, our previous work
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    empirically found that the dominant tokenizer VQ-VAE
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ]
    </cite>
    in existing works captures too low-level information for LLMs to effectively perform multimodal comprehension tasks.
Existing image tokenizers fail to meet the requirements of unifying the generation of images and texts and facilitating multimodal training.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    To this end, we introduce
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">
     SEED
    </span>
    , a VQ-based image tokenizer that produces discrete visual codes with 1D causal dependency and necessary high-level semantics for both visual comprehension and generation tasks, as shown in Fig.
    <a class="ltx_ref" href="#S1.F2" title="Figure 2 ‚Ä£ 1 Introduction ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    (a).
The off-the-shelf LLMs can be readily equipped with SEED by treating discrete visual tokens as new words and updating the vocabulary.
We would like to emphasize the design principles of SEED.
(1)
    <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">
     Why causal-dependent tokens?
    </span>
    Existing visual tokens (
    <span class="ltx_text ltx_font_italic" id="S1.p3.1.3">
     e.g.
    </span>
    , from VQ-VAE or CLIP-ViT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib8" title="">
      8
     </a>
     ]
    </cite>
    ) are generated using 2D context, which is incompatible with the unidirectional attention in dominant LLMs and counterintuitive for text-to-image tasks requiring raster order prediction. Thus, we convert 2D raster-ordered embeddings into a sequence of semantic codes with 1D causal dependency.
(2)
    <span class="ltx_text ltx_font_italic" id="S1.p3.1.4">
     Why high-level semantics?
    </span>
    Since visual and textual tokens in LLMs are expected to be interoperable‚Äîsharing weights and training objectives‚Äîthey should encompass the same degree of semantics to prevent misalignment, i.e., the high-level semantics inherently present in words.
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="150" id="S1.F2.g1" src="/html/2310.01218/assets/x2.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 2:
    </span>
    (a) SEED is a discrete image tokenizer, producing quantized visual codes with 1D causal dependency and high-level semantics. (b) With SEED tokenizer, LLM is able to perform scalable multimodal autoregression on interleaved visual and textual data with next-word-prediction objective.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    Specifically, the SEED tokenizer is composed of a ViT encoder
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib9" title="">
      9
     </a>
     ]
    </cite>
    , Causal Q-Former, VQ Codebook
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ]
    </cite>
    , multi-layer perceptron (MLP), and a UNet decoder
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ]
    </cite>
    .
The ViT encoder and UNet decoder are directly derived from the pre-trained BLIP-2
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib11" title="">
      11
     </a>
     ]
    </cite>
    and unCLIP-SD model
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib13" title="">
      13
     </a>
     ]
    </cite>
    , respectively.
(1)
    <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">
     Tokenize:
    </span>
    Causal Q-Former converts 2D raster-ordered features produced by the ViT encoder into a sequence of causal semantic embeddings, which are further discretized by the VQ Codebook.
(2)
    <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">
     De-Tokenize:
    </span>
    The discrete visual codes are decoded into generation embedding via MLP. The generation embedding is aligned with the latent space of unCLIP-SD so that realistic images with consistent semantics can be generated using the off-the-shelf SD-UNet.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    We further present
    <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">
     SEED-LLaMA
    </span>
    by equipping the pre-trained LLM
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib2" title="">
      2
     </a>
     ]
    </cite>
    with SEED tokenizer.
SEED-LLaMA is pretrained on multimodal data, including image-text pairs, video-text pairs, and interleaved image-text data, toward the training objective of next-word prediction as shown in Fig.
    <a class="ltx_ref" href="#S1.F2" title="Figure 2 ‚Ä£ 1 Introduction ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    (b).
Such an easy-to-implement and unified proxy task facilitates scalable multimodal pretraining.
We further apply multimodal instruction tuning to align SEED-LLaMA with human instructions through supervised fine-tuning.
Our model demonstrates extensive emergent abilities such as multi-turn in-context image and text generation given multimodal instructions as shown in Fig.
    <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    . We also benchmark on a broad range of tasks including image captioning, image/video question answering, and text-to-image generation, receiving competitive performance.
   </p>
  </div>
  <div class="ltx_para" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    In summary, our contributions are three-fold.
(1) We introduce SEED, an advanced image tokenizer, designed based on the insights that visual tokens compatible with LLMs should capture high-level semantics while being generated with 1D causal dependency. The tailored SEED improves the scalability of subsequent multimodal training.
(2) We present SEED-LLaMA, composed of a pretrained LLM and SEED tokenizer, through large-scale multimodal pretraining and instruction tuning under the next-word-prediction training objective. It successfully unified multimodal comprehension and generation tasks in one framework.
(3) SEED-LLaMA shows competitive results on existing multimodal tasks (e.g., text-to-image, image-to-text) and further demonstrates emergent abilities in multi-turn in-context multimodal understanding, reasoning, and generation.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p ltx_align_left" id="S2.p1.1">
    <span class="ltx_text ltx_font_bold" id="S2.p1.1.1">
     MLLMs for Comprehension and Generation.
    </span>
   </p>
  </div>
  <div class="ltx_para" id="S2.p2">
   <p class="ltx_p" id="S2.p2.1">
    With the impressive success of Large language models
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib2" title="">
      2
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib3" title="">
      3
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib4" title="">
      4
     </a>
     ]
    </cite>
    (LLMs), recent studies work on Multimodal LLM (MLLM) to improve visual
    <span class="ltx_text ltx_font_bold" id="S2.p2.1.1">
     comprehension
    </span>
    through utilizing the strong generality of LLMs. Previous work
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib14" title="">
      14
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib11" title="">
      11
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib15" title="">
      15
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib17" title="">
      17
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib18" title="">
      18
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib19" title="">
      19
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib20" title="">
      20
     </a>
     ]
    </cite>
    align visual features of pre-trained image encoder with LLMs on image-text datasets. However, these work commonly use the prediction of the next
    <span class="ltx_text ltx_font_italic" id="S2.p2.1.2">
     text token
    </span>
    as the objective, thus can only output texts.
   </p>
  </div>
  <div class="ltx_para" id="S2.p3">
   <p class="ltx_p" id="S2.p3.1">
    To empower LLMs with the image
    <span class="ltx_text ltx_font_bold" id="S2.p3.1.1">
     generation
    </span>
    ability, CogView
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib6" title="">
      6
     </a>
     ]
    </cite>
    pre-trains a visual tokenizer by reconstructing image pixels, and fine-tunes GPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib3" title="">
      3
     </a>
     ]
    </cite>
    with the objective of next-token prediction. GILL
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib21" title="">
      21
     </a>
     ]
    </cite>
    learns a mapping between the embeddings of a LLM and a frozen text-to-image generation model. Both work aim to generate images with LLMs, without being explicitly designed for unifying multimodal comprehension and generation.
   </p>
  </div>
  <div class="ltx_para" id="S2.p4">
   <p class="ltx_p" id="S2.p4.1">
    Our concurrent works
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib22" title="">
      22
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib23" title="">
      23
     </a>
     ]
    </cite>
    both perform multimodal autoregression including the generation of images and texts. CM3Leon
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib23" title="">
      23
     </a>
     ]
    </cite>
    utilizes discrete visual codes from a image tokenizer
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib24" title="">
      24
     </a>
     ]
    </cite>
    pre-trained on image pixel reconstruction and performs image-to-text and text-to-image autoregression. However, it yields suboptimal performance in visual comprehension tasks (e.g., CIDEr 61.6 vs. ours 126.9 on COCO image captioning) because the image tokenizer captures too low-level information. Emu
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib22" title="">
      22
     </a>
     ]
    </cite>
    employs continuous visual representations and is pre-trained on interleaved multimodal sequences through classifying the next text token or
    <span class="ltx_text ltx_font_bold" id="S2.p4.1.1">
     regressing
    </span>
    the next visual embedding. For image generation, Emu further fine-tunes a SD model to accommodate the output representations from the LLM. By contrast, we pre-train a discrete image tokenizer, where the visual codes can be decoded to realistic images using the off-the-shelf SD model, and perform multimodal autoregressive with a unified next-word-prediction objective, which facilitates scalable multimodal training.
   </p>
  </div>
  <div class="ltx_para" id="S2.p5">
   <p class="ltx_p ltx_align_left" id="S2.p5.1">
    <span class="ltx_text ltx_font_bold" id="S2.p5.1.1">
     Visual Tokenizer.
    </span>
   </p>
  </div>
  <div class="ltx_para" id="S2.p6">
   <p class="ltx_p" id="S2.p6.1">
    Visual tokenizer aims to represent images as a sequence of discrete tokens. Previous work
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib25" title="">
      25
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib26" title="">
      26
     </a>
     ]
    </cite>
    trains a Vector Quantized Variational AutoEncoders (VQ-VAE) by reconstructing image pixels, which captures only low-level details such as color, texture and edge. Beit v2
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib27" title="">
      27
     </a>
     ]
    </cite>
    trains a visual tokenizer through reconstructing high-level features from the teacher model, but its visual codes from 2D features of a vision transformer
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib9" title="">
      9
     </a>
     ]
    </cite>
    are incompatible with the unidirectional attention in dominant LLMs for image generation. By contrast, we present SEED tokenizer, which produces discrete visual codes with 1D causal dependency and high-level semantics.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Method
  </h2>
  <figure class="ltx_figure" id="S3.F3">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="260" id="S3.F3.g1" src="/html/2310.01218/assets/x3.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 3:
    </span>
    Overview of
    <span class="ltx_text ltx_font_bold" id="S3.F3.2.1">
     SEED
    </span>
    tokenizer, which produces discrete visual codes with causal dependency and high-level semantics. The generation embedding from visual codes can be decoded to realistic images with the frozen unCLIP
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib13" title="">
      13
     </a>
     ]
    </cite>
    SD, which is conditioned on image embedding.
   </figcaption>
  </figure>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    SEED Tokenizer
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     As shown in Fig.
     <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‚Ä£ 3 Method ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     , the SEED tokenizer is composed of a ViT encoder
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib9" title="">
       9
      </a>
      ]
     </cite>
     , Causal Q-Former, VQ Codebook
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib7" title="">
       7
      </a>
      ]
     </cite>
     , multi-layer perceptron (MLP), and a UNet decoder
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib10" title="">
       10
      </a>
      ]
     </cite>
     .
The ViT encoder and UNet decoder are directly derived from the pre-trained BLIP-2
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib11" title="">
       11
      </a>
      ]
     </cite>
     and unCLIP
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib13" title="">
       13
      </a>
      ]
     </cite>
     Stable Diffusion (unCLIP-SD)
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib12" title="">
       12
      </a>
      ]
     </cite>
     , respectively. We first train a Causal Q-Former to convert 2D raster-ordered features (16
     <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1">
      <semantics id="S3.SS1.p1.1.m1.1a">
       <mo id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">
        √ó
       </mo>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b">
        <times id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">
        </times>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">
        \times
       </annotation>
      </semantics>
     </math>
     16 tokens) produced by the ViT encoder into a sequence of causal embeddings (32 tokens). We then train a visual codebook to discretize the causal embeddings to quantized visual codes (32 tokens) with causal dependency. We employ a MLP to decode the visual codes into generation embedding (1 token), which is aligned with the latent space of the pre-trained unCLIP-SD conditioned on image embedding. Our previous work
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib1" title="">
       1
      </a>
      ]
     </cite>
     aligns generation embeddings with the text embeddings of SD
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib12" title="">
       12
      </a>
      ]
     </cite>
     , and we analyze the difference in Sec.
     <a class="ltx_ref" href="#S4.SS3" title="4.3 Ablation Study ‚Ä£ 4 Experiment ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
      <span class="ltx_text ltx_ref_tag">
       4.3
      </span>
     </a>
     . We pre-train SEED tokenizer on CC3M
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ]
     </cite>
     , Unsplash
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib29" title="">
       29
      </a>
      ]
     </cite>
     , LAION-COCO
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib30" title="">
       30
      </a>
      ]
     </cite>
     and MS-COCO
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib31" title="">
       31
      </a>
      ]
     </cite>
     .
    </p>
   </div>
   <section class="ltx_subsubsection" id="S3.SS1.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.1.1
     </span>
     Training Stage I: Causal Q-Former
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS1.p1">
     <p class="ltx_p" id="S3.SS1.SSS1.p1.1">
      As shown in Fig.
      <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‚Ä£ 3 Method ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      , a set number of learnable query embeddings (32 tokens) and features of a pre-trained ViT encoder
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib8" title="">
        8
       </a>
       ]
      </cite>
      are fed into the Causal Q-former to encode a fixed number of causal embeddings (32 tokens) of the input image. Specifically, the query embeddings can interact with only previous queries through self-attention layers with causal mask, and interact with frozen image features through cross-attention layers. We adopt contrastive learning to optimize Causal Q-former fine-tuned from BLIP-2 Q-Former on
image-text pairs. We use contrastive loss to maximize the similarity between the
      <span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p1.1.1">
       final
      </span>
      causal embedding and text features of the corresponding caption.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS1.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.1.2
     </span>
     Training Stage II: Visual Tokenize and De-tokenize
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS2.p1">
     <p class="ltx_p" id="S3.SS1.SSS2.p1.1">
      As shown in Fig.
      <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‚Ä£ 3 Method ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      , we train a VQ codebook to discretize the causal embeddings (32 tokens) into quantized visual codes (32 tokens). Specifically, a quantizer looks up the nearest neighbor in the codebook for each causal embedding and obtains the corresponding code. We employ a decoder, which is a multi-layer Transformer
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib9" title="">
        9
       </a>
       ]
      </cite>
      , to reconstruct the continuous causal embeddings from discrete codes. During training, we maximize the cosine similarity between the output of the decoder and the causal embeddings. We further employ a MLP to reconstruct the image embedding (1 token) of a frozen unCLIP-SD from discrete codes. During training, we minimize the MSE loss between the generation embedding and the image embedding of unCLIP-SD. During inference, the generation embedding are fed into the off-the-shelf SD-UNet to decode realistic images.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    SEED-LLaMA
   </h3>
   <section class="ltx_subsubsection" id="S3.SS2.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.1
     </span>
     Training Stage I: Multimodal Pretraining
    </h4>
    <figure class="ltx_figure" id="S3.F4">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="163" id="S3.F4.g1" src="/html/2310.01218/assets/x4.png" width="461"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 4:
      </span>
      Overview of the multimodal autoregressive pretraining on interleaved visual and textual data for
      <span class="ltx_text ltx_font_bold" id="S3.F4.2.1">
       SEED-LLaMA
      </span>
      . Visual inputs are pre-processed into discrete tokens to conserve computational resources. Given the multimodal discrete sequence, a unified next-word-prediction objective is employed. During inference, visual codes are decoded into a realistic image by SEED De-Tokenization.
     </figcaption>
    </figure>
    <div class="ltx_para" id="S3.SS2.SSS1.p1">
     <p class="ltx_p" id="S3.SS2.SSS1.p1.3">
      As shown in Fig.
      <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‚Ä£ 3.2.1 Training Stage I: Multimodal Pretraining ‚Ä£ 3.2 SEED-LLaMA ‚Ä£ 3 Method ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
       <span class="ltx_text ltx_ref_tag">
        4
       </span>
      </a>
      , SEED-LLaMA adopts a unified next-word-prediction training objective on interleaved visual and textual data. Specifically, visual inputs are first discretized into a sequence of causal codes by SEED tokenizer. Then the interleaved visual codes and text tokens are fed into the pretrained LLM for performing multimodal autoregression, where the visual codes are treated as new words and the vocabulary of the LLM is updated accordingly. We maximize the likelihood in a unified autoregressive manner as follows:
     </p>
     <table class="ltx_equation ltx_eqn_table" id="S3.E1">
      <tbody>
       <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
        <td class="ltx_eqn_cell ltx_eqn_center_padleft">
        </td>
        <td class="ltx_eqn_cell ltx_align_center">
         <math alttext="L(\mathcal{U})=\sum_{i}\log P\left(u_{i}\mid u_{i-k},\ldots,u_{i-1};\Theta\right)" class="ltx_Math" display="block" id="S3.E1.m1.4">
          <semantics id="S3.E1.m1.4a">
           <mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">
            <mrow id="S3.E1.m1.4.4.3" xref="S3.E1.m1.4.4.3.cmml">
             <mi id="S3.E1.m1.4.4.3.2" xref="S3.E1.m1.4.4.3.2.cmml">
              L
             </mi>
             <mo id="S3.E1.m1.4.4.3.1" lspace="0em" rspace="0em" xref="S3.E1.m1.4.4.3.1.cmml">
              ‚Äã
             </mo>
             <mrow id="S3.E1.m1.4.4.3.3.2" xref="S3.E1.m1.4.4.3.cmml">
              <mo id="S3.E1.m1.4.4.3.3.2.1" stretchy="false" xref="S3.E1.m1.4.4.3.cmml">
               (
              </mo>
              <mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">
               ùí∞
              </mi>
              <mo id="S3.E1.m1.4.4.3.3.2.2" stretchy="false" xref="S3.E1.m1.4.4.3.cmml">
               )
              </mo>
             </mrow>
            </mrow>
            <mo id="S3.E1.m1.4.4.2" rspace="0.111em" xref="S3.E1.m1.4.4.2.cmml">
             =
            </mo>
            <mrow id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.cmml">
             <munder id="S3.E1.m1.4.4.1.2" xref="S3.E1.m1.4.4.1.2.cmml">
              <mo id="S3.E1.m1.4.4.1.2.2" movablelimits="false" xref="S3.E1.m1.4.4.1.2.2.cmml">
               ‚àë
              </mo>
              <mi id="S3.E1.m1.4.4.1.2.3" xref="S3.E1.m1.4.4.1.2.3.cmml">
               i
              </mi>
             </munder>
             <mrow id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml">
              <mrow id="S3.E1.m1.4.4.1.1.3" xref="S3.E1.m1.4.4.1.1.3.cmml">
               <mi id="S3.E1.m1.4.4.1.1.3.1" xref="S3.E1.m1.4.4.1.1.3.1.cmml">
                log
               </mi>
               <mo id="S3.E1.m1.4.4.1.1.3a" lspace="0.167em" xref="S3.E1.m1.4.4.1.1.3.cmml">
                ‚Å°
               </mo>
               <mi id="S3.E1.m1.4.4.1.1.3.2" xref="S3.E1.m1.4.4.1.1.3.2.cmml">
                P
               </mi>
              </mrow>
              <mo id="S3.E1.m1.4.4.1.1.2" lspace="0em" rspace="0em" xref="S3.E1.m1.4.4.1.1.2.cmml">
               ‚Äã
              </mo>
              <mrow id="S3.E1.m1.4.4.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml">
               <mo id="S3.E1.m1.4.4.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml">
                (
               </mo>
               <mrow id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml">
                <msub id="S3.E1.m1.4.4.1.1.1.1.1.4" xref="S3.E1.m1.4.4.1.1.1.1.1.4.cmml">
                 <mi id="S3.E1.m1.4.4.1.1.1.1.1.4.2" xref="S3.E1.m1.4.4.1.1.1.1.1.4.2.cmml">
                  u
                 </mi>
                 <mi id="S3.E1.m1.4.4.1.1.1.1.1.4.3" xref="S3.E1.m1.4.4.1.1.1.1.1.4.3.cmml">
                  i
                 </mi>
                </msub>
                <mo id="S3.E1.m1.4.4.1.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.1.3.cmml">
                 ‚à£
                </mo>
                <mrow id="S3.E1.m1.4.4.1.1.1.1.1.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.2.3.cmml">
                 <msub id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml">
                  <mi id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">
                   u
                  </mi>
                  <mrow id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.cmml">
                   <mi id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.2.cmml">
                    i
                   </mi>
                   <mo id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml">
                    ‚àí
                   </mo>
                   <mi id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.3.cmml">
                    k
                   </mi>
                  </mrow>
                 </msub>
                 <mo id="S3.E1.m1.4.4.1.1.1.1.1.2.2.3" xref="S3.E1.m1.4.4.1.1.1.1.1.2.3.cmml">
                  ,
                 </mo>
                 <mi id="S3.E1.m1.2.2" mathvariant="normal" xref="S3.E1.m1.2.2.cmml">
                  ‚Ä¶
                 </mi>
                 <mo id="S3.E1.m1.4.4.1.1.1.1.1.2.2.4" xref="S3.E1.m1.4.4.1.1.1.1.1.2.3.cmml">
                  ,
                 </mo>
                 <msub id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.cmml">
                  <mi id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2.cmml">
                   u
                  </mi>
                  <mrow id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3.cmml">
                   <mi id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3.2" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3.2.cmml">
                    i
                   </mi>
                   <mo id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3.1" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3.1.cmml">
                    ‚àí
                   </mo>
                   <mn id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3.3" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3.3.cmml">
                    1
                   </mn>
                  </mrow>
                 </msub>
                 <mo id="S3.E1.m1.4.4.1.1.1.1.1.2.2.5" xref="S3.E1.m1.4.4.1.1.1.1.1.2.3.cmml">
                  ;
                 </mo>
                 <mi id="S3.E1.m1.3.3" mathvariant="normal" xref="S3.E1.m1.3.3.cmml">
                  Œò
                 </mi>
                </mrow>
               </mrow>
               <mo id="S3.E1.m1.4.4.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml">
                )
               </mo>
              </mrow>
             </mrow>
            </mrow>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b">
            <apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">
             <eq id="S3.E1.m1.4.4.2.cmml" xref="S3.E1.m1.4.4.2">
             </eq>
             <apply id="S3.E1.m1.4.4.3.cmml" xref="S3.E1.m1.4.4.3">
              <times id="S3.E1.m1.4.4.3.1.cmml" xref="S3.E1.m1.4.4.3.1">
              </times>
              <ci id="S3.E1.m1.4.4.3.2.cmml" xref="S3.E1.m1.4.4.3.2">
               ùêø
              </ci>
              <ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">
               ùí∞
              </ci>
             </apply>
             <apply id="S3.E1.m1.4.4.1.cmml" xref="S3.E1.m1.4.4.1">
              <apply id="S3.E1.m1.4.4.1.2.cmml" xref="S3.E1.m1.4.4.1.2">
               <csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.2.1.cmml" xref="S3.E1.m1.4.4.1.2">
                subscript
               </csymbol>
               <sum id="S3.E1.m1.4.4.1.2.2.cmml" xref="S3.E1.m1.4.4.1.2.2">
               </sum>
               <ci id="S3.E1.m1.4.4.1.2.3.cmml" xref="S3.E1.m1.4.4.1.2.3">
                ùëñ
               </ci>
              </apply>
              <apply id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1.1">
               <times id="S3.E1.m1.4.4.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.2">
               </times>
               <apply id="S3.E1.m1.4.4.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.3">
                <log id="S3.E1.m1.4.4.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.3.1">
                </log>
                <ci id="S3.E1.m1.4.4.1.1.3.2.cmml" xref="S3.E1.m1.4.4.1.1.3.2">
                 ùëÉ
                </ci>
               </apply>
               <apply id="S3.E1.m1.4.4.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1">
                <csymbol cd="latexml" id="S3.E1.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.3">
                 conditional
                </csymbol>
                <apply id="S3.E1.m1.4.4.1.1.1.1.1.4.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.4">
                 <csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.4">
                  subscript
                 </csymbol>
                 <ci id="S3.E1.m1.4.4.1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.4.2">
                  ùë¢
                 </ci>
                 <ci id="S3.E1.m1.4.4.1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.4.3">
                  ùëñ
                 </ci>
                </apply>
                <list id="S3.E1.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2">
                 <apply id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1">
                  <csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1">
                   subscript
                  </csymbol>
                  <ci id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.2">
                   ùë¢
                  </ci>
                  <apply id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3">
                   <minus id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.1">
                   </minus>
                   <ci id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.2">
                    ùëñ
                   </ci>
                   <ci id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.3">
                    ùëò
                   </ci>
                  </apply>
                 </apply>
                 <ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">
                  ‚Ä¶
                 </ci>
                 <apply id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2">
                  <csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2">
                   subscript
                  </csymbol>
                  <ci id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.2">
                   ùë¢
                  </ci>
                  <apply id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3">
                   <minus id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3.1">
                   </minus>
                   <ci id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3.2">
                    ùëñ
                   </ci>
                   <cn id="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.2.3.3">
                    1
                   </cn>
                  </apply>
                 </apply>
                 <ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">
                  Œò
                 </ci>
                </list>
               </apply>
              </apply>
             </apply>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.E1.m1.4c">
            L(\mathcal{U})=\sum_{i}\log P\left(u_{i}\mid u_{i-k},\ldots,u_{i-1};\Theta\right)
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padright">
        </td>
        <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
         <span class="ltx_tag ltx_tag_equation ltx_align_right">
          (1)
         </span>
        </td>
       </tr>
      </tbody>
     </table>
     <p class="ltx_p" id="S3.SS2.SSS1.p1.2">
      where
      <math alttext="u_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1.1">
       <semantics id="S3.SS2.SSS1.p1.1.m1.1a">
        <msub id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">
         <mi id="S3.SS2.SSS1.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml">
          u
         </mi>
         <mi id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml">
          i
         </mi>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b">
         <apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">
          <csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">
           subscript
          </csymbol>
          <ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2">
           ùë¢
          </ci>
          <ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3">
           ùëñ
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">
         u_{i}
        </annotation>
       </semantics>
      </math>
      represents visual code or text token, and
      <math alttext="\Theta" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.2.m2.1">
       <semantics id="S3.SS2.SSS1.p1.2.m2.1a">
        <mi id="S3.SS2.SSS1.p1.2.m2.1.1" mathvariant="normal" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml">
         Œò
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b">
         <ci id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">
          Œò
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">
         \Theta
        </annotation>
       </semantics>
      </math>
      denotes the the parameters of the transformer. We initialize SEED-LLaMA from a pre-trained LLM, and add 8192 visual codes to the vocabulary. The embedding layer and decoder head layer in the transformer are expanded and the parameters of added visual codes are randomly initialized.
     </p>
    </div>
    <div class="ltx_para" id="S3.SS2.SSS1.p2">
     <p class="ltx_p" id="S3.SS2.SSS1.p2.1">
      For efficiency, we first train SEED-LLaMA using LoRA
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib32" title="">
        32
       </a>
       ]
      </cite>
      tuning and together optimize the parameters of the embedding layer and decoder head layer due to the added visual codes. We then merge the parameters of LoRA onto the LLM backbone and fine-tune all parameters except for the embedding layer. We freeze the embedding layer since we observe that fine-tuning it together with other parameters can lead to unstable training loss, which is also reported in BLOOM
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib33" title="">
        33
       </a>
       ]
      </cite>
      and GLM-130B
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib34" title="">
        34
       </a>
       ]
      </cite>
      . We preprocess the images and videos into discrete tokens beforehand to conserve computational resources. We perform pretraining using two versions of LLM, Vicuna-7B and Llama2-chat-13B, with 64 A100-40G GPUs, and yield SEED-LLaMA-8B (144 hours) and SEED-LLaMA-14B (216 hours), respectively. See Appendix.
      <a class="ltx_ref" href="#A2" title="Appendix B Pretraining ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
       <span class="ltx_text ltx_ref_tag">
        B
       </span>
      </a>
      for details.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS2.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.2
     </span>
     Training Stage II: Multimodal Instruction Tuning
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS2.p1">
     <p class="ltx_p" id="S3.SS2.SSS2.p1.1">
      We perform multimodal instruction tuning on SEED-LLaMA to align it with human instructions through supervised finetuning on public datasets. The details of datasets can be found in Appendix.
      <a class="ltx_ref" href="#A3" title="Appendix C Instruction Tuning ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
       <span class="ltx_text ltx_ref_tag">
        C
       </span>
      </a>
      . We fine-tune a LoRA module on the pre-trained SEED-LLaMA with the template as below,
     </p>
     <table class="ltx_equation ltx_eqn_table" id="S3.E2">
      <tbody>
       <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
        <td class="ltx_eqn_cell ltx_eqn_center_padleft">
        </td>
        <td class="ltx_eqn_cell ltx_align_center">
         <span class="ltx_text ltx_markedasmath" id="S3.E2.1">
          USER: ‚ÄÉ&lt;Instruction&gt;‚ÄÉASSISTANT:‚ÄÉ&lt;Answer&gt;
         </span>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padright">
        </td>
        <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="0">
         <span class="ltx_tag ltx_tag_equation ltx_align_right">
          (2)
         </span>
        </td>
       </tr>
      </tbody>
     </table>
     <p class="ltx_p" id="S3.SS2.SSS2.p1.2">
      Only the content of &lt;Answer&gt; is accounted for loss. The overall instruction tuning phase takes 16 hours for SEED-LLaMA-8B and 27 hours for SEED-LLaMA-14B with 32 A100-80G GPUs.
     </p>
    </div>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Experiment
  </h2>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    SEED Tokenizer
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p ltx_align_left" id="S4.SS1.p1.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">
      Evaluation of Causal Embeddings.
     </span>
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.1">
     We evaluate the performance of Causal Q-Former on the image-text retrieval using COCO
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib35" title="">
       35
      </a>
      ]
     </cite>
     and Flickr30K
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib36" title="">
       36
      </a>
      ]
     </cite>
     . The performance is measured by
     <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.1">
      Recall@K
     </em>
     (R@K). Note that we adopt the dual-stream paradigm for inference and remove the image-text-matching (ITM) re-rank module in BLIP-2 for a fair comparison. As shown in Tab.
     <span class="ltx_ref ltx_missing_label ltx_ref_self">
      LABEL:tab:retrieval
     </span>
     , our Causal Q-former achieves better results than BLIP-2 in terms of an aggregated metric
     <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.2">
      Recall@mean
     </em>
     . It demonstrates that the output query embeddings with causal dependency do not drop performance than the output embeddings with bi-directional attention in BLIP-2.
    </p>
   </div>
   <figure class="ltx_table" id="S4.T1">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 1:
     </span>
     Evaluation of Image-Text Retrieval. Causal codes are quantized causal embeddings.
    </figcaption>
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.4" style="width:433.6pt;height:82.1pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-86.8pt,16.4pt) scale(0.714201234023617,0.714201234023617) ;">
      <table class="ltx_tabular ltx_align_middle" id="S4.T1.4.4">
       <tr class="ltx_tr" id="S4.T1.4.4.5">
        <td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.4.4.5.1" rowspan="3" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text" id="S4.T1.4.4.5.1.1">
          Model
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="7" id="S4.T1.4.4.5.2" style="padding-top:2pt;padding-bottom:2pt;">
         Flickr30K (1K test set)
        </td>
        <td class="ltx_td ltx_align_center ltx_border_tt" colspan="7" id="S4.T1.4.4.5.3" style="padding-top:2pt;padding-bottom:2pt;">
         COCO (5K test set)
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.4.4.4">
        <td class="ltx_td ltx_align_center" colspan="3" id="S4.T1.1.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">
         Image
         <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1">
          <semantics id="S4.T1.1.1.1.1.m1.1a">
           <mo id="S4.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.1.m1.1.1.cmml">
            ‚Üí
           </mo>
           <annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b">
            <ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">
             ‚Üí
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">
            \rightarrow
           </annotation>
          </semantics>
         </math>
         Text
        </td>
        <td class="ltx_td ltx_align_center" colspan="3" id="S4.T1.2.2.2.2" style="padding-top:2pt;padding-bottom:2pt;">
         Text
         <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.2.m1.1">
          <semantics id="S4.T1.2.2.2.2.m1.1a">
           <mo id="S4.T1.2.2.2.2.m1.1.1" stretchy="false" xref="S4.T1.2.2.2.2.m1.1.1.cmml">
            ‚Üí
           </mo>
           <annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b">
            <ci id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1">
             ‚Üí
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">
            \rightarrow
           </annotation>
          </semantics>
         </math>
         Image
        </td>
        <td class="ltx_td ltx_border_r" id="S4.T1.4.4.4.5" style="padding-top:2pt;padding-bottom:2pt;">
        </td>
        <td class="ltx_td ltx_align_center" colspan="3" id="S4.T1.3.3.3.3" style="padding-top:2pt;padding-bottom:2pt;">
         Image
         <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T1.3.3.3.3.m1.1">
          <semantics id="S4.T1.3.3.3.3.m1.1a">
           <mo id="S4.T1.3.3.3.3.m1.1.1" stretchy="false" xref="S4.T1.3.3.3.3.m1.1.1.cmml">
            ‚Üí
           </mo>
           <annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m1.1b">
            <ci id="S4.T1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1">
             ‚Üí
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m1.1c">
            \rightarrow
           </annotation>
          </semantics>
         </math>
         Text
        </td>
        <td class="ltx_td ltx_align_center" colspan="3" id="S4.T1.4.4.4.4" style="padding-top:2pt;padding-bottom:2pt;">
         Text
         <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T1.4.4.4.4.m1.1">
          <semantics id="S4.T1.4.4.4.4.m1.1a">
           <mo id="S4.T1.4.4.4.4.m1.1.1" stretchy="false" xref="S4.T1.4.4.4.4.m1.1.1.cmml">
            ‚Üí
           </mo>
           <annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.4.m1.1b">
            <ci id="S4.T1.4.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.4.m1.1.1">
             ‚Üí
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.m1.1c">
            \rightarrow
           </annotation>
          </semantics>
         </math>
         Image
        </td>
        <td class="ltx_td" id="S4.T1.4.4.4.6" style="padding-top:2pt;padding-bottom:2pt;">
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.4.4.6">
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1" style="padding-top:2pt;padding-bottom:2pt;">
         R@1
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.2" style="padding-top:2pt;padding-bottom:2pt;">
         R@5
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.3" style="padding-top:2pt;padding-bottom:2pt;">
         R@10
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.4" style="padding-top:2pt;padding-bottom:2pt;">
         R@1
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.5" style="padding-top:2pt;padding-bottom:2pt;">
         R@5
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.6" style="padding-top:2pt;padding-bottom:2pt;">
         R@10
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.6.7" style="padding-top:2pt;padding-bottom:2pt;">
         R@m
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.8" style="padding-top:2pt;padding-bottom:2pt;">
         R@1
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.9" style="padding-top:2pt;padding-bottom:2pt;">
         R@5
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.10" style="padding-top:2pt;padding-bottom:2pt;">
         R@10
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.11" style="padding-top:2pt;padding-bottom:2pt;">
         R@1
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.12" style="padding-top:2pt;padding-bottom:2pt;">
         R@5
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.13" style="padding-top:2pt;padding-bottom:2pt;">
         R@10
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.14" style="padding-top:2pt;padding-bottom:2pt;">
         R@m
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.4.4.7">
        <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.4.4.7.1" style="padding-top:2pt;padding-bottom:2pt;">
         BLIP-2
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib11" title="">
           11
          </a>
          ]
         </cite>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.2" style="padding-top:2pt;padding-bottom:2pt;">
         81.9
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.3" style="padding-top:2pt;padding-bottom:2pt;">
         98.4
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.4" style="padding-top:2pt;padding-bottom:2pt;">
         99.7
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.5" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.7.5.1">
          82.4
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.6" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.7.6.1">
          96.5
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.7" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.7.7.1">
          98.4
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.7.8" style="padding-top:2pt;padding-bottom:2pt;">
         92.9
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.9" style="padding-top:2pt;padding-bottom:2pt;">
         65.3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.10" style="padding-top:2pt;padding-bottom:2pt;">
         89.9
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.11" style="padding-top:2pt;padding-bottom:2pt;">
         95.3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.12" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.7.12.1">
          59.1
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.13" style="padding-top:2pt;padding-bottom:2pt;">
         82.7
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.14" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.7.14.1">
          89.4
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.15" style="padding-top:2pt;padding-bottom:2pt;">
         80.3
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.4.4.8">
        <td class="ltx_td ltx_align_left" id="S4.T1.4.4.8.1" style="padding-top:2pt;padding-bottom:2pt;">
         SEED (causal embedding)
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.2" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.2.1">
          91.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.3" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.1">
          99.5
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.4" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.4.1">
          100.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.5" style="padding-top:2pt;padding-bottom:2pt;">
         79.3
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.6" style="padding-top:2pt;padding-bottom:2pt;">
         94.8
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.7" style="padding-top:2pt;padding-bottom:2pt;">
         97.1
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.8.8" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.8.1">
          93.6
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.9" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.9.1">
          74.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.10" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.10.1">
          93.1
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.11" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.11.1">
          96.7
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.12" style="padding-top:2pt;padding-bottom:2pt;">
         59.0
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.13" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.13.1">
          82.8
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.14" style="padding-top:2pt;padding-bottom:2pt;">
         89.2
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.15" style="padding-top:2pt;padding-bottom:2pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.15.1">
          82.5
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.4.4.9">
        <td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.4.4.9.1" style="padding-top:2pt;padding-bottom:2pt;">
         SEED (causal code)
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.9.2" style="padding-top:2pt;padding-bottom:2pt;">
         85.4
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.9.3" style="padding-top:2pt;padding-bottom:2pt;">
         98.3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.9.4" style="padding-top:2pt;padding-bottom:2pt;">
         99.6
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.9.5" style="padding-top:2pt;padding-bottom:2pt;">
         73.7
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.9.6" style="padding-top:2pt;padding-bottom:2pt;">
         92.3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.9.7" style="padding-top:2pt;padding-bottom:2pt;">
         95.7
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.4.4.9.8" style="padding-top:2pt;padding-bottom:2pt;">
         90.8
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.9.9" style="padding-top:2pt;padding-bottom:2pt;">
         66.9
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.9.10" style="padding-top:2pt;padding-bottom:2pt;">
         89.3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.9.11" style="padding-top:2pt;padding-bottom:2pt;">
         94.4
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.9.12" style="padding-top:2pt;padding-bottom:2pt;">
         53.2
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.9.13" style="padding-top:2pt;padding-bottom:2pt;">
         78.8
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.9.14" style="padding-top:2pt;padding-bottom:2pt;">
         86.6
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.9.15" style="padding-top:2pt;padding-bottom:2pt;">
         78.2
        </td>
       </tr>
      </table>
     </span>
    </div>
   </figure>
   <figure class="ltx_figure" id="S4.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="117" id="S4.F5.g1" src="/html/2310.01218/assets/x5.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5:
     </span>
     Reconstruction images of SEED tokenizer (
     <span class="ltx_text ltx_font_italic" id="S4.F5.10.1">
      i.e.
     </span>
     , original image
     <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F5.5.m1.1">
      <semantics id="S4.F5.5.m1.1b">
       <mo id="S4.F5.5.m1.1.1" stretchy="false" xref="S4.F5.5.m1.1.1.cmml">
        ‚Üí
       </mo>
       <annotation-xml encoding="MathML-Content" id="S4.F5.5.m1.1c">
        <ci id="S4.F5.5.m1.1.1.cmml" xref="S4.F5.5.m1.1.1">
         ‚Üí
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.F5.5.m1.1d">
        \rightarrow
       </annotation>
      </semantics>
     </math>
     SEED tokenize
     <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F5.6.m2.1">
      <semantics id="S4.F5.6.m2.1b">
       <mo id="S4.F5.6.m2.1.1" stretchy="false" xref="S4.F5.6.m2.1.1.cmml">
        ‚Üí
       </mo>
       <annotation-xml encoding="MathML-Content" id="S4.F5.6.m2.1c">
        <ci id="S4.F5.6.m2.1.1.cmml" xref="S4.F5.6.m2.1.1">
         ‚Üí
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.F5.6.m2.1d">
        \rightarrow
       </annotation>
      </semantics>
     </math>
     causal visual codes
     <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F5.7.m3.1">
      <semantics id="S4.F5.7.m3.1b">
       <mo id="S4.F5.7.m3.1.1" stretchy="false" xref="S4.F5.7.m3.1.1.cmml">
        ‚Üí
       </mo>
       <annotation-xml encoding="MathML-Content" id="S4.F5.7.m3.1c">
        <ci id="S4.F5.7.m3.1.1.cmml" xref="S4.F5.7.m3.1.1">
         ‚Üí
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.F5.7.m3.1d">
        \rightarrow
       </annotation>
      </semantics>
     </math>
     SEED de-tokenize
     <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F5.8.m4.1">
      <semantics id="S4.F5.8.m4.1b">
       <mo id="S4.F5.8.m4.1.1" stretchy="false" xref="S4.F5.8.m4.1.1.cmml">
        ‚Üí
       </mo>
       <annotation-xml encoding="MathML-Content" id="S4.F5.8.m4.1c">
        <ci id="S4.F5.8.m4.1.1.cmml" xref="S4.F5.8.m4.1.1">
         ‚Üí
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.F5.8.m4.1d">
        \rightarrow
       </annotation>
      </semantics>
     </math>
     reconstructed image).
    </figcaption>
   </figure>
   <div class="ltx_para" id="S4.SS1.p3">
    <p class="ltx_p ltx_align_left" id="S4.SS1.p3.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">
      Evaluation of Causal Codes.
     </span>
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p4">
    <p class="ltx_p" id="S4.SS1.p4.1">
     We evaluate causal codes on the image-text retrieval, where the reconstructed embeddings from causal codes are used for retrieval. As shown in Tab.
     <span class="ltx_ref ltx_missing_label ltx_ref_self">
      LABEL:tab:retrieval
     </span>
     , discrete codes exhibit competitive performance compared to BLIP-2, which demonstrates that the discrete codes from SEED tokenizer capture high-level semantics, which are suitable for visual comprehension.
    </p>
   </div>
   <figure class="ltx_table ltx_align_floatright" id="S4.T2">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 2:
     </span>
     Evaluation of Image Generation.
    </figcaption>
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:173.4pt;height:113pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-58.3pt,38.0pt) scale(0.597978808968919,0.597978808968919) ;">
      <table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1">
       <tr class="ltx_tr" id="S4.T2.1.1.2">
        <td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.1.2.1">
         Model
        </td>
        <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.2.2">
         COCO
        </td>
        <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.2.3">
         Flickr30K
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.1.1.3">
        <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.1">
         <span class="ltx_text ltx_font_italic" id="S4.T2.1.1.3.1.1">
          Image-to-image
         </span>
        </td>
        <td class="ltx_td ltx_border_t" id="S4.T2.1.1.3.2">
        </td>
        <td class="ltx_td ltx_border_t" id="S4.T2.1.1.3.3">
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.1.1.4">
        <td class="ltx_td ltx_align_left" id="S4.T2.1.1.4.1">
         unCLIP
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib13" title="">
           13
          </a>
          ]
         </cite>
         SD
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.2">
         <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.2.1">
          79.30
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3">
         <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.1">
          79.55
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.1.1.1">
        <td class="ltx_td ltx_align_left" id="S4.T2.1.1.1.1">
         <math alttext="\text{SEED}^{\text{text}}" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1">
          <semantics id="S4.T2.1.1.1.1.m1.1a">
           <msup id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">
            <mtext id="S4.T2.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.m1.1.1.2a.cmml">
             SEED
            </mtext>
            <mtext id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3a.cmml">
             text
            </mtext>
           </msup>
           <annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b">
            <apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">
             <csymbol cd="ambiguous" id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">
              superscript
             </csymbol>
             <ci id="S4.T2.1.1.1.1.m1.1.1.2a.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2">
              <mtext id="S4.T2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2">
               SEED
              </mtext>
             </ci>
             <ci id="S4.T2.1.1.1.1.m1.1.1.3a.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3">
              <mtext id="S4.T2.1.1.1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.T2.1.1.1.1.m1.1.1.3">
               text
              </mtext>
             </ci>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">
            \text{SEED}^{\text{text}}
           </annotation>
          </semantics>
         </math>
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib1" title="">
           1
          </a>
          ]
         </cite>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.2">
         68.23
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.3">
         65.22
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.1.1.5">
        <td class="ltx_td ltx_align_left" id="S4.T2.1.1.5.1">
         SEED
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.2">
         77.35
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.3">
         76.52
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.1.1.6">
        <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.6.1">
         <span class="ltx_text ltx_font_italic" id="S4.T2.1.1.6.1.1">
          Text-to-image
         </span>
        </td>
        <td class="ltx_td ltx_border_t" id="S4.T2.1.1.6.2">
        </td>
        <td class="ltx_td ltx_border_t" id="S4.T2.1.1.6.3">
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.1.1.7">
        <td class="ltx_td ltx_align_left" id="S4.T2.1.1.7.1">
         GILL
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib37" title="">
           37
          </a>
          ]
         </cite>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.2">
         67.45
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.3">
         65.16
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.1.1.8">
        <td class="ltx_td ltx_align_left" id="S4.T2.1.1.8.1">
         Emu
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib22" title="">
           22
          </a>
          ]
         </cite>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.1.8.2">
         66.46
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.1.8.3">
         64.82
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.1.1.9">
        <td class="ltx_td ltx_align_left" id="S4.T2.1.1.9.1">
         SEED-LLaMA
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.2">
         69.07
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.3">
         65.54
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.1.1.10">
        <td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.1.10.1">
         SEED-LLaMA-I
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.1.10.2">
         <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.10.2.1">
          70.68
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.1.10.3">
         <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.10.3.1">
          66.55
         </span>
        </td>
       </tr>
      </table>
     </span>
    </div>
   </figure>
   <div class="ltx_para" id="S4.SS1.p5">
    <p class="ltx_p" id="S4.SS1.p5.1">
     We further evaluate image reconstruction on COCO and Flickr30K dataset. SEED first discretizes input images into causal codes (32 tokens) and obtain generation embedding (1 token), which are fed into the unCLIP-SD-UNet for reconstruction. We follow GILL
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib21" title="">
       21
      </a>
      ]
     </cite>
     to compute the CLIP similarity score as the metric to evaluate the semantic consistency.
As shown in Tab.
     <span class="ltx_ref ltx_missing_label ltx_ref_self">
      LABEL:tab:clip_score
     </span>
     , compared with the upper bound unCLIP-SD, SEED only slightly drops performance.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p6">
    <p class="ltx_p" id="S4.SS1.p6.1">
     We visualize the reconstructed images of SEED tokenizer in Fig.
     <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‚Ä£ 4.1 SEED Tokenizer ‚Ä£ 4 Experiment ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     . Through obtaining the generation embedding from the causal visual codes, realistic images can be generated using the frozen SD-UNet, which maintain consistent semantics with inputs.
     <span class="ltx_text ltx_font_italic" id="S4.SS1.p6.1.1">
      The above evaluation and visualization demonstrate the versatility of SEED visual tokens for both comprehension and generation tasks.
     </span>
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    SEED-LLaMA
   </h3>
   <figure class="ltx_table" id="S4.T3">
    <figcaption class="ltx_caption ltx_centering" style="font-size:90%;">
     <span class="ltx_tag ltx_tag_table">
      Table 3:
     </span>
     Comparison for multimodal comprehension. ‚ÄúImage Gen‚Äù denotes whether the model can generate images besides texts, and ‚Äú-I‚Äù denotes the instruction tuned model. The best results are
     <span class="ltx_text ltx_font_bold" id="S4.T3.22.1">
      bold
     </span>
     and the second best are
     <span class="ltx_text ltx_ulem_uline" id="S4.T3.23.2">
      underlined
     </span>
     .
    </figcaption>
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.15" style="width:416.3pt;height:255.9pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-59.8pt,36.7pt) scale(0.776894937188902,0.776894937188902) ;">
      <table class="ltx_tabular ltx_align_middle" id="S4.T3.15.15">
       <tr class="ltx_tr" id="S4.T3.15.15.16">
        <td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T3.15.15.16.1" rowspan="2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.16.1.1" style="font-size:90%;">
          Models
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.15.15.16.2" rowspan="2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.16.2.1" style="font-size:90%;">
          Size
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.15.15.16.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.16.3.1" style="font-size:90%;">
          Image
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="5" id="S4.T3.15.15.16.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.16.4.1" style="font-size:90%;">
          Image-Text Tasks
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T3.15.15.16.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.16.5.1" style="font-size:90%;">
          Video-Text Tasks
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.15.15.17">
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.15.15.17.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.17.1.1" style="font-size:90%;">
          Gen
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.15.15.17.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.17.2.1" style="font-size:90%;">
          COCO
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.15.15.17.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.17.3.1" style="font-size:90%;">
          VQAv2
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.15.15.17.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.17.4.1" style="font-size:90%;">
          OKVQA
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.15.15.17.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.17.5.1" style="font-size:90%;">
          VizWiz
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.15.15.17.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <table class="ltx_tabular ltx_align_middle" id="S4.T3.15.15.17.6.1">
          <tr class="ltx_tr" id="S4.T3.15.15.17.6.1.1">
           <td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.15.15.17.6.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">
            <span class="ltx_text" id="S4.T3.15.15.17.6.1.1.1.1" style="font-size:90%;">
             SEED
            </span>
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T3.15.15.17.6.1.2">
           <td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.15.15.17.6.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">
            <span class="ltx_text" id="S4.T3.15.15.17.6.1.2.1.1" style="font-size:90%;">
             Bench
            </span>
           </td>
          </tr>
         </table>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.15.15.17.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.17.7.1" style="font-size:90%;">
          MSVDQA
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.15.15.17.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.17.8.1" style="font-size:90%;">
          MSRVTTQA
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.15.15.17.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.17.9.1" style="font-size:90%;">
          NExTQA
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.1.1.1">
        <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.1.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.1.1.1.2.1" style="font-size:90%;">
          Flamingo
         </span>
         <cite class="ltx_cite ltx_citemacro_cite">
          <span class="ltx_text" id="S4.T3.1.1.1.2.2.1" style="font-size:90%;">
           [
          </span>
          <a class="ltx_ref" href="#bib.bib19" title="">
           19
          </a>
          <span class="ltx_text" id="S4.T3.1.1.1.2.3.2" style="font-size:90%;">
           ]
          </span>
         </cite>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.1.1.1.3.1" style="font-size:90%;">
          9B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.m1.1">
          <semantics id="S4.T3.1.1.1.1.m1.1a">
           <mo id="S4.T3.1.1.1.1.m1.1.1" mathsize="90%" xref="S4.T3.1.1.1.1.m1.1.1.cmml">
            √ó
           </mo>
           <annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b">
            <times id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">
            </times>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">
            \times
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.1.1.1.4.1" style="font-size:90%;">
          79.4
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.1.1.1.5.1" style="font-size:90%;">
          51.8
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.1.1.1.6.1" style="font-size:90%;">
          44.7
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.1.1.1.7.1" style="font-size:90%;">
          28.8
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.1.1.1.8.1" style="font-size:90%;">
          42.7
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.1.1.1.9.1" style="font-size:90%;">
          30.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.1.1.1.10.1" style="font-size:90%;">
          13.7
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.1.1.1.11.1" style="font-size:90%;">
          23.0
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.2.2.2">
        <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.2.2.2.2.1" style="font-size:90%;">
          BLIP-2
         </span>
         <cite class="ltx_cite ltx_citemacro_cite">
          <span class="ltx_text" id="S4.T3.2.2.2.2.2.1" style="font-size:90%;">
           [
          </span>
          <a class="ltx_ref" href="#bib.bib38" title="">
           38
          </a>
          <span class="ltx_text" id="S4.T3.2.2.2.2.3.2" style="font-size:90%;">
           ]
          </span>
         </cite>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.2.2.2.3.1" style="font-size:90%;">
          4.1B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.2.2.2.1.m1.1">
          <semantics id="S4.T3.2.2.2.1.m1.1a">
           <mo id="S4.T3.2.2.2.1.m1.1.1" mathsize="90%" xref="S4.T3.2.2.2.1.m1.1.1.cmml">
            √ó
           </mo>
           <annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b">
            <times id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1">
            </times>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">
            \times
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.4.1" style="font-size:90%;">
          144.5
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.2.2.2.5.1" style="font-size:90%;">
          63.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.2.2.2.6.1" style="font-size:90%;">
          40.7
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.2.2.2.7.1" style="font-size:90%;">
          29.8
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.2.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.2.2.2.8.1" style="font-size:90%;">
          49.7
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.2.2.2.9.1" style="font-size:90%;">
          33.7
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.2.2.2.10.1" style="font-size:90%;">
          16.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.2.2.2.11.1" style="font-size:90%;">
          -
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.3.3.3">
        <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.3.3.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.3.3.3.2.1" style="font-size:90%;">
          InstructBLIP
         </span>
         <cite class="ltx_cite ltx_citemacro_cite">
          <span class="ltx_text" id="S4.T3.3.3.3.2.2.1" style="font-size:90%;">
           [
          </span>
          <a class="ltx_ref" href="#bib.bib11" title="">
           11
          </a>
          <span class="ltx_text" id="S4.T3.3.3.3.2.3.2" style="font-size:90%;">
           ]
          </span>
         </cite>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.3.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.3.3.3.3.1" style="font-size:90%;">
          8.1B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.3.3.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.3.3.3.1.m1.1">
          <semantics id="S4.T3.3.3.3.1.m1.1a">
           <mo id="S4.T3.3.3.3.1.m1.1.1" mathsize="90%" xref="S4.T3.3.3.3.1.m1.1.1.cmml">
            √ó
           </mo>
           <annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.1.m1.1b">
            <times id="S4.T3.3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1">
            </times>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.3.3.3.1.m1.1c">
            \times
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.3.3.3.4.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.3.3.3.5.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.3.3.3.6.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.3.3.3.7.1" style="font-size:90%;">
          34.5
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.3.3.3.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T3.3.3.3.8.1" style="font-size:90%;">
          58.8
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_ulem_uline" id="S4.T3.3.3.3.9.1" style="font-size:90%;">
          41.8
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.3.3.3.10.1" style="font-size:90%;">
          22.1
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.3.3.3.11.1" style="font-size:90%;">
          -
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.4.4.4">
        <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.4.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.4.4.4.2.1" style="font-size:90%;">
          Kosmos-1
         </span>
         <cite class="ltx_cite ltx_citemacro_cite">
          <span class="ltx_text" id="S4.T3.4.4.4.2.2.1" style="font-size:90%;">
           [
          </span>
          <a class="ltx_ref" href="#bib.bib39" title="">
           39
          </a>
          <span class="ltx_text" id="S4.T3.4.4.4.2.3.2" style="font-size:90%;">
           ]
          </span>
         </cite>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.4.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.4.4.4.3.1" style="font-size:90%;">
          1.6B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.4.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.4.4.4.1.m1.1">
          <semantics id="S4.T3.4.4.4.1.m1.1a">
           <mo id="S4.T3.4.4.4.1.m1.1.1" mathsize="90%" xref="S4.T3.4.4.4.1.m1.1.1.cmml">
            √ó
           </mo>
           <annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.m1.1b">
            <times id="S4.T3.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.m1.1.1">
            </times>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.m1.1c">
            \times
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.4.4.4.4.1" style="font-size:90%;">
          84.7
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.4.4.4.5.1" style="font-size:90%;">
          51.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.4.4.4.6.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.4.4.4.7.1" style="font-size:90%;">
          29.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.4.4.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.4.4.4.8.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.4.4.4.9.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.4.4.4.10.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.4.4.4.11.1" style="font-size:90%;">
          -
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.5.5.5">
        <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.5.5.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.5.5.5.2.1" style="font-size:90%;">
          Kosmos-2
         </span>
         <cite class="ltx_cite ltx_citemacro_cite">
          <span class="ltx_text" id="S4.T3.5.5.5.2.2.1" style="font-size:90%;">
           [
          </span>
          <a class="ltx_ref" href="#bib.bib40" title="">
           40
          </a>
          <span class="ltx_text" id="S4.T3.5.5.5.2.3.2" style="font-size:90%;">
           ]
          </span>
         </cite>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.5.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.5.5.5.3.1" style="font-size:90%;">
          1.6B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.5.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.5.5.5.1.m1.1">
          <semantics id="S4.T3.5.5.5.1.m1.1a">
           <mo id="S4.T3.5.5.5.1.m1.1.1" mathsize="90%" xref="S4.T3.5.5.5.1.m1.1.1.cmml">
            √ó
           </mo>
           <annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.m1.1b">
            <times id="S4.T3.5.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.m1.1.1">
            </times>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.m1.1c">
            \times
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.5.5.5.4.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.5.5.5.5.1" style="font-size:90%;">
          45.6
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.5.5.5.6.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.5.5.5.7.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.5.5.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.5.5.5.8.1" style="font-size:90%;">
          54.4
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.5.5.5.9.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.5.5.5.10.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.5.5.5.11.1" style="font-size:90%;">
          -
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.6.6.6">
        <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.6.6.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.6.6.6.2.1" style="font-size:90%;">
          MetaLLM
         </span>
         <cite class="ltx_cite ltx_citemacro_cite">
          <span class="ltx_text" id="S4.T3.6.6.6.2.2.1" style="font-size:90%;">
           [
          </span>
          <a class="ltx_ref" href="#bib.bib41" title="">
           41
          </a>
          <span class="ltx_text" id="S4.T3.6.6.6.2.3.2" style="font-size:90%;">
           ]
          </span>
         </cite>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.6.6.6.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.6.6.6.3.1" style="font-size:90%;">
          1.7B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.6.6.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.6.6.6.1.m1.1">
          <semantics id="S4.T3.6.6.6.1.m1.1a">
           <mo id="S4.T3.6.6.6.1.m1.1.1" mathsize="90%" xref="S4.T3.6.6.6.1.m1.1.1.cmml">
            √ó
           </mo>
           <annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.1.m1.1b">
            <times id="S4.T3.6.6.6.1.m1.1.1.cmml" xref="S4.T3.6.6.6.1.m1.1.1">
            </times>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.6.6.6.1.m1.1c">
            \times
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.6.6.6.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.6.6.6.4.1" style="font-size:90%;">
          82.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.6.6.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.6.6.6.5.1" style="font-size:90%;">
          41.1
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.6.6.6.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.6.6.6.6.1" style="font-size:90%;">
          11.4
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.6.6.6.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.6.6.6.7.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.6.6.6.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.6.6.6.8.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.6.6.6.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.6.6.6.9.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.6.6.6.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.6.6.6.10.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.6.6.6.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.6.6.6.11.1" style="font-size:90%;">
          -
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.7.7.7">
        <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.7.7.7.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.7.7.7.2.1" style="font-size:90%;">
          IDEFICS
         </span>
         <cite class="ltx_cite ltx_citemacro_cite">
          <span class="ltx_text" id="S4.T3.7.7.7.2.2.1" style="font-size:90%;">
           [
          </span>
          <a class="ltx_ref" href="#bib.bib42" title="">
           42
          </a>
          <span class="ltx_text" id="S4.T3.7.7.7.2.3.2" style="font-size:90%;">
           ]
          </span>
         </cite>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.7.7.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.7.7.7.3.1" style="font-size:90%;">
          80B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.7.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.7.7.7.1.m1.1">
          <semantics id="S4.T3.7.7.7.1.m1.1a">
           <mo id="S4.T3.7.7.7.1.m1.1.1" mathsize="90%" xref="S4.T3.7.7.7.1.m1.1.1.cmml">
            √ó
           </mo>
           <annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.1.m1.1b">
            <times id="S4.T3.7.7.7.1.m1.1.1.cmml" xref="S4.T3.7.7.7.1.m1.1.1">
            </times>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.7.7.7.1.m1.1c">
            \times
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.7.7.7.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.7.7.7.4.1" style="font-size:90%;">
          91.8
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.7.7.7.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.7.7.7.5.1" style="font-size:90%;">
          60.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.7.7.7.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_ulem_uline" id="S4.T3.7.7.7.6.1" style="font-size:90%;">
          45.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.7.7.7.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.7.7.7.7.1" style="font-size:90%;">
          36.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.7.7.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.7.7.7.8.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.7.7.7.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.7.7.7.9.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.7.7.7.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.7.7.7.10.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.7.7.7.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.7.7.7.11.1" style="font-size:90%;">
          -
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.8.8.8">
        <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.8.8.8.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.8.8.8.2.1" style="font-size:90%;">
          IDEFICS-I
         </span>
         <cite class="ltx_cite ltx_citemacro_cite">
          <span class="ltx_text" id="S4.T3.8.8.8.2.2.1" style="font-size:90%;">
           [
          </span>
          <a class="ltx_ref" href="#bib.bib42" title="">
           42
          </a>
          <span class="ltx_text" id="S4.T3.8.8.8.2.3.2" style="font-size:90%;">
           ]
          </span>
         </cite>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.8.8.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.8.8.8.3.1" style="font-size:90%;">
          80B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.8.8.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.8.8.8.1.m1.1">
          <semantics id="S4.T3.8.8.8.1.m1.1a">
           <mo id="S4.T3.8.8.8.1.m1.1.1" mathsize="90%" xref="S4.T3.8.8.8.1.m1.1.1.cmml">
            √ó
           </mo>
           <annotation-xml encoding="MathML-Content" id="S4.T3.8.8.8.1.m1.1b">
            <times id="S4.T3.8.8.8.1.m1.1.1.cmml" xref="S4.T3.8.8.8.1.m1.1.1">
            </times>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.8.8.8.1.m1.1c">
            \times
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.8.8.8.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.8.8.8.4.1" style="font-size:90%;">
          117.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.8.8.8.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.8.8.8.5.1" style="font-size:90%;">
          37.4
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.8.8.8.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.8.8.8.6.1" style="font-size:90%;">
          36.9
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.8.8.8.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.8.8.8.7.1" style="font-size:90%;">
          26.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.8.8.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.8.8.8.8.1" style="font-size:90%;">
          53.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.8.8.8.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.8.8.8.9.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.8.8.8.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.8.8.8.10.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.8.8.8.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.8.8.8.11.1" style="font-size:90%;">
          -
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.9.9.9">
        <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.9.9.9.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.9.9.9.2.1" style="font-size:90%;">
          CM3Leon
         </span>
         <cite class="ltx_cite ltx_citemacro_cite">
          <span class="ltx_text" id="S4.T3.9.9.9.2.2.1" style="font-size:90%;">
           [
          </span>
          <a class="ltx_ref" href="#bib.bib23" title="">
           23
          </a>
          <span class="ltx_text" id="S4.T3.9.9.9.2.3.2" style="font-size:90%;">
           ]
          </span>
         </cite>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.9.9.9.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.9.9.9.3.1" style="font-size:90%;">
          7B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.9.9.9.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.9.9.9.1.m1.1">
          <semantics id="S4.T3.9.9.9.1.m1.1a">
           <mi id="S4.T3.9.9.9.1.m1.1.1" mathsize="90%" mathvariant="normal" xref="S4.T3.9.9.9.1.m1.1.1.cmml">
            ‚úì
           </mi>
           <annotation-xml encoding="MathML-Content" id="S4.T3.9.9.9.1.m1.1b">
            <ci id="S4.T3.9.9.9.1.m1.1.1.cmml" xref="S4.T3.9.9.9.1.m1.1.1">
             ‚úì
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.9.9.9.1.m1.1c">
            \checkmark
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.9.9.9.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.9.9.9.4.1" style="font-size:90%;">
          61.6
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.9.9.9.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.9.9.9.5.1" style="font-size:90%;">
          47.6
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.9.9.9.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.9.9.9.6.1" style="font-size:90%;">
          23.8
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.9.9.9.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.9.9.9.7.1" style="font-size:90%;">
          37.6
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.9.9.9.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.9.9.9.8.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.9.9.9.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.9.9.9.9.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.9.9.9.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.9.9.9.10.1" style="font-size:90%;">
          -
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.9.9.9.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.9.9.9.11.1" style="font-size:90%;">
          -
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.10.10.10">
        <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.10.10.10.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.10.10.10.2.1" style="font-size:90%;">
          Emu
         </span>
         <cite class="ltx_cite ltx_citemacro_cite">
          <span class="ltx_text" id="S4.T3.10.10.10.2.2.1" style="font-size:90%;">
           [
          </span>
          <a class="ltx_ref" href="#bib.bib22" title="">
           22
          </a>
          <span class="ltx_text" id="S4.T3.10.10.10.2.3.2" style="font-size:90%;">
           ]
          </span>
         </cite>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.10.10.10.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.10.10.10.3.1" style="font-size:90%;">
          14B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.10.10.10.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.10.10.10.1.m1.1">
          <semantics id="S4.T3.10.10.10.1.m1.1a">
           <mi id="S4.T3.10.10.10.1.m1.1.1" mathsize="90%" mathvariant="normal" xref="S4.T3.10.10.10.1.m1.1.1.cmml">
            ‚úì
           </mi>
           <annotation-xml encoding="MathML-Content" id="S4.T3.10.10.10.1.m1.1b">
            <ci id="S4.T3.10.10.10.1.m1.1.1.cmml" xref="S4.T3.10.10.10.1.m1.1.1">
             ‚úì
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.10.10.10.1.m1.1c">
            \checkmark
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.10.10.10.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.10.10.10.4.1" style="font-size:90%;">
          112.4
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.10.10.10.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.10.10.10.5.1" style="font-size:90%;">
          52.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.10.10.10.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.10.10.10.6.1" style="font-size:90%;">
          38.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.10.10.10.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.10.10.10.7.1" style="font-size:90%;">
          34.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.10.10.10.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.10.10.10.8.1" style="font-size:90%;">
          47.3
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.10.10.10.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.10.10.10.9.1" style="font-size:90%;">
          18.8
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.10.10.10.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.10.10.10.10.1" style="font-size:90%;">
          8.3
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.10.10.10.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.10.10.10.11.1" style="font-size:90%;">
          19.6
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.11.11.11">
        <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.11.11.11.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.11.11.11.2.1" style="font-size:90%;">
          Emu-I
         </span>
         <cite class="ltx_cite ltx_citemacro_cite">
          <span class="ltx_text" id="S4.T3.11.11.11.2.2.1" style="font-size:90%;">
           [
          </span>
          <a class="ltx_ref" href="#bib.bib22" title="">
           22
          </a>
          <span class="ltx_text" id="S4.T3.11.11.11.2.3.2" style="font-size:90%;">
           ]
          </span>
         </cite>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.11.11.11.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.11.11.11.3.1" style="font-size:90%;">
          14B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.11.11.11.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.11.11.11.1.m1.1">
          <semantics id="S4.T3.11.11.11.1.m1.1a">
           <mo id="S4.T3.11.11.11.1.m1.1.1" mathsize="90%" xref="S4.T3.11.11.11.1.m1.1.1.cmml">
            √ó
           </mo>
           <annotation-xml encoding="MathML-Content" id="S4.T3.11.11.11.1.m1.1b">
            <times id="S4.T3.11.11.11.1.m1.1.1.cmml" xref="S4.T3.11.11.11.1.m1.1.1">
            </times>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.11.11.11.1.m1.1c">
            \times
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.11.11.11.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.11.11.11.4.1" style="font-size:90%;">
          117.7
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.11.11.11.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.11.11.11.5.1" style="font-size:90%;">
          40.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.11.11.11.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.11.11.11.6.1" style="font-size:90%;">
          34.7
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.11.11.11.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.11.11.11.7.1" style="font-size:90%;">
          35.4
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.11.11.11.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_ulem_uline" id="S4.T3.11.11.11.8.1" style="font-size:90%;">
          58.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.11.11.11.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.11.11.11.9.1" style="font-size:90%;">
          32.4
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.11.11.11.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.11.11.11.10.1" style="font-size:90%;">
          14.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.11.11.11.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.11.11.11.11.1" style="font-size:90%;">
          6.8
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.12.12.12">
        <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.12.12.12.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T3.12.12.12.2.1" style="font-size:90%;">
          SEED-LLaMA
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.12.12.12.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.12.12.12.3.1" style="font-size:90%;">
          8B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.12.12.12.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.12.12.12.1.m1.1">
          <semantics id="S4.T3.12.12.12.1.m1.1a">
           <mi id="S4.T3.12.12.12.1.m1.1.1" mathsize="90%" mathvariant="normal" xref="S4.T3.12.12.12.1.m1.1.1.cmml">
            ‚úì
           </mi>
           <annotation-xml encoding="MathML-Content" id="S4.T3.12.12.12.1.m1.1b">
            <ci id="S4.T3.12.12.12.1.m1.1.1.cmml" xref="S4.T3.12.12.12.1.m1.1.1">
             ‚úì
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.12.12.12.1.m1.1c">
            \checkmark
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.12.12.12.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.12.12.12.4.1" style="font-size:90%;">
          123.6
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.12.12.12.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.12.12.12.5.1" style="font-size:90%;">
          44.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.12.12.12.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.12.12.12.6.1" style="font-size:90%;">
          29.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.12.12.12.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.12.12.12.7.1" style="font-size:90%;">
          21.5
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.12.12.12.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.12.12.12.8.1" style="font-size:90%;">
          42.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.12.12.12.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.12.12.12.9.1" style="font-size:90%;">
          11.5
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.12.12.12.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.12.12.12.10.1" style="font-size:90%;">
          5.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.12.12.12.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.12.12.12.11.1" style="font-size:90%;">
          14.3
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.13.13.13">
        <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.13.13.13.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T3.13.13.13.2.1" style="font-size:90%;">
          SEED-LLaMA-I
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.13.13.13.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.13.13.13.3.1" style="font-size:90%;">
          8B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.13.13.13.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.13.13.13.1.m1.1">
          <semantics id="S4.T3.13.13.13.1.m1.1a">
           <mi id="S4.T3.13.13.13.1.m1.1.1" mathsize="90%" mathvariant="normal" xref="S4.T3.13.13.13.1.m1.1.1.cmml">
            ‚úì
           </mi>
           <annotation-xml encoding="MathML-Content" id="S4.T3.13.13.13.1.m1.1b">
            <ci id="S4.T3.13.13.13.1.m1.1.1.cmml" xref="S4.T3.13.13.13.1.m1.1.1">
             ‚úì
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.13.13.13.1.m1.1c">
            \checkmark
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.13.13.13.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.13.13.13.4.1" style="font-size:90%;">
          124.5
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.13.13.13.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T3.13.13.13.5.1" style="font-size:90%;">
          66.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.13.13.13.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T3.13.13.13.6.1" style="font-size:90%;">
          45.9
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.13.13.13.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T3.13.13.13.7.1" style="font-size:90%;">
          55.1
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.13.13.13.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.13.13.13.8.1" style="font-size:90%;">
          51.5
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.13.13.13.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.13.13.13.9.1" style="font-size:90%;">
          40.9
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.13.13.13.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_ulem_uline" id="S4.T3.13.13.13.10.1" style="font-size:90%;">
          30.8
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.13.13.13.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T3.13.13.13.11.1" style="font-size:90%;">
          24.9
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.14.14.14">
        <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.14.14.14.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T3.14.14.14.2.1" style="font-size:90%;">
          SEED-LLaMA
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.14.14.14.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.14.14.14.3.1" style="font-size:90%;">
          14B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.14.14.14.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.14.14.14.1.m1.1">
          <semantics id="S4.T3.14.14.14.1.m1.1a">
           <mi id="S4.T3.14.14.14.1.m1.1.1" mathsize="90%" mathvariant="normal" xref="S4.T3.14.14.14.1.m1.1.1.cmml">
            ‚úì
           </mi>
           <annotation-xml encoding="MathML-Content" id="S4.T3.14.14.14.1.m1.1b">
            <ci id="S4.T3.14.14.14.1.m1.1.1.cmml" xref="S4.T3.14.14.14.1.m1.1.1">
             ‚úì
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.14.14.14.1.m1.1c">
            \checkmark
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.14.14.14.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.14.14.14.4.1" style="font-size:90%;">
          125.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.14.14.14.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.14.14.14.5.1" style="font-size:90%;">
          48.1
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.14.14.14.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.14.14.14.6.1" style="font-size:90%;">
          27.1
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.14.14.14.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.14.14.14.7.1" style="font-size:90%;">
          23.3
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.14.14.14.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.14.14.14.8.1" style="font-size:90%;">
          46.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.14.14.14.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.14.14.14.9.1" style="font-size:90%;">
          13.9
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.14.14.14.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.14.14.14.10.1" style="font-size:90%;">
          3.7
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.14.14.14.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.14.14.14.11.1" style="font-size:90%;">
          11.3
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.15.15.15">
        <td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T3.15.15.15.2" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T3.15.15.15.2.1" style="font-size:90%;">
          SEED-LLaMA-I
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.15.15.15.3" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.15.3.1" style="font-size:90%;">
          14B
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.15.15.15.1" style="padding-left:3.0pt;padding-right:3.0pt;">
         <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T3.15.15.15.1.m1.1">
          <semantics id="S4.T3.15.15.15.1.m1.1a">
           <mi id="S4.T3.15.15.15.1.m1.1.1" mathsize="90%" mathvariant="normal" xref="S4.T3.15.15.15.1.m1.1.1.cmml">
            ‚úì
           </mi>
           <annotation-xml encoding="MathML-Content" id="S4.T3.15.15.15.1.m1.1b">
            <ci id="S4.T3.15.15.15.1.m1.1.1.cmml" xref="S4.T3.15.15.15.1.m1.1.1">
             ‚úì
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S4.T3.15.15.15.1.m1.1c">
            \checkmark
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.15.15.15.4" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_ulem_uline" id="S4.T3.15.15.15.4.1" style="font-size:90%;">
          126.9
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.15.15.15.5" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_ulem_uline" id="S4.T3.15.15.15.5.1" style="font-size:90%;">
          63.4
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.15.15.15.6" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.15.6.1" style="font-size:90%;">
          43.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.15.15.15.7" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_ulem_uline" id="S4.T3.15.15.15.7.1" style="font-size:90%;">
          49.4
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.15.15.15.8" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text" id="S4.T3.15.15.15.8.1" style="font-size:90%;">
          53.7
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.15.15.15.9" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T3.15.15.15.9.1" style="font-size:90%;">
          45.2
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.15.15.15.10" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T3.15.15.15.10.1" style="font-size:90%;">
          35.3
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.15.15.15.11" style="padding-left:3.0pt;padding-right:3.0pt;">
         <span class="ltx_text ltx_ulem_uline" id="S4.T3.15.15.15.11.1" style="font-size:90%;">
          24.7
         </span>
        </td>
       </tr>
      </table>
     </span>
    </div>
   </figure>
   <figure class="ltx_figure" id="S4.F6">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="446" id="S4.F6.g1" src="/html/2310.01218/assets/x6.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 6:
     </span>
     Qualitative examples of multi-turn in-context image and text
generation by SEED-LLaMA given multimodal instructions.
    </figcaption>
   </figure>
   <section class="ltx_subsubsection" id="S4.SS2.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.2.1
     </span>
     Quantitative Evaluation
    </h4>
    <div class="ltx_para" id="S4.SS2.SSS1.p1">
     <p class="ltx_p ltx_align_left" id="S4.SS2.SSS1.p1.1">
      <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p1.1.1">
       Multimodal Comprehension.
      </span>
     </p>
    </div>
    <div class="ltx_para" id="S4.SS2.SSS1.p2">
     <p class="ltx_p" id="S4.SS2.SSS1.p2.1">
      We evaluate SEED-LLaMA on a wide range of multimodal comprehension tasks including image captioning and image/video question answering. Details of these benchmarks and evaluation metrics are provided in Appendix.
      <a class="ltx_ref" href="#A4" title="Appendix D Evaluation ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
       <span class="ltx_text ltx_ref_tag">
        D
       </span>
      </a>
      . As shown in Tab.
      <a class="ltx_ref" href="#S4.T3" title="Table 3 ‚Ä£ 4.2 SEED-LLaMA ‚Ä£ 4 Experiment ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      , our SEED-LLaMA achieves competitive performance in both the image and video understanding tasks compared with MLLMs that use continuous visual representations. The results demonstrate that our SEED tokenizer can generate discrete visual codes with high-level semantics, which facilities the visual comprehension. We can observe that pretraining from a LLM with larger model size improves performance on SEED-Bench and instruction tuning further contributes to enhanced results. Note that as pointed out by recent work
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib43" title="">
        43
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib44" title="">
        44
       </a>
       ]
      </cite>
      , previous VQA benchmarks listed in Tab.
      <a class="ltx_ref" href="#S4.T3" title="Table 3 ‚Ä£ 4.2 SEED-LLaMA ‚Ä£ 4 Experiment ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      are not tailored for evaluating MLLMs with open-from output, since they require an exact match between the model prediction and the target word or phrase.
The qualitative examples of multimodal comprehension is provided in Appendix.
      <a class="ltx_ref" href="#A5" title="Appendix E Qualitative Cases ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
       <span class="ltx_text ltx_ref_tag">
        E
       </span>
      </a>
      .
     </p>
    </div>
    <div class="ltx_para" id="S4.SS2.SSS1.p3">
     <p class="ltx_p ltx_align_left" id="S4.SS2.SSS1.p3.1">
      <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p3.1.1">
       Text-to-image Generation.
      </span>
     </p>
    </div>
    <div class="ltx_para" id="S4.SS2.SSS1.p4">
     <p class="ltx_p" id="S4.SS2.SSS1.p4.1">
      We evaluate the text-to-image generation on MS-COCO
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib31" title="">
        31
       </a>
       ]
      </cite>
      and Flickr30K
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib36" title="">
        36
       </a>
       ]
      </cite>
      and compute the pair-wise CLIP similarity score as the evaluation metric following GILL
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib37" title="">
        37
       </a>
       ]
      </cite>
      . As shown in Tab.
      <span class="ltx_ref ltx_missing_label ltx_ref_self">
       LABEL:tab:clip_score
      </span>
      , images generated by our SEED-LLaMA from textual descriptions show higher similarity with the ground-truth images. The results demonstrate that SEED-LLaMA generates images that are highly correlated with text prompts via a frozen SD-UNet. We show qualitative examples of text-to-image generation in Appendix.
      <a class="ltx_ref" href="#A5" title="Appendix E Qualitative Cases ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
       <span class="ltx_text ltx_ref_tag">
        E
       </span>
      </a>
      .
     </p>
    </div>
    <figure class="ltx_figure" id="S4.F7">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="455" id="S4.F7.g1" src="/html/2310.01218/assets/x7.png" width="461"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 7:
      </span>
      Qualitative examples of compositional image generation by SEED-LLaMA.
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S4.SS2.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.2.2
     </span>
     Emergent Ability
    </h4>
    <div class="ltx_para" id="S4.SS2.SSS2.p1">
     <p class="ltx_p ltx_align_left" id="S4.SS2.SSS2.p1.1">
      <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p1.1.1">
       Multi-turn In-context Multimodal Generation.
      </span>
     </p>
    </div>
    <div class="ltx_para" id="S4.SS2.SSS2.p2">
     <p class="ltx_p" id="S4.SS2.SSS2.p2.1">
      As shown in Fig.
      <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
       <span class="ltx_text ltx_ref_tag">
        1
       </span>
      </a>
      and Fig.
      <a class="ltx_ref" href="#S4.F6" title="Figure 6 ‚Ä£ 4.2 SEED-LLaMA ‚Ä£ 4 Experiment ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
       <span class="ltx_text ltx_ref_tag">
        6
       </span>
      </a>
      , given multimodal instructions including images and open-form texts from a user, our SEED-LLaMA can respond with synthesized image (
      <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p2.1.1">
       e.g.
      </span>
      , a dog in front of the Golden Gate Bridge),
sequentially generated images (
      <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p2.1.2">
       e.g.
      </span>
      , a cartoon cat in different scenes), instruction-followed image (
      <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p2.1.3">
       e.g.
      </span>
      , a closer look-up of a cherry blossom), various forms of texts via creation and real-world knowledge (
      <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p2.1.4">
       e.g.
      </span>
      , a story, a poem and flower identification). The results illustrate the impressive capability of SEED-LLaMA in reasoning and generating long-context multimodal content.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS2.SSS2.p3">
     <p class="ltx_p ltx_align_left" id="S4.SS2.SSS2.p3.1">
      <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p3.1.1">
       Compositional Image Generation.
      </span>
     </p>
    </div>
    <div class="ltx_para" id="S4.SS2.SSS2.p4">
     <p class="ltx_p" id="S4.SS2.SSS2.p4.1">
      As shown in Fig.
      <a class="ltx_ref" href="#S4.F7" title="Figure 7 ‚Ä£ 4.2.1 Quantitative Evaluation ‚Ä£ 4.2 SEED-LLaMA ‚Ä£ 4 Experiment ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
       <span class="ltx_text ltx_ref_tag">
        7
       </span>
      </a>
      , our SEED-LLaMA can realize a variety of zero-shot compositional image generation as below,
     </p>
     <ul class="ltx_itemize" id="S4.I1">
      <li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        ‚Ä¢
       </span>
       <div class="ltx_para" id="S4.I1.i1.p1">
        <p class="ltx_p" id="S4.I1.i1.p1.1">
         Stylized Image Generation. SEED-LLaMA can take a text prompt and a style reference image as inputs and produce an output image that adheres to both the style and text prompt.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        ‚Ä¢
       </span>
       <div class="ltx_para" id="S4.I1.i2.p1">
        <p class="ltx_p" id="S4.I1.i2.p1.1">
         Image Blending. SEED-LLaMA can take two images as inputs and generate an image that blends the visual components of the input images.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        ‚Ä¢
       </span>
       <div class="ltx_para" id="S4.I1.i3.p1">
        <p class="ltx_p" id="S4.I1.i3.p1.1">
         Multimodal Composition. SEED-LLaMA can take an image prompt and a text prompt as inputs and generate a composite image that combines the multimodal inputs.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        ‚Ä¢
       </span>
       <div class="ltx_para" id="S4.I1.i4.p1">
        <p class="ltx_p" id="S4.I1.i4.p1.1">
         In-context Generation. SEED-LLaMA can take images, their textual references, and text prompts as inputs and generate context-related images.
        </p>
       </div>
      </li>
     </ul>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    Ablation Study
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p ltx_align_left" id="S4.SS3.p1.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">
      Generation Embedding.
     </span>
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p2">
    <p class="ltx_p" id="S4.SS3.p2.3">
     The generation embedding of SEED is aligned with the image embedding of unCLIP-SD, and can be decoded to realistic images with the unCLIP-SD-UNet. In our previous work
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib1" title="">
       1
      </a>
      ]
     </cite>
     , we train a visual tokenizer
     <math alttext="\text{SEED}^{\text{text}}" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1">
      <semantics id="S4.SS3.p2.1.m1.1a">
       <msup id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">
        <mtext id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2a.cmml">
         SEED
        </mtext>
        <mtext id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3a.cmml">
         text
        </mtext>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b">
        <apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">
         <csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">
          superscript
         </csymbol>
         <ci id="S4.SS3.p2.1.m1.1.1.2a.cmml" xref="S4.SS3.p2.1.m1.1.1.2">
          <mtext id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">
           SEED
          </mtext>
         </ci>
         <ci id="S4.SS3.p2.1.m1.1.1.3a.cmml" xref="S4.SS3.p2.1.m1.1.1.3">
          <mtext id="S4.SS3.p2.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p2.1.m1.1.1.3">
           text
          </mtext>
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">
        \text{SEED}^{\text{text}}
       </annotation>
      </semantics>
     </math>
     through aligning the generation embeddings with the text embeddings (77 tokens) of SD
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib12" title="">
       12
      </a>
      ]
     </cite>
     conditioned on texts. As shown in Tab.
     <span class="ltx_ref ltx_missing_label ltx_ref_self">
      LABEL:tab:clip_score
     </span>
     , the similarity between the reconstructed images of
     <math alttext="\text{SEED}^{\text{text}}" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1">
      <semantics id="S4.SS3.p2.2.m2.1a">
       <msup id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">
        <mtext id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2a.cmml">
         SEED
        </mtext>
        <mtext id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3a.cmml">
         text
        </mtext>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b">
        <apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">
         <csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">
          superscript
         </csymbol>
         <ci id="S4.SS3.p2.2.m2.1.1.2a.cmml" xref="S4.SS3.p2.2.m2.1.1.2">
          <mtext id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">
           SEED
          </mtext>
         </ci>
         <ci id="S4.SS3.p2.2.m2.1.1.3a.cmml" xref="S4.SS3.p2.2.m2.1.1.3">
          <mtext id="S4.SS3.p2.2.m2.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p2.2.m2.1.1.3">
           text
          </mtext>
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">
        \text{SEED}^{\text{text}}
       </annotation>
      </semantics>
     </math>
     and original images drop heavily. The semantic representations of texts can not fully preserve the rich visual information of images. The visual comparison of the the reconstructed images between
     <math alttext="\text{SEED}^{\text{text}}" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1">
      <semantics id="S4.SS3.p2.3.m3.1a">
       <msup id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml">
        <mtext id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2a.cmml">
         SEED
        </mtext>
        <mtext id="S4.SS3.p2.3.m3.1.1.3" xref="S4.SS3.p2.3.m3.1.1.3a.cmml">
         text
        </mtext>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b">
        <apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">
         <csymbol cd="ambiguous" id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">
          superscript
         </csymbol>
         <ci id="S4.SS3.p2.3.m3.1.1.2a.cmml" xref="S4.SS3.p2.3.m3.1.1.2">
          <mtext id="S4.SS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.2">
           SEED
          </mtext>
         </ci>
         <ci id="S4.SS3.p2.3.m3.1.1.3a.cmml" xref="S4.SS3.p2.3.m3.1.1.3">
          <mtext id="S4.SS3.p2.3.m3.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p2.3.m3.1.1.3">
           text
          </mtext>
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">
        \text{SEED}^{\text{text}}
       </annotation>
      </semantics>
     </math>
     and SEED are provided in Appendix.
     <a class="ltx_ref" href="#A1" title="Appendix A SEED Tokenizer ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
      <span class="ltx_text ltx_ref_tag">
       A
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p3">
    <p class="ltx_p ltx_align_left" id="S4.SS3.p3.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">
      Causal Visual Codes vs. Bilateral Visual Codes.
     </span>
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p4">
    <p class="ltx_p" id="S4.SS3.p4.8">
     We train a Causal Q-Former to convert 2D features produced by the ViT encoder into a sequence of causal semantic
embeddings, which are further discretized as causal visual codes. To verify whether the causal visual codes are necessary for compatibility with LLM, we train a visual tokenizer
     <math alttext="\text{SEED}^{\text{Bi}}" class="ltx_Math" display="inline" id="S4.SS3.p4.1.m1.1">
      <semantics id="S4.SS3.p4.1.m1.1a">
       <msup id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml">
        <mtext id="S4.SS3.p4.1.m1.1.1.2" xref="S4.SS3.p4.1.m1.1.1.2a.cmml">
         SEED
        </mtext>
        <mtext id="S4.SS3.p4.1.m1.1.1.3" xref="S4.SS3.p4.1.m1.1.1.3a.cmml">
         Bi
        </mtext>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b">
        <apply id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1">
         <csymbol cd="ambiguous" id="S4.SS3.p4.1.m1.1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1">
          superscript
         </csymbol>
         <ci id="S4.SS3.p4.1.m1.1.1.2a.cmml" xref="S4.SS3.p4.1.m1.1.1.2">
          <mtext id="S4.SS3.p4.1.m1.1.1.2.cmml" xref="S4.SS3.p4.1.m1.1.1.2">
           SEED
          </mtext>
         </ci>
         <ci id="S4.SS3.p4.1.m1.1.1.3a.cmml" xref="S4.SS3.p4.1.m1.1.1.3">
          <mtext id="S4.SS3.p4.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p4.1.m1.1.1.3">
           Bi
          </mtext>
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">
        \text{SEED}^{\text{Bi}}
       </annotation>
      </semantics>
     </math>
     , which produces bilateral visual codes from a pre-trained Q-Former with bilateral self-attention. We then pre-train
     <math alttext="\text{SEED}^{\text{Bi}}" class="ltx_Math" display="inline" id="S4.SS3.p4.2.m2.1">
      <semantics id="S4.SS3.p4.2.m2.1a">
       <msup id="S4.SS3.p4.2.m2.1.1" xref="S4.SS3.p4.2.m2.1.1.cmml">
        <mtext id="S4.SS3.p4.2.m2.1.1.2" xref="S4.SS3.p4.2.m2.1.1.2a.cmml">
         SEED
        </mtext>
        <mtext id="S4.SS3.p4.2.m2.1.1.3" xref="S4.SS3.p4.2.m2.1.1.3a.cmml">
         Bi
        </mtext>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S4.SS3.p4.2.m2.1b">
        <apply id="S4.SS3.p4.2.m2.1.1.cmml" xref="S4.SS3.p4.2.m2.1.1">
         <csymbol cd="ambiguous" id="S4.SS3.p4.2.m2.1.1.1.cmml" xref="S4.SS3.p4.2.m2.1.1">
          superscript
         </csymbol>
         <ci id="S4.SS3.p4.2.m2.1.1.2a.cmml" xref="S4.SS3.p4.2.m2.1.1.2">
          <mtext id="S4.SS3.p4.2.m2.1.1.2.cmml" xref="S4.SS3.p4.2.m2.1.1.2">
           SEED
          </mtext>
         </ci>
         <ci id="S4.SS3.p4.2.m2.1.1.3a.cmml" xref="S4.SS3.p4.2.m2.1.1.3">
          <mtext id="S4.SS3.p4.2.m2.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p4.2.m2.1.1.3">
           Bi
          </mtext>
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS3.p4.2.m2.1c">
        \text{SEED}^{\text{Bi}}
       </annotation>
      </semantics>
     </math>
     -
     <math alttext="\text{LLM}^{\ast}" class="ltx_Math" display="inline" id="S4.SS3.p4.3.m3.1">
      <semantics id="S4.SS3.p4.3.m3.1a">
       <msup id="S4.SS3.p4.3.m3.1.1" xref="S4.SS3.p4.3.m3.1.1.cmml">
        <mtext id="S4.SS3.p4.3.m3.1.1.2" xref="S4.SS3.p4.3.m3.1.1.2a.cmml">
         LLM
        </mtext>
        <mo id="S4.SS3.p4.3.m3.1.1.3" xref="S4.SS3.p4.3.m3.1.1.3.cmml">
         ‚àó
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S4.SS3.p4.3.m3.1b">
        <apply id="S4.SS3.p4.3.m3.1.1.cmml" xref="S4.SS3.p4.3.m3.1.1">
         <csymbol cd="ambiguous" id="S4.SS3.p4.3.m3.1.1.1.cmml" xref="S4.SS3.p4.3.m3.1.1">
          superscript
         </csymbol>
         <ci id="S4.SS3.p4.3.m3.1.1.2a.cmml" xref="S4.SS3.p4.3.m3.1.1.2">
          <mtext id="S4.SS3.p4.3.m3.1.1.2.cmml" xref="S4.SS3.p4.3.m3.1.1.2">
           LLM
          </mtext>
         </ci>
         <ci id="S4.SS3.p4.3.m3.1.1.3.cmml" xref="S4.SS3.p4.3.m3.1.1.3">
          ‚àó
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS3.p4.3.m3.1c">
        \text{LLM}^{\ast}
       </annotation>
      </semantics>
     </math>
     and
     <span class="ltx_text ltx_markedasmath" id="S4.SS3.p4.8.1">
      SEED
     </span>
     -
     <math alttext="\text{LLM}^{\ast}" class="ltx_Math" display="inline" id="S4.SS3.p4.5.m5.1">
      <semantics id="S4.SS3.p4.5.m5.1a">
       <msup id="S4.SS3.p4.5.m5.1.1" xref="S4.SS3.p4.5.m5.1.1.cmml">
        <mtext id="S4.SS3.p4.5.m5.1.1.2" xref="S4.SS3.p4.5.m5.1.1.2a.cmml">
         LLM
        </mtext>
        <mo id="S4.SS3.p4.5.m5.1.1.3" xref="S4.SS3.p4.5.m5.1.1.3.cmml">
         ‚àó
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S4.SS3.p4.5.m5.1b">
        <apply id="S4.SS3.p4.5.m5.1.1.cmml" xref="S4.SS3.p4.5.m5.1.1">
         <csymbol cd="ambiguous" id="S4.SS3.p4.5.m5.1.1.1.cmml" xref="S4.SS3.p4.5.m5.1.1">
          superscript
         </csymbol>
         <ci id="S4.SS3.p4.5.m5.1.1.2a.cmml" xref="S4.SS3.p4.5.m5.1.1.2">
          <mtext id="S4.SS3.p4.5.m5.1.1.2.cmml" xref="S4.SS3.p4.5.m5.1.1.2">
           LLM
          </mtext>
         </ci>
         <ci id="S4.SS3.p4.5.m5.1.1.3.cmml" xref="S4.SS3.p4.5.m5.1.1.3">
          ‚àó
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS3.p4.5.m5.1c">
        \text{LLM}^{\ast}
       </annotation>
      </semantics>
     </math>
     on image-text pairs and evaluate the text-to-image generation on COCO test set. Given 5000 captions of COCO,
     <math alttext="\text{SEED}^{\text{Bi}}" class="ltx_Math" display="inline" id="S4.SS3.p4.6.m6.1">
      <semantics id="S4.SS3.p4.6.m6.1a">
       <msup id="S4.SS3.p4.6.m6.1.1" xref="S4.SS3.p4.6.m6.1.1.cmml">
        <mtext id="S4.SS3.p4.6.m6.1.1.2" xref="S4.SS3.p4.6.m6.1.1.2a.cmml">
         SEED
        </mtext>
        <mtext id="S4.SS3.p4.6.m6.1.1.3" xref="S4.SS3.p4.6.m6.1.1.3a.cmml">
         Bi
        </mtext>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S4.SS3.p4.6.m6.1b">
        <apply id="S4.SS3.p4.6.m6.1.1.cmml" xref="S4.SS3.p4.6.m6.1.1">
         <csymbol cd="ambiguous" id="S4.SS3.p4.6.m6.1.1.1.cmml" xref="S4.SS3.p4.6.m6.1.1">
          superscript
         </csymbol>
         <ci id="S4.SS3.p4.6.m6.1.1.2a.cmml" xref="S4.SS3.p4.6.m6.1.1.2">
          <mtext id="S4.SS3.p4.6.m6.1.1.2.cmml" xref="S4.SS3.p4.6.m6.1.1.2">
           SEED
          </mtext>
         </ci>
         <ci id="S4.SS3.p4.6.m6.1.1.3a.cmml" xref="S4.SS3.p4.6.m6.1.1.3">
          <mtext id="S4.SS3.p4.6.m6.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p4.6.m6.1.1.3">
           Bi
          </mtext>
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS3.p4.6.m6.1c">
        \text{SEED}^{\text{Bi}}
       </annotation>
      </semantics>
     </math>
     -LLM only generates 2134 images successfully while
     <span class="ltx_text ltx_markedasmath" id="S4.SS3.p4.8.2">
      SEED
     </span>
     -
     <math alttext="\text{LLM}^{\ast}" class="ltx_Math" display="inline" id="S4.SS3.p4.8.m8.1">
      <semantics id="S4.SS3.p4.8.m8.1a">
       <msup id="S4.SS3.p4.8.m8.1.1" xref="S4.SS3.p4.8.m8.1.1.cmml">
        <mtext id="S4.SS3.p4.8.m8.1.1.2" xref="S4.SS3.p4.8.m8.1.1.2a.cmml">
         LLM
        </mtext>
        <mo id="S4.SS3.p4.8.m8.1.1.3" xref="S4.SS3.p4.8.m8.1.1.3.cmml">
         ‚àó
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S4.SS3.p4.8.m8.1b">
        <apply id="S4.SS3.p4.8.m8.1.1.cmml" xref="S4.SS3.p4.8.m8.1.1">
         <csymbol cd="ambiguous" id="S4.SS3.p4.8.m8.1.1.1.cmml" xref="S4.SS3.p4.8.m8.1.1">
          superscript
         </csymbol>
         <ci id="S4.SS3.p4.8.m8.1.1.2a.cmml" xref="S4.SS3.p4.8.m8.1.1.2">
          <mtext id="S4.SS3.p4.8.m8.1.1.2.cmml" xref="S4.SS3.p4.8.m8.1.1.2">
           LLM
          </mtext>
         </ci>
         <ci id="S4.SS3.p4.8.m8.1.1.3.cmml" xref="S4.SS3.p4.8.m8.1.1.3">
          ‚àó
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS3.p4.8.m8.1c">
        \text{LLM}^{\ast}
       </annotation>
      </semantics>
     </math>
     generates 4997 images (Failure cases occur when the model predicts a number of visual tokens not equal to 32). The results demonstrate that the non-causal codes lead to highly unstable model performance since they contradict with the left-to-right autoregressive mechanism of LLM.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p5">
    <p class="ltx_p ltx_align_left" id="S4.SS3.p5.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS3.p5.1.1">
      SEED-LLaMA Pretraining.
     </span>
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p6">
    <p class="ltx_p" id="S4.SS3.p6.1">
     We first train SEED-LLaMA using LoRA tuning, and then merge the parameters of LoRA with the original LLM and fine-tune all parameters except for the embedding layer.
    </p>
   </div>
   <figure class="ltx_table ltx_align_floatright" id="S4.T4">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 4:
     </span>
     Evaluation of image captioning and text-to-image generation on COCO test set.
    </figcaption>
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.1" style="width:173.4pt;height:53.2pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-12.6pt,3.9pt) scale(0.87274040333396,0.87274040333396) ;">
      <table class="ltx_tabular ltx_align_middle" id="S4.T4.1.1">
       <tr class="ltx_tr" id="S4.T4.1.1.1">
        <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.1">
         Pretraining
        </td>
        <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.2">
         Captioning
        </td>
        <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.3">
         Generation
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T4.1.1.2">
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.1">
         LoRA
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.2">
         124.5
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.3">
         68.87
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T4.1.1.3">
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.3.1">
         LoRA + Fully
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.3.2">
         <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.3.2.1">
          125.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.3.3">
         <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.3.3.1">
          69.07
         </span>
        </td>
       </tr>
      </table>
     </span>
    </div>
   </figure>
   <div class="ltx_para" id="S4.SS3.p7">
    <p class="ltx_p" id="S4.SS3.p7.1">
     To explore whether fully fine-tuning helps, we evaluate the performance of the model before and after fully fine-tuning on image captioning and text-to-image generation, with evaluation metric CIDEr and clip similarity score. Tab.
     <span class="ltx_ref ltx_missing_label ltx_ref_self">
      LABEL:tab:ablation
     </span>
     shows that fully fine-tuning the LoRA tuned model enhances model‚Äôs capability for both image comprehension and generation.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    We present SEED, a discrete image tokenizer, designed based on the premise that visual tokens compatible with LLMs should capture high-level semantics while being generated with 1D causal dependency. SEED enables LLMs to be trained with multimodal data following the original recipe of text (
    <span class="ltx_text ltx_font_italic" id="S5.p1.1.1">
     i.e.
    </span>
    , next-word prediction), which is mature and scalable. We further present
SEED-LLaMA via multimodal pretraining and instruction tuning on the interleaved visual and textual data with SEED tokenizer. SEED-LLaMA not only exhibits remarkable performance across multimodal comprehension and image generation tasks, but also demonstrates extensive compositional emergent abilities. We hope that SEED would draw increased attention to visual tokenizers. A more rational visual tokenizer could substantially reduce the complexity of multimodal LLM training.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_tag_bibitem">
     [1]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
      Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">
      Planting a seed of vision in large language model.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib1.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2307.08041
     </span>
     <span class="ltx_text" id="bib.bib1.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_tag_bibitem">
     [2]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
      Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">
      Llama: Open and efficient foundation language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib2.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2302.13971
     </span>
     <span class="ltx_text" id="bib.bib2.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_tag_bibitem">
     [3]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
      Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared¬†D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">
      Language models are few-shot learners.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib3.3.1" style="font-size:90%;">
      Advances in neural information processing systems
     </span>
     <span class="ltx_text" id="bib.bib3.4.2" style="font-size:90%;">
      , 33:1877‚Äì1901, 2020.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_tag_bibitem">
     [4]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
      Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung¬†Won Chung, Charles Sutton, Sebastian Gehrmann, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">
      Palm: Scaling language modeling with pathways.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib4.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2204.02311
     </span>
     <span class="ltx_text" id="bib.bib4.4.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_tag_bibitem">
     [5]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
      Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">
      Zero-shot text-to-image generation.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib5.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib5.4.2" style="font-size:90%;">
      International Conference on Machine Learning
     </span>
     <span class="ltx_text" id="bib.bib5.5.3" style="font-size:90%;">
      , pages 8821‚Äì8831. PMLR, 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_tag_bibitem">
     [6]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
      Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da¬†Yin, Junyang Lin, Xu¬†Zou, Zhou Shao, Hongxia Yang, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">
      Cogview: Mastering text-to-image generation via transformers.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib6.3.1" style="font-size:90%;">
      Advances in Neural Information Processing Systems
     </span>
     <span class="ltx_text" id="bib.bib6.4.2" style="font-size:90%;">
      , 34:19822‚Äì19835, 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_tag_bibitem">
     [7]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
      Aaron Van Den¬†Oord, Oriol Vinyals, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">
      Neural discrete representation learning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib7.3.1" style="font-size:90%;">
      Advances in neural information processing systems
     </span>
     <span class="ltx_text" id="bib.bib7.4.2" style="font-size:90%;">
      , 30, 2017.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_tag_bibitem">
     [8]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
      Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">
      Eva-clip: Improved training techniques for clip at scale.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib8.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2303.15389
     </span>
     <span class="ltx_text" id="bib.bib8.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_tag_bibitem">
     [9]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
      Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">
      An image is worth 16x16 words: Transformers for image recognition at scale.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib9.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2010.11929
     </span>
     <span class="ltx_text" id="bib.bib9.4.2" style="font-size:90%;">
      , 2020.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_tag_bibitem">
     [10]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
      Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">
      U-net: Convolutional networks for biomedical image segmentation.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib10.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib10.4.2" style="font-size:90%;">
      Medical Image Computing and Computer-Assisted Intervention‚ÄìMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18
     </span>
     <span class="ltx_text" id="bib.bib10.5.3" style="font-size:90%;">
      , pages 234‚Äì241. Springer, 2015.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_tag_bibitem">
     [11]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
      Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">
      Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib11.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2301.12597
     </span>
     <span class="ltx_text" id="bib.bib11.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_tag_bibitem">
     [12]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
      Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">
      High-resolution image synthesis with latent diffusion models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib12.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib12.4.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
     </span>
     <span class="ltx_text" id="bib.bib12.5.3" style="font-size:90%;">
      , pages 10684‚Äì10695, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_tag_bibitem">
     [13]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
      Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">
      Hierarchical text-conditional image generation with clip latents.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib13.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2204.06125
     </span>
     <span class="ltx_text" id="bib.bib13.4.2" style="font-size:90%;">
      , 1(2):3, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_tag_bibitem">
     [14]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
      Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">
      mplug-owl: Modularization empowers large language models with multimodality.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib14.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2304.14178
     </span>
     <span class="ltx_text" id="bib.bib14.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_tag_bibitem">
     [15]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
      Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">
      Minigpt-4: Enhancing vision-language understanding with advanced large language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib15.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2304.10592
     </span>
     <span class="ltx_text" id="bib.bib15.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_tag_bibitem">
     [16]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
      Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu¬†Qiao.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">
      Llama-adapter: Efficient fine-tuning of language models with zero-init attention.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib16.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2303.16199
     </span>
     <span class="ltx_text" id="bib.bib16.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_tag_bibitem">
     [17]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
      Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">
      Llama-adapter v2: Parameter-efficient visual instruction model.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib17.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2304.15010
     </span>
     <span class="ltx_text" id="bib.bib17.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_tag_bibitem">
     [18]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
      Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong¬†Jae Lee.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">
      Visual instruction tuning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib18.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2304.08485
     </span>
     <span class="ltx_text" id="bib.bib18.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_tag_bibitem">
     [19]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
      Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">
      Flamingo: a visual language model for few-shot learning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib19.3.1" style="font-size:90%;">
      Advances in Neural Information Processing Systems
     </span>
     <span class="ltx_text" id="bib.bib19.4.2" style="font-size:90%;">
      , 35:23716‚Äì23736, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_tag_bibitem">
     [20]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
      Danny Driess, Fei Xia, Mehdi¬†SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">
      Palm-e: An embodied multimodal language model.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib20.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2303.03378
     </span>
     <span class="ltx_text" id="bib.bib20.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_tag_bibitem">
     [21]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
      Jing¬†Yu Koh, Daniel Fried, and Ruslan Salakhutdinov.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">
      Generating images with multimodal language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib21.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2305.17216
     </span>
     <span class="ltx_text" id="bib.bib21.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_tag_bibitem">
     [22]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
      Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">
      Generative pretraining in multimodality.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib22.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2307.05222
     </span>
     <span class="ltx_text" id="bib.bib22.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_tag_bibitem">
     [23]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
      Yu¬†Lili, Shi Bowen, Pasunuru Ram, Miller Benjamin, Golovneva Olga, Wang Tianlu, Babu Arun, Tang Binh, Karrer Brian, Sheynin Shelly, Ross Candace, Polyak Adam, Howes Russ, Sharma Vasu, Xu¬†Jacob, Singer Uriel, Li¬†(AI) Daniel, Ghosh Gargi, Taigman Yaniv, Fazel-Zarandi Maryam, Celikyilmaz Asli, Zettlemoyer Luke, and Aghajanyan Armen.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">
      Scaling autoregressive multi-modal models: Pretraining and instruction tuning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib23.3.1" style="font-size:90%;">
      2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_tag_bibitem">
     [24]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
      Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">
      Make-a-scene: Scene-based text-to-image generation with human priors.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib24.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib24.4.2" style="font-size:90%;">
      European Conference on Computer Vision
     </span>
     <span class="ltx_text" id="bib.bib24.5.3" style="font-size:90%;">
      , pages 89‚Äì106. Springer, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_tag_bibitem">
     [25]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
      Patrick Esser, Robin Rombach, and Bjorn Ommer.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">
      Taming transformers for high-resolution image synthesis.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib25.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib25.4.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
     </span>
     <span class="ltx_text" id="bib.bib25.5.3" style="font-size:90%;">
      , pages 12873‚Äì12883, 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_tag_bibitem">
     [26]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
      Yuchao Gu, Xintao Wang, Yixiao Ge, Ying Shan, Xiaohu Qie, and Mike¬†Zheng Shou.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">
      Rethinking the objectives of vector-quantized tokenizers for image synthesis.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib26.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2212.03185
     </span>
     <span class="ltx_text" id="bib.bib26.4.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_tag_bibitem">
     [27]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
      Zhiliang Peng, Li¬†Dong, Hangbo Bao, Qixiang Ye, and Furu Wei.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">
      Beit v2: Masked image modeling with vector-quantized visual tokenizers.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib27.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2208.06366
     </span>
     <span class="ltx_text" id="bib.bib27.4.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_tag_bibitem">
     [28]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
      Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">
      Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib28.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib28.4.2" style="font-size:90%;">
      Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
     </span>
     <span class="ltx_text" id="bib.bib28.5.3" style="font-size:90%;">
      , pages 2556‚Äì2565, 2018.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_tag_bibitem">
     [29]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
      Ali¬†Zahid Luke¬†Chesser, Timothy¬†Carbone.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">
      Unsplash.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib29.3.1" style="font-size:90%;">
      https://github.com/unsplash/datasets, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_tag_bibitem">
     [30]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
      Schuhmann Christoph, K√∂pf Andreas, Vencu Richard, Coombes Theo, and Beaumont Romain.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">
      Laion coco: 600m synthetic captions from laion2b-en.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib30.3.1" style="font-size:90%;">
      [EB/OL], 2022.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib30.4.1" style="font-size:90%;">
      https://laion.ai/blog/laion-coco/.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_tag_bibitem">
     [31]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
      Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll√°r, and C¬†Lawrence Zitnick.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">
      Microsoft coco captions: Data collection and evaluation server.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib31.3.1" style="font-size:90%;">
      arXiv preprint arXiv:1504.00325
     </span>
     <span class="ltx_text" id="bib.bib31.4.2" style="font-size:90%;">
      , 2015.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_tag_bibitem">
     [32]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
      Edward¬†J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu¬†Wang, and Weizhu Chen.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">
      Lora: Low-rank adaptation of large language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib32.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2106.09685
     </span>
     <span class="ltx_text" id="bib.bib32.4.2" style="font-size:90%;">
      , 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_tag_bibitem">
     [33]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
      Teven¬†Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Iliƒá, Daniel Hesslow, Roman Castagn√©, Alexandra¬†Sasha Luccioni, Fran√ßois Yvon, Matthias Gall√©, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">
      Bloom: A 176b-parameter open-access multilingual language model.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib33.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2211.05100
     </span>
     <span class="ltx_text" id="bib.bib33.4.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_tag_bibitem">
     [34]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
      Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">
      Glm-130b: An open bilingual pre-trained model.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib34.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2210.02414
     </span>
     <span class="ltx_text" id="bib.bib34.4.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_tag_bibitem">
     [35]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
      Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C¬†Lawrence Zitnick.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">
      Microsoft coco: Common objects in context.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib35.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib35.4.2" style="font-size:90%;">
      Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13
     </span>
     <span class="ltx_text" id="bib.bib35.5.3" style="font-size:90%;">
      , pages 740‚Äì755. Springer, 2014.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_tag_bibitem">
     [36]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
      Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">
      From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib36.3.1" style="font-size:90%;">
      Transactions of the Association for Computational Linguistics
     </span>
     <span class="ltx_text" id="bib.bib36.4.2" style="font-size:90%;">
      , 2:67‚Äì78, 2014.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_tag_bibitem">
     [37]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
      Jing¬†Yu Koh, Daniel Fried, and Ruslan Salakhutdinov.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">
      Generating images with multimodal language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib37.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2305.17216
     </span>
     <span class="ltx_text" id="bib.bib37.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_tag_bibitem">
     [38]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
      Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">
      Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib38.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib38.4.2" style="font-size:90%;">
      International Conference on Machine Learning
     </span>
     <span class="ltx_text" id="bib.bib38.5.3" style="font-size:90%;">
      , pages 12888‚Äì12900. PMLR, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_tag_bibitem">
     [39]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
      Shaohan Huang, Li¬†Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais¬†Khan Mohammed, Qiang Liu, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">
      Language is not all you need: Aligning perception with language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib39.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2302.14045
     </span>
     <span class="ltx_text" id="bib.bib39.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_tag_bibitem">
     [40]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
      Zhiliang Peng, Wenhui Wang, Li¬†Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib40.2.1" style="font-size:90%;">
      Kosmos-2: Grounding multimodal large language models to the world.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib40.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2306.14824
     </span>
     <span class="ltx_text" id="bib.bib40.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_tag_bibitem">
     [41]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib41.1.1" style="font-size:90%;">
      Yaru Hao, Haoyu Song, Li¬†Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib41.2.1" style="font-size:90%;">
      Language models are general-purpose interfaces.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib41.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2206.06336
     </span>
     <span class="ltx_text" id="bib.bib41.4.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_tag_bibitem">
     [42]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib42.1.1" style="font-size:90%;">
      Hugo Lauren√ßon, Lucile Saulnier, L√©o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander¬†M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib42.2.1" style="font-size:90%;">
      Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_tag_bibitem">
     [43]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib43.1.1" style="font-size:90%;">
      Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo¬†Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib43.2.1" style="font-size:90%;">
      Mmbench: Is your multi-modal model an all-around player?
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib43.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2307.06281
     </span>
     <span class="ltx_text" id="bib.bib43.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_tag_bibitem">
     [44]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib44.1.1" style="font-size:90%;">
      Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib44.2.1" style="font-size:90%;">
      Seed-bench: Benchmarking multimodal llms with generative comprehension.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib44.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2307.16125
     </span>
     <span class="ltx_text" id="bib.bib44.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_tag_bibitem">
     [45]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib45.1.1" style="font-size:90%;">
      Max Bain, Arsha Nagrani, G√ºl Varol, and Andrew Zisserman.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib45.2.1" style="font-size:90%;">
      Frozen in time: A joint video and image encoder for end-to-end retrieval.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib45.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib45.4.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF International Conference on Computer Vision
     </span>
     <span class="ltx_text" id="bib.bib45.5.3" style="font-size:90%;">
      , pages 1728‚Äì1738, 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_tag_bibitem">
     [46]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib46.1.1" style="font-size:90%;">
      Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir¬†Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William¬†Yang Wang, and Yejin Choi.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib46.2.1" style="font-size:90%;">
      Multimodal c4: An open, billion-scale corpus of images interleaved with text.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib46.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2304.06939
     </span>
     <span class="ltx_text" id="bib.bib46.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_tag_bibitem">
     [47]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib47.1.1" style="font-size:90%;">
      Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu¬†Su.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib47.2.1" style="font-size:90%;">
      Magicbrush: A manually annotated dataset for instruction-guided image editing.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib47.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2306.10012
     </span>
     <span class="ltx_text" id="bib.bib47.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_tag_bibitem">
     [48]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib48.1.1" style="font-size:90%;">
      Junting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi¬†Wang, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib48.2.1" style="font-size:90%;">
      Journeydb: A benchmark for generative image understanding.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib48.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2307.00716
     </span>
     <span class="ltx_text" id="bib.bib48.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_tag_bibitem">
     [49]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib49.1.1" style="font-size:90%;">
      Zijie¬†J Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen¬†Horng Chau.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib49.2.1" style="font-size:90%;">
      Diffusiondb: A large-scale prompt gallery dataset for text-to-image generative models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib49.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2210.14896
     </span>
     <span class="ltx_text" id="bib.bib49.4.2" style="font-size:90%;">
      , 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_tag_bibitem">
     [50]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib50.1.1" style="font-size:90%;">
      Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et¬†al.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib50.2.1" style="font-size:90%;">
      Visual storytelling.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib50.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib50.4.2" style="font-size:90%;">
      Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Human language technologies
     </span>
     <span class="ltx_text" id="bib.bib50.5.3" style="font-size:90%;">
      , pages 1233‚Äì1239, 2016.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib51">
    <span class="ltx_tag ltx_tag_bibitem">
     [51]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib51.1.1" style="font-size:90%;">
      Tim Brooks, Aleksander Holynski, and Alexei¬†A Efros.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib51.2.1" style="font-size:90%;">
      Instructpix2pix: Learning to follow image editing instructions.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib51.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib51.4.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </span>
     <span class="ltx_text" id="bib.bib51.5.3" style="font-size:90%;">
      , pages 18392‚Äì18402, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib52">
    <span class="ltx_tag ltx_tag_bibitem">
     [52]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib52.1.1" style="font-size:90%;">
      Bo¬†Zhao, Boya Wu, and Tiejun Huang.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib52.2.1" style="font-size:90%;">
      Svit: Scaling up visual instruction tuning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib52.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2307.04087
     </span>
     <span class="ltx_text" id="bib.bib52.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib53">
    <span class="ltx_tag ltx_tag_bibitem">
     [53]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib53.1.1" style="font-size:90%;">
      Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib53.2.1" style="font-size:90%;">
      Llavar: Enhanced visual instruction tuning for text-rich image understanding.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib53.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2306.17107
     </span>
     <span class="ltx_text" id="bib.bib53.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib54">
    <span class="ltx_tag ltx_tag_bibitem">
     [54]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib54.1.1" style="font-size:90%;">
      Bo¬†Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib54.2.1" style="font-size:90%;">
      Mimic-it: Multi-modal in-context instruction tuning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib54.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2306.05425
     </span>
     <span class="ltx_text" id="bib.bib54.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib55">
    <span class="ltx_tag ltx_tag_bibitem">
     [55]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib55.1.1" style="font-size:90%;">
      Fangyu Liu, Guy Emerson, and Nigel Collier.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib55.2.1" style="font-size:90%;">
      Visual spatial reasoning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib55.3.1" style="font-size:90%;">
      Transactions of the Association for Computational Linguistics
     </span>
     <span class="ltx_text" id="bib.bib55.4.2" style="font-size:90%;">
      , 11:635‚Äì651, 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib56">
    <span class="ltx_tag ltx_tag_bibitem">
     [56]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib56.1.1" style="font-size:90%;">
      Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib56.2.1" style="font-size:90%;">
      Textcaps: a dataset for image captioning with reading comprehension.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib56.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib56.4.2" style="font-size:90%;">
      Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part II 16
     </span>
     <span class="ltx_text" id="bib.bib56.5.3" style="font-size:90%;">
      , pages 742‚Äì758. Springer, 2020.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib57">
    <span class="ltx_tag ltx_tag_bibitem">
     [57]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib57.1.1" style="font-size:90%;">
      Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib57.2.1" style="font-size:90%;">
      Making the v in vqa matter: Elevating the role of image understanding in visual question answering.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib57.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib57.4.2" style="font-size:90%;">
      Proceedings of the IEEE conference on computer vision and pattern recognition
     </span>
     <span class="ltx_text" id="bib.bib57.5.3" style="font-size:90%;">
      , pages 6904‚Äì6913, 2017.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib58">
    <span class="ltx_tag ltx_tag_bibitem">
     [58]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib58.1.1" style="font-size:90%;">
      Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib58.2.1" style="font-size:90%;">
      Ok-vqa: A visual question answering benchmark requiring external knowledge.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib58.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib58.4.2" style="font-size:90%;">
      Proceedings of the IEEE/cvf conference on computer vision and pattern recognition
     </span>
     <span class="ltx_text" id="bib.bib58.5.3" style="font-size:90%;">
      , pages 3195‚Äì3204, 2019.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib59">
    <span class="ltx_tag ltx_tag_bibitem">
     [59]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib59.1.1" style="font-size:90%;">
      Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib59.2.1" style="font-size:90%;">
      A-okvqa: A benchmark for visual question answering using world knowledge.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib59.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib59.4.2" style="font-size:90%;">
      European Conference on Computer Vision
     </span>
     <span class="ltx_text" id="bib.bib59.5.3" style="font-size:90%;">
      , pages 146‚Äì162. Springer, 2022.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib60">
    <span class="ltx_tag ltx_tag_bibitem">
     [60]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib60.1.1" style="font-size:90%;">
      Drew¬†A Hudson and Christopher¬†D Manning.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib60.2.1" style="font-size:90%;">
      Gqa: A new dataset for real-world visual reasoning and compositional question answering.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib60.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib60.4.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
     </span>
     <span class="ltx_text" id="bib.bib60.5.3" style="font-size:90%;">
      , pages 6700‚Äì6709, 2019.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib61">
    <span class="ltx_tag ltx_tag_bibitem">
     [61]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib61.1.1" style="font-size:90%;">
      Danna Gurari, Qing Li, Abigale¬†J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey¬†P Bigham.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib61.2.1" style="font-size:90%;">
      Vizwiz grand challenge: Answering visual questions from blind people.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib61.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib61.4.2" style="font-size:90%;">
      Proceedings of the IEEE conference on computer vision and pattern recognition
     </span>
     <span class="ltx_text" id="bib.bib61.5.3" style="font-size:90%;">
      , pages 3608‚Äì3617, 2018.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib62">
    <span class="ltx_tag ltx_tag_bibitem">
     [62]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib62.1.1" style="font-size:90%;">
      Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu¬†Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib62.2.1" style="font-size:90%;">
      Towards vqa models that can read.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib62.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib62.4.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
     </span>
     <span class="ltx_text" id="bib.bib62.5.3" style="font-size:90%;">
      , pages 8317‚Äì8326, 2019.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib63">
    <span class="ltx_tag ltx_tag_bibitem">
     [63]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib63.1.1" style="font-size:90%;">
      Anand Mishra, Shashank Shekhar, Ajeet¬†Kumar Singh, and Anirban Chakraborty.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib63.2.1" style="font-size:90%;">
      Ocr-vqa: Visual question answering by reading text in images.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib63.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib63.4.2" style="font-size:90%;">
      2019 international conference on document analysis and recognition (ICDAR)
     </span>
     <span class="ltx_text" id="bib.bib63.5.3" style="font-size:90%;">
      , pages 947‚Äì952. IEEE, 2019.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib64">
    <span class="ltx_tag ltx_tag_bibitem">
     [64]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib64.1.1" style="font-size:90%;">
      Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad¬†Shahbaz Khan.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib64.2.1" style="font-size:90%;">
      Video-chatgpt: Towards detailed video understanding via large vision and language models.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib64.3.1" style="font-size:90%;">
      arXiv preprint arXiv:2306.05424
     </span>
     <span class="ltx_text" id="bib.bib64.4.2" style="font-size:90%;">
      , 2023.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib65">
    <span class="ltx_tag ltx_tag_bibitem">
     [65]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib65.1.1" style="font-size:90%;">
      Fabian Caba¬†Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos¬†Niebles.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib65.2.1" style="font-size:90%;">
      Activitynet: A large-scale video benchmark for human activity understanding.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib65.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib65.4.2" style="font-size:90%;">
      Proceedings of the ieee conference on computer vision and pattern recognition
     </span>
     <span class="ltx_text" id="bib.bib65.5.3" style="font-size:90%;">
      , pages 961‚Äì970, 2015.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib66">
    <span class="ltx_tag ltx_tag_bibitem">
     [66]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib66.1.1" style="font-size:90%;">
      Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib66.2.1" style="font-size:90%;">
      Next-qa: Next phase of question-answering to explaining temporal actions.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib66.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib66.4.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
     </span>
     <span class="ltx_text" id="bib.bib66.5.3" style="font-size:90%;">
      , pages 9777‚Äì9786, 2021.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib67">
    <span class="ltx_tag ltx_tag_bibitem">
     [67]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib67.1.1" style="font-size:90%;">
      David Chen and William¬†B Dolan.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib67.2.1" style="font-size:90%;">
      Collecting highly parallel data for paraphrase evaluation.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib67.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib67.4.2" style="font-size:90%;">
      Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies
     </span>
     <span class="ltx_text" id="bib.bib67.5.3" style="font-size:90%;">
      , pages 190‚Äì200, 2011.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib68">
    <span class="ltx_tag ltx_tag_bibitem">
     [68]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib68.1.1" style="font-size:90%;">
      Jun Xu, Tao Mei, Ting Yao, and Yong Rui.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib68.2.1" style="font-size:90%;">
      Msr-vtt: A large video description dataset for bridging video and language.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib68.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib68.4.2" style="font-size:90%;">
      Proceedings of the IEEE conference on computer vision and pattern recognition
     </span>
     <span class="ltx_text" id="bib.bib68.5.3" style="font-size:90%;">
      , pages 5288‚Äì5296, 2016.
     </span>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib69">
    <span class="ltx_tag ltx_tag_bibitem">
     [69]
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib69.1.1" style="font-size:90%;">
      Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib69.2.1" style="font-size:90%;">
      Just ask: Learning to answer questions from millions of narrated videos.
     </span>
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text" id="bib.bib69.3.1" style="font-size:90%;">
      In
     </span>
     <span class="ltx_text ltx_font_italic" id="bib.bib69.4.2" style="font-size:90%;">
      Proceedings of the IEEE/CVF international conference on computer vision
     </span>
     <span class="ltx_text" id="bib.bib69.5.3" style="font-size:90%;">
      , pages 1686‚Äì1697, 2021.
     </span>
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   SEED Tokenizer
  </h2>
  <div class="ltx_para" id="A1.p1">
   <p class="ltx_p" id="A1.p1.3">
    The generation embedding of SEED is aligned with the image embedding of unCLIP
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib13" title="">
      13
     </a>
     ]
    </cite>
    SD, and can be decoded to realistic images with the unCLIP-SD-UNet. In our previous work
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    , we train a visual tokenizer
    <math alttext="\text{SEED}^{\text{text}}" class="ltx_Math" display="inline" id="A1.p1.1.m1.1">
     <semantics id="A1.p1.1.m1.1a">
      <msup id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml">
       <mtext id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2a.cmml">
        SEED
       </mtext>
       <mtext id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3a.cmml">
        text
       </mtext>
      </msup>
      <annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b">
       <apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1">
        <csymbol cd="ambiguous" id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1">
         superscript
        </csymbol>
        <ci id="A1.p1.1.m1.1.1.2a.cmml" xref="A1.p1.1.m1.1.1.2">
         <mtext id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2">
          SEED
         </mtext>
        </ci>
        <ci id="A1.p1.1.m1.1.1.3a.cmml" xref="A1.p1.1.m1.1.1.3">
         <mtext id="A1.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="A1.p1.1.m1.1.1.3">
          text
         </mtext>
        </ci>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">
       \text{SEED}^{\text{text}}
      </annotation>
     </semantics>
    </math>
    through aligning the generation embeddings with the text embeddings (77 tokens) of SD
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ]
    </cite>
    , and the generation embeddings can be decoded to images with the SD-UNet. The visual comparison of the the reconstructed images between
    <math alttext="\text{SEED}^{\text{text}}" class="ltx_Math" display="inline" id="A1.p1.2.m2.1">
     <semantics id="A1.p1.2.m2.1a">
      <msup id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml">
       <mtext id="A1.p1.2.m2.1.1.2" xref="A1.p1.2.m2.1.1.2a.cmml">
        SEED
       </mtext>
       <mtext id="A1.p1.2.m2.1.1.3" xref="A1.p1.2.m2.1.1.3a.cmml">
        text
       </mtext>
      </msup>
      <annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b">
       <apply id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1">
        <csymbol cd="ambiguous" id="A1.p1.2.m2.1.1.1.cmml" xref="A1.p1.2.m2.1.1">
         superscript
        </csymbol>
        <ci id="A1.p1.2.m2.1.1.2a.cmml" xref="A1.p1.2.m2.1.1.2">
         <mtext id="A1.p1.2.m2.1.1.2.cmml" xref="A1.p1.2.m2.1.1.2">
          SEED
         </mtext>
        </ci>
        <ci id="A1.p1.2.m2.1.1.3a.cmml" xref="A1.p1.2.m2.1.1.3">
         <mtext id="A1.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="A1.p1.2.m2.1.1.3">
          text
         </mtext>
        </ci>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">
       \text{SEED}^{\text{text}}
      </annotation>
     </semantics>
    </math>
    and SEED are shown in Fig.
    <a class="ltx_ref" href="#A1.F8" title="Figure 8 ‚Ä£ Appendix A SEED Tokenizer ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
     <span class="ltx_text ltx_ref_tag">
      8
     </span>
    </a>
    . We can observe that compared with
    <math alttext="\text{SEED}^{\text{text}}" class="ltx_Math" display="inline" id="A1.p1.3.m3.1">
     <semantics id="A1.p1.3.m3.1a">
      <msup id="A1.p1.3.m3.1.1" xref="A1.p1.3.m3.1.1.cmml">
       <mtext id="A1.p1.3.m3.1.1.2" xref="A1.p1.3.m3.1.1.2a.cmml">
        SEED
       </mtext>
       <mtext id="A1.p1.3.m3.1.1.3" xref="A1.p1.3.m3.1.1.3a.cmml">
        text
       </mtext>
      </msup>
      <annotation-xml encoding="MathML-Content" id="A1.p1.3.m3.1b">
       <apply id="A1.p1.3.m3.1.1.cmml" xref="A1.p1.3.m3.1.1">
        <csymbol cd="ambiguous" id="A1.p1.3.m3.1.1.1.cmml" xref="A1.p1.3.m3.1.1">
         superscript
        </csymbol>
        <ci id="A1.p1.3.m3.1.1.2a.cmml" xref="A1.p1.3.m3.1.1.2">
         <mtext id="A1.p1.3.m3.1.1.2.cmml" xref="A1.p1.3.m3.1.1.2">
          SEED
         </mtext>
        </ci>
        <ci id="A1.p1.3.m3.1.1.3a.cmml" xref="A1.p1.3.m3.1.1.3">
         <mtext id="A1.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="A1.p1.3.m3.1.1.3">
          text
         </mtext>
        </ci>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.p1.3.m3.1c">
       \text{SEED}^{\text{text}}
      </annotation>
     </semantics>
    </math>
    , the images reconstructed by SEED can better preserve the visual information of the original images.
   </p>
  </div>
  <figure class="ltx_figure" id="A1.F8">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="226" id="A1.F8.g1" src="/html/2310.01218/assets/x8.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 8:
    </span>
    (a) Input image. (b) Reconstruction images of
    <math alttext="\text{SEED}^{\text{text}}" class="ltx_Math" display="inline" id="A1.F8.2.m1.1">
     <semantics id="A1.F8.2.m1.1b">
      <msup id="A1.F8.2.m1.1.1" xref="A1.F8.2.m1.1.1.cmml">
       <mtext id="A1.F8.2.m1.1.1.2" xref="A1.F8.2.m1.1.1.2a.cmml">
        SEED
       </mtext>
       <mtext id="A1.F8.2.m1.1.1.3" xref="A1.F8.2.m1.1.1.3a.cmml">
        text
       </mtext>
      </msup>
      <annotation-xml encoding="MathML-Content" id="A1.F8.2.m1.1c">
       <apply id="A1.F8.2.m1.1.1.cmml" xref="A1.F8.2.m1.1.1">
        <csymbol cd="ambiguous" id="A1.F8.2.m1.1.1.1.cmml" xref="A1.F8.2.m1.1.1">
         superscript
        </csymbol>
        <ci id="A1.F8.2.m1.1.1.2a.cmml" xref="A1.F8.2.m1.1.1.2">
         <mtext id="A1.F8.2.m1.1.1.2.cmml" xref="A1.F8.2.m1.1.1.2">
          SEED
         </mtext>
        </ci>
        <ci id="A1.F8.2.m1.1.1.3a.cmml" xref="A1.F8.2.m1.1.1.3">
         <mtext id="A1.F8.2.m1.1.1.3.cmml" mathsize="70%" xref="A1.F8.2.m1.1.1.3">
          text
         </mtext>
        </ci>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.F8.2.m1.1d">
       \text{SEED}^{\text{text}}
      </annotation>
     </semantics>
    </math>
    tokenizer
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    , which is aligned with the feature space of a SD conditioned on text embeddings. (c) Reconstruction images of SEED tokenizer, which is aligned with the feature space of a SD conditioned on image embedding.
   </figcaption>
  </figure>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Pretraining
  </h2>
  <section class="ltx_subsection" id="A2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.1
    </span>
    Pretraining Data
   </h3>
   <div class="ltx_para" id="A2.SS1.p1">
    <p class="ltx_p" id="A2.SS1.p1.1">
     As shown in Tab.
     <a class="ltx_ref" href="#A2.T5" title="Table 5 ‚Ä£ B.1 Pretraining Data ‚Ä£ Appendix B Pretraining ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     , we utilize diverse categories of datasets as pretraining data, which can be summarized as follows.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS1.p2">
    <p class="ltx_p ltx_align_left" id="A2.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="A2.SS1.p2.1.1">
      Image-text Pairs.
     </span>
    </p>
   </div>
   <div class="ltx_para" id="A2.SS1.p3">
    <p class="ltx_p" id="A2.SS1.p3.1">
     We use the image-text pairs from CC3M
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ]
     </cite>
     , Unsplash
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib29" title="">
       29
      </a>
      ]
     </cite>
     , LAION-COCO
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib30" title="">
       30
      </a>
      ]
     </cite>
     and MS-COCO
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib31" title="">
       31
      </a>
      ]
     </cite>
     . We filtered the samples in these datasets based on image resolution, aspect ratio, and visual-textual similarity. We randomly place images or text at the forefront, in order to achieve the generation of captions based on images and vice versa.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS1.p4">
    <p class="ltx_p ltx_align_left" id="A2.SS1.p4.1">
     <span class="ltx_text ltx_font_bold" id="A2.SS1.p4.1.1">
      Video-text Pairs.
     </span>
    </p>
   </div>
   <div class="ltx_para" id="A2.SS1.p5">
    <p class="ltx_p" id="A2.SS1.p5.1">
     We use a large-scale dataset WebVid-10M
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib45" title="">
       45
      </a>
      ]
     </cite>
     containing videos and captions. We implemented heuristic rules to exclude extraneous metadata, such as the resolution of the original video and camera parameters. We sample four frames of each video for training.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS1.p6">
    <p class="ltx_p ltx_align_left" id="A2.SS1.p6.1">
     <span class="ltx_text ltx_font_bold" id="A2.SS1.p6.1.1">
      Interleaved Image and Text.
     </span>
    </p>
   </div>
   <div class="ltx_para" id="A2.SS1.p7">
    <p class="ltx_p" id="A2.SS1.p7.1">
     We use publicly available MMC4
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib46" title="">
       46
      </a>
      ]
     </cite>
     and OBELISC
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib42" title="">
       42
      </a>
      ]
     </cite>
     datasets, which were extracted and thoroughly filtered from Common Crawl. Specifically, we employ the MMC4-core split, consisting of 7.3 million samples, and the complete OBELISC dataset, containing 141 million samples. For documents in MMC4, we create a sequence of length 1024 and randomly shuffle the order of images and their corresponding texts (those with the highest CLIP score). As for OBELISC, we generate a sequence of length 1024 based on the order of data in the dataset.
    </p>
   </div>
   <figure class="ltx_table" id="A2.T5">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 5:
     </span>
     Description of pretraining datasets of SEED-LLaMA.
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="A2.T5.1">
     <tr class="ltx_tr" id="A2.T5.1.1">
      <td class="ltx_td ltx_align_left ltx_border_tt" id="A2.T5.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       Dataset Name
      </td>
      <td class="ltx_td ltx_align_left ltx_border_tt" id="A2.T5.1.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       Dataset Description
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T5.1.2">
      <td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       <table class="ltx_tabular ltx_align_middle" id="A2.T5.1.2.1.1">
        <tr class="ltx_tr" id="A2.T5.1.2.1.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.2.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          COCO Caption
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.2.1.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.2.1.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib31" title="">
            31
           </a>
           ]
          </cite>
         </td>
        </tr>
       </table>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.1.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       <table class="ltx_tabular ltx_align_middle" id="A2.T5.1.2.2.1">
        <tr class="ltx_tr" id="A2.T5.1.2.2.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.2.2.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          0.5M image-text pairs with¬†human-written captions. Specifically,
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.2.2.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.2.2.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          Karpathy train split is used.
         </td>
        </tr>
       </table>
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T5.1.3">
      <td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.1.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       <table class="ltx_tabular ltx_align_middle" id="A2.T5.1.3.1.1">
        <tr class="ltx_tr" id="A2.T5.1.3.1.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.3.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          CC3M
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.3.1.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.3.1.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib28" title="">
            28
           </a>
           ]
          </cite>
         </td>
        </tr>
       </table>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.1.3.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       3.3M image-text pairs¬†from the web.
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T5.1.4">
      <td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.1.4.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       <table class="ltx_tabular ltx_align_middle" id="A2.T5.1.4.1.1">
        <tr class="ltx_tr" id="A2.T5.1.4.1.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.4.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          Unsplash
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.4.1.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.4.1.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib29" title="">
            29
           </a>
           ]
          </cite>
         </td>
        </tr>
       </table>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.1.4.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       <table class="ltx_tabular ltx_align_middle" id="A2.T5.1.4.2.1">
        <tr class="ltx_tr" id="A2.T5.1.4.2.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.4.2.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          4.8M image-text pairs, in which images are composed of high-quality
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.4.2.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.4.2.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          Unsplash photos.
         </td>
        </tr>
       </table>
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T5.1.5">
      <td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.1.5.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       <table class="ltx_tabular ltx_align_middle" id="A2.T5.1.5.1.1">
        <tr class="ltx_tr" id="A2.T5.1.5.1.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.5.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          LAION-COCO
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.5.1.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.5.1.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib30" title="">
            30
           </a>
           ]
          </cite>
         </td>
        </tr>
       </table>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.1.5.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       <table class="ltx_tabular ltx_align_middle" id="A2.T5.1.5.2.1">
        <tr class="ltx_tr" id="A2.T5.1.5.2.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.5.2.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          600M image-text pairs, where the caption is generated by the BLIP
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.5.2.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.5.2.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib38" title="">
            38
           </a>
           ]
          </cite>
          .
         </td>
        </tr>
       </table>
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T5.1.6">
      <td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.1.6.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       <table class="ltx_tabular ltx_align_middle" id="A2.T5.1.6.1.1">
        <tr class="ltx_tr" id="A2.T5.1.6.1.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.6.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          MMC4
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.6.1.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.6.1.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib46" title="">
            46
           </a>
           ]
          </cite>
         </td>
        </tr>
       </table>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.1.6.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       <table class="ltx_tabular ltx_align_middle" id="A2.T5.1.6.2.1">
        <tr class="ltx_tr" id="A2.T5.1.6.2.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.6.2.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          101M image-interleaved documents collected from Common Crawl.
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.6.2.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.6.2.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          We use the mmc4-core split which is consist of 7.3M documents. We
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.6.2.1.3">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.6.2.1.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          randomly shuffle the order of images and their corresponding text
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.6.2.1.4">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.6.2.1.4.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          (those with the¬†highest CLIP score).
         </td>
        </tr>
       </table>
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T5.1.7">
      <td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.1.7.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       <table class="ltx_tabular ltx_align_middle" id="A2.T5.1.7.1.1">
        <tr class="ltx_tr" id="A2.T5.1.7.1.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.7.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          OBELISC
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.7.1.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.7.1.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib42" title="">
            42
           </a>
           ]
          </cite>
         </td>
        </tr>
       </table>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.1.7.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       141M image-interleaved documents collected from Common Crawl.
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T5.1.8">
      <td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T5.1.8.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       <table class="ltx_tabular ltx_align_middle" id="A2.T5.1.8.1.1">
        <tr class="ltx_tr" id="A2.T5.1.8.1.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.8.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          WebVid
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.8.1.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.8.1.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          <cite class="ltx_cite ltx_citemacro_cite">
           [
           <a class="ltx_ref" href="#bib.bib45" title="">
            45
           </a>
           ]
          </cite>
         </td>
        </tr>
       </table>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T5.1.8.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
       <table class="ltx_tabular ltx_align_middle" id="A2.T5.1.8.2.1">
        <tr class="ltx_tr" id="A2.T5.1.8.2.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.8.2.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          8M video-text pairs, we have implemented heuristic rules to exclude
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.8.2.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.8.2.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          extraneous metadata,such as¬†the resolution of the original video and
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T5.1.8.2.1.3">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A2.T5.1.8.2.1.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
          camera parameters.
         </td>
        </tr>
       </table>
      </td>
     </tr>
    </table>
   </figure>
  </section>
  <section class="ltx_subsection" id="A2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.2
    </span>
    Pretraining Hyperparameters
   </h3>
   <div class="ltx_para" id="A2.SS2.p1">
    <p class="ltx_p" id="A2.SS2.p1.1">
     We report the detailed pretraining hyperparameters of SEED-LLaMA in Tab.
     <a class="ltx_ref" href="#A2.T6" title="Table 6 ‚Ä£ B.2 Pretraining Hyperparameters ‚Ä£ Appendix B Pretraining ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     .
    </p>
   </div>
   <figure class="ltx_table" id="A2.T6">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 6:
     </span>
     Summary of pretraining hyperparameters of SEED-LLaMA.
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="A2.T6.4">
     <tr class="ltx_tr" id="A2.T6.4.5">
      <td class="ltx_td ltx_align_left ltx_border_tt" id="A2.T6.4.5.1" style="padding:1pt 9.0pt;">
       Configuration
      </td>
      <td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T6.4.5.2" style="padding:1pt 9.0pt;">
       SEED 8B
      </td>
      <td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T6.4.5.3" style="padding:1pt 9.0pt;">
       SEED 14B
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.4.6">
      <td class="ltx_td ltx_align_left ltx_border_t" id="A2.T6.4.6.1" style="padding:1pt 9.0pt;">
       Vision encoder
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A2.T6.4.6.2" style="padding:1pt 9.0pt;">
       EVA-CLIP
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.4.7">
      <td class="ltx_td ltx_align_left" id="A2.T6.4.7.1" style="padding:1pt 9.0pt;">
       LLM
      </td>
      <td class="ltx_td ltx_align_center" id="A2.T6.4.7.2" style="padding:1pt 9.0pt;">
       Vicuna-7B
      </td>
      <td class="ltx_td ltx_align_center" id="A2.T6.4.7.3" style="padding:1pt 9.0pt;">
       LLaMA2-Chat-13B
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.4.8">
      <td class="ltx_td ltx_align_left" id="A2.T6.4.8.1" style="padding:1pt 9.0pt;">
       Training Strategy
      </td>
      <td class="ltx_td ltx_align_center" colspan="2" id="A2.T6.4.8.2" style="padding:1pt 9.0pt;">
       LoRA + Fully fine-tuning
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.4.9">
      <td class="ltx_td ltx_align_left" id="A2.T6.4.9.1" style="padding:1pt 9.0pt;">
       Peak learning rate
      </td>
      <td class="ltx_td ltx_align_center" colspan="2" id="A2.T6.4.9.2" style="padding:1pt 9.0pt;">
       1.5e-4
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.4.10">
      <td class="ltx_td ltx_align_left" id="A2.T6.4.10.1" style="padding:1pt 9.0pt;">
       Warmup ratio
      </td>
      <td class="ltx_td ltx_align_center" colspan="2" id="A2.T6.4.10.2" style="padding:1pt 9.0pt;">
       0.03
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.4.11">
      <td class="ltx_td ltx_align_left" id="A2.T6.4.11.1" style="padding:1pt 9.0pt;">
       LR schedule
      </td>
      <td class="ltx_td ltx_align_center" colspan="2" id="A2.T6.4.11.2" style="padding:1pt 9.0pt;">
       Cosine decay
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.4.12">
      <td class="ltx_td ltx_align_left" id="A2.T6.4.12.1" style="padding:1pt 9.0pt;">
       Optimizer
      </td>
      <td class="ltx_td ltx_align_center" colspan="2" id="A2.T6.4.12.2" style="padding:1pt 9.0pt;">
       AdamW
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.3.3">
      <td class="ltx_td ltx_align_left" id="A2.T6.3.3.4" style="padding:1pt 9.0pt;">
       Optimizer hyper-parameters
      </td>
      <td class="ltx_td ltx_align_center" colspan="2" id="A2.T6.3.3.3" style="padding:1pt 9.0pt;">
       <math alttext="\beta_{1}" class="ltx_Math" display="inline" id="A2.T6.1.1.1.m1.1">
        <semantics id="A2.T6.1.1.1.m1.1a">
         <msub id="A2.T6.1.1.1.m1.1.1" xref="A2.T6.1.1.1.m1.1.1.cmml">
          <mi id="A2.T6.1.1.1.m1.1.1.2" xref="A2.T6.1.1.1.m1.1.1.2.cmml">
           Œ≤
          </mi>
          <mn id="A2.T6.1.1.1.m1.1.1.3" xref="A2.T6.1.1.1.m1.1.1.3.cmml">
           1
          </mn>
         </msub>
         <annotation-xml encoding="MathML-Content" id="A2.T6.1.1.1.m1.1b">
          <apply id="A2.T6.1.1.1.m1.1.1.cmml" xref="A2.T6.1.1.1.m1.1.1">
           <csymbol cd="ambiguous" id="A2.T6.1.1.1.m1.1.1.1.cmml" xref="A2.T6.1.1.1.m1.1.1">
            subscript
           </csymbol>
           <ci id="A2.T6.1.1.1.m1.1.1.2.cmml" xref="A2.T6.1.1.1.m1.1.1.2">
            ùõΩ
           </ci>
           <cn id="A2.T6.1.1.1.m1.1.1.3.cmml" type="integer" xref="A2.T6.1.1.1.m1.1.1.3">
            1
           </cn>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="A2.T6.1.1.1.m1.1c">
          \beta_{1}
         </annotation>
        </semantics>
       </math>
       ,
       <math alttext="\beta_{2}" class="ltx_Math" display="inline" id="A2.T6.2.2.2.m2.1">
        <semantics id="A2.T6.2.2.2.m2.1a">
         <msub id="A2.T6.2.2.2.m2.1.1" xref="A2.T6.2.2.2.m2.1.1.cmml">
          <mi id="A2.T6.2.2.2.m2.1.1.2" xref="A2.T6.2.2.2.m2.1.1.2.cmml">
           Œ≤
          </mi>
          <mn id="A2.T6.2.2.2.m2.1.1.3" xref="A2.T6.2.2.2.m2.1.1.3.cmml">
           2
          </mn>
         </msub>
         <annotation-xml encoding="MathML-Content" id="A2.T6.2.2.2.m2.1b">
          <apply id="A2.T6.2.2.2.m2.1.1.cmml" xref="A2.T6.2.2.2.m2.1.1">
           <csymbol cd="ambiguous" id="A2.T6.2.2.2.m2.1.1.1.cmml" xref="A2.T6.2.2.2.m2.1.1">
            subscript
           </csymbol>
           <ci id="A2.T6.2.2.2.m2.1.1.2.cmml" xref="A2.T6.2.2.2.m2.1.1.2">
            ùõΩ
           </ci>
           <cn id="A2.T6.2.2.2.m2.1.1.3.cmml" type="integer" xref="A2.T6.2.2.2.m2.1.1.3">
            2
           </cn>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="A2.T6.2.2.2.m2.1c">
          \beta_{2}
         </annotation>
        </semantics>
       </math>
       ,
       <math alttext="\epsilon" class="ltx_Math" display="inline" id="A2.T6.3.3.3.m3.1">
        <semantics id="A2.T6.3.3.3.m3.1a">
         <mi id="A2.T6.3.3.3.m3.1.1" xref="A2.T6.3.3.3.m3.1.1.cmml">
          œµ
         </mi>
         <annotation-xml encoding="MathML-Content" id="A2.T6.3.3.3.m3.1b">
          <ci id="A2.T6.3.3.3.m3.1.1.cmml" xref="A2.T6.3.3.3.m3.1.1">
           italic-œµ
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="A2.T6.3.3.3.m3.1c">
          \epsilon
         </annotation>
        </semantics>
       </math>
       = 0.9, 0.98, le-6
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.4.4">
      <td class="ltx_td ltx_align_left" id="A2.T6.4.4.2" style="padding:1pt 9.0pt;">
       Image resolution
      </td>
      <td class="ltx_td ltx_align_center" colspan="2" id="A2.T6.4.4.1" style="padding:1pt 9.0pt;">
       224
       <math alttext="\times" class="ltx_Math" display="inline" id="A2.T6.4.4.1.m1.1">
        <semantics id="A2.T6.4.4.1.m1.1a">
         <mo id="A2.T6.4.4.1.m1.1.1" xref="A2.T6.4.4.1.m1.1.1.cmml">
          √ó
         </mo>
         <annotation-xml encoding="MathML-Content" id="A2.T6.4.4.1.m1.1b">
          <times id="A2.T6.4.4.1.m1.1.1.cmml" xref="A2.T6.4.4.1.m1.1.1">
          </times>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="A2.T6.4.4.1.m1.1c">
          \times
         </annotation>
        </semantics>
       </math>
       224
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.4.13">
      <td class="ltx_td ltx_align_left" id="A2.T6.4.13.1" style="padding:1pt 9.0pt;">
       Weight decay
      </td>
      <td class="ltx_td ltx_align_center" colspan="2" id="A2.T6.4.13.2" style="padding:1pt 9.0pt;">
       0.05
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.4.14">
      <td class="ltx_td ltx_align_left" id="A2.T6.4.14.1" style="padding:1pt 9.0pt;">
       Iterations
      </td>
      <td class="ltx_td ltx_align_center" colspan="2" id="A2.T6.4.14.2" style="padding:1pt 9.0pt;">
       30k + 10k
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.4.15">
      <td class="ltx_td ltx_align_left" id="A2.T6.4.15.1" style="padding:1pt 9.0pt;">
       Data
      </td>
      <td class="ltx_td ltx_align_center" colspan="2" id="A2.T6.4.15.2" style="padding:1pt 9.0pt;">
       <table class="ltx_tabular ltx_align_middle" id="A2.T6.4.15.2.1">
        <tr class="ltx_tr" id="A2.T6.4.15.2.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_center" id="A2.T6.4.15.2.1.1.1" style="padding:1pt 9.0pt;">
          (MS-COCO, CC3M, Unsplash), LAION-COCO,
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T6.4.15.2.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_center" id="A2.T6.4.15.2.1.2.1" style="padding:1pt 9.0pt;">
          OBELISC, MMC4, WebVid
         </td>
        </tr>
       </table>
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.4.16">
      <td class="ltx_td ltx_align_left" id="A2.T6.4.16.1" style="padding:1pt 9.0pt;">
       Sequence length per dataset
      </td>
      <td class="ltx_td ltx_align_center" colspan="2" id="A2.T6.4.16.2" style="padding:1pt 9.0pt;">
       160, 128, 1024, 1024, 200
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.4.17">
      <td class="ltx_td ltx_align_left" id="A2.T6.4.17.1" style="padding:1pt 9.0pt;">
       Batch size per dataset
      </td>
      <td class="ltx_td ltx_align_center" id="A2.T6.4.17.2" style="padding:1pt 9.0pt;">
       146, 180, 26, 26, 116
      </td>
      <td class="ltx_td ltx_align_center" id="A2.T6.4.17.3" style="padding:1pt 9.0pt;">
       46, 56, 8, 8, 36
      </td>
     </tr>
     <tr class="ltx_tr" id="A2.T6.4.18">
      <td class="ltx_td ltx_align_left ltx_border_bb" id="A2.T6.4.18.1" style="padding:1pt 9.0pt;">
       Sample ratio per dataset
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" colspan="2" id="A2.T6.4.18.2" style="padding:1pt 9.0pt;">
       4.5%, 54.5%, 9.1%, 27.3%, 4.5%
      </td>
     </tr>
    </table>
   </figure>
  </section>
 </section>
 <section class="ltx_appendix" id="A3">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix C
   </span>
   Instruction Tuning
  </h2>
  <div class="ltx_para" id="A3.p1">
   <p class="ltx_p" id="A3.p1.1">
    We summarize the datasets and their prompts for supervised instruction tuning of SEED-LLaMA in Tab.
    <a class="ltx_ref" href="#A3.T7" title="Table 7 ‚Ä£ Appendix C Instruction Tuning ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
     <span class="ltx_text ltx_ref_tag">
      7
     </span>
    </a>
    and Tab.
    <a class="ltx_ref" href="#A3.T8" title="Table 8 ‚Ä£ Appendix C Instruction Tuning ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
     <span class="ltx_text ltx_ref_tag">
      8
     </span>
    </a>
    . Note that MagicBrush
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib47" title="">
      47
     </a>
     ]
    </cite>
    contains both the single-turn and multi-turn scenarios, and we only use the single-turn for multimodal prompt image generation.
   </p>
  </div>
  <figure class="ltx_table" id="A3.T7">
   <figcaption class="ltx_caption ltx_centering" style="font-size:90%;">
    <span class="ltx_tag ltx_tag_table">
     Table 7:
    </span>
    Description of datasets in the instruction tuning of SEED-LLaMA.
   </figcaption>
   <table class="ltx_tabular ltx_centering ltx_align_middle" id="A3.T7.3">
    <tr class="ltx_tr" id="A3.T7.3.1">
     <td class="ltx_td ltx_align_center" id="A3.T7.3.1.1">
      <span class="ltx_text" id="A3.T7.3.1.1.1" style="font-size:90%;">
       Task
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.1.2">
      <span class="ltx_text" id="A3.T7.3.1.2.1" style="font-size:90%;">
       Dataset Name
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.1.3">
      <span class="ltx_text" id="A3.T7.3.1.3.1" style="font-size:90%;">
       Dataset Description
      </span>
     </td>
     <td class="ltx_td ltx_align_right" id="A3.T7.3.1.4">
      <span class="ltx_text" id="A3.T7.3.1.4.1" style="font-size:90%;">
       Type
      </span>
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.2">
     <td class="ltx_td ltx_nopad_r ltx_align_right" id="A3.T7.3.2.1">
      <span class="ltx_text" id="A3.T7.3.2.1.1" style="font-size:90%;">
       Text-to-Image
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.2.2">
     </td>
     <td class="ltx_td" id="A3.T7.3.2.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.2.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.3">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.3.1">
      <span class="ltx_text" id="A3.T7.3.3.1.1" style="font-size:90%;">
       Generation
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.3.2">
      <span class="ltx_text" id="A3.T7.3.3.2.1" style="font-size:90%;">
       JourneyDB
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.3.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.3.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.4">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.4.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.4.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib48" title="">
        48
       </a>
       <span class="ltx_text" id="A3.T7.3.4.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.4.2">
      <span class="ltx_text" id="A3.T7.3.4.2.1" style="font-size:90%;">
       It contains 4429K Midjourney images, with text
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.4.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.4.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.5">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.5.1">
      <span class="ltx_text" id="A3.T7.3.5.1.1" style="font-size:90%;">
       prompt, image caption, and QA pairs.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.5.2">
      <span class="ltx_text" id="A3.T7.3.5.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.5.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.5.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.6">
     <td class="ltx_td" id="A3.T7.3.6.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.6.2">
      <span class="ltx_text" id="A3.T7.3.6.2.1" style="font-size:90%;">
       DiffusionDB
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.6.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.6.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.7">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.7.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.7.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib49" title="">
        49
       </a>
       <span class="ltx_text" id="A3.T7.3.7.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.7.2">
      <span class="ltx_text" id="A3.T7.3.7.2.1" style="font-size:90%;">
       It contains 14 million images generated by Stable
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.7.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.7.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.8">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.8.1">
      <span class="ltx_text" id="A3.T7.3.8.1.1" style="font-size:90%;">
       Diffusion using prompts by real users.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.8.2">
      <span class="ltx_text" id="A3.T7.3.8.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.8.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.8.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.9">
     <td class="ltx_td" id="A3.T7.3.9.1">
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.9.2">
      <span class="ltx_text" id="A3.T7.3.9.2.1" style="font-size:90%;">
       LAION-Aesthetics
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.9.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.9.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.10">
     <td class="ltx_td" id="A3.T7.3.10.1">
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.10.2">
      <span class="ltx_text" id="A3.T7.3.10.2.1" style="font-size:90%;">
       It contains several collections of subsets from
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.10.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.10.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.11">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.11.1">
      <span class="ltx_text" id="A3.T7.3.11.1.1" style="font-size:90%;">
       LAION 5B with high visual quality.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.11.2">
      <span class="ltx_text" id="A3.T7.3.11.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.11.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.11.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.12">
     <td class="ltx_td" id="A3.T7.3.12.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.12.2">
      <span class="ltx_text" id="A3.T7.3.12.2.1" style="font-size:90%;">
       VIST
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.12.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.12.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.13">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.13.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.13.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib50" title="">
        50
       </a>
       <span class="ltx_text" id="A3.T7.3.13.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.13.2">
      <span class="ltx_text" id="A3.T7.3.13.2.1" style="font-size:90%;">
       It contains photos in 20K sequences, aligned to
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.13.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.13.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.14">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.14.1">
      <span class="ltx_text" id="A3.T7.3.14.1.1" style="font-size:90%;">
       both caption and story language.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.14.2">
      <span class="ltx_text" id="A3.T7.3.14.2.1" style="font-size:90%;">
       Multi-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.14.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.14.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.15">
     <td class="ltx_td ltx_nopad_r ltx_align_right" id="A3.T7.3.15.1">
      <span class="ltx_text" id="A3.T7.3.15.1.1" style="font-size:90%;">
       Multimodal
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.15.2">
     </td>
     <td class="ltx_td" id="A3.T7.3.15.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.15.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.16">
     <td class="ltx_td ltx_align_center" id="A3.T7.3.16.1">
      <span class="ltx_text" id="A3.T7.3.16.1.1" style="font-size:90%;">
       Prompt Image
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.16.2">
     </td>
     <td class="ltx_td" id="A3.T7.3.16.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.16.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.17">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.17.1">
      <span class="ltx_text" id="A3.T7.3.17.1.1" style="font-size:90%;">
       Generation
      </span>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.17.2">
      <span class="ltx_text" id="A3.T7.3.17.2.1" style="font-size:90%;">
       Instructpix2pix
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.17.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.17.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.18">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.18.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.18.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib51" title="">
        51
       </a>
       <span class="ltx_text" id="A3.T7.3.18.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.18.2">
      <span class="ltx_text" id="A3.T7.3.18.2.1" style="font-size:90%;">
       It contains text editing instructions and the
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.18.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.18.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.19">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.19.1">
      <span class="ltx_text" id="A3.T7.3.19.1.1" style="font-size:90%;">
       corresponding images, with 454K samples.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.19.2">
      <span class="ltx_text" id="A3.T7.3.19.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.19.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.19.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.20">
     <td class="ltx_td" id="A3.T7.3.20.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.20.2">
      <span class="ltx_text" id="A3.T7.3.20.2.1" style="font-size:90%;">
       MagicBrush
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.20.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.20.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.21">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.21.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.21.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib47" title="">
        47
       </a>
       <span class="ltx_text" id="A3.T7.3.21.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.21.2">
      <span class="ltx_text" id="A3.T7.3.21.2.1" style="font-size:90%;">
       It contains 10K manually annotated triplets
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.21.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.21.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.22">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.22.1">
      <span class="ltx_text" id="A3.T7.3.22.1.1" style="font-size:90%;">
       (source image, instruction, target image).
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.22.2">
      <span class="ltx_text" id="A3.T7.3.22.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.22.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.22.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.23">
     <td class="ltx_td ltx_nopad_r ltx_align_right" id="A3.T7.3.23.1">
      <span class="ltx_text" id="A3.T7.3.23.1.1" style="font-size:90%;">
       Image
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.23.2">
     </td>
     <td class="ltx_td" id="A3.T7.3.23.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.23.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.24">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.24.1">
      <span class="ltx_text" id="A3.T7.3.24.1.1" style="font-size:90%;">
       Conversation
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.24.2">
      <span class="ltx_text" id="A3.T7.3.24.2.1" style="font-size:90%;">
       LLaVA
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.24.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.24.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.25">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.25.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.25.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib18" title="">
        18
       </a>
       <span class="ltx_text" id="A3.T7.3.25.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.25.2">
      <span class="ltx_text" id="A3.T7.3.25.2.1" style="font-size:90%;">
       We use 58K multi-turn conversations¬†between an
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.25.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.25.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.26">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.26.1">
      <span class="ltx_text" id="A3.T7.3.26.1.1" style="font-size:90%;">
       assistant and a person.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.26.2">
      <span class="ltx_text" id="A3.T7.3.26.2.1" style="font-size:90%;">
       Multi-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.26.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.26.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.27">
     <td class="ltx_td" id="A3.T7.3.27.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.27.2">
      <span class="ltx_text" id="A3.T7.3.27.2.1" style="font-size:90%;">
       SVIT
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.27.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.27.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.28">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.28.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.28.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib52" title="">
        52
       </a>
       <span class="ltx_text" id="A3.T7.3.28.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.28.2">
      <span class="ltx_text" id="A3.T7.3.28.2.1" style="font-size:90%;">
       It contains conversations, complex reasoning,
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.28.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.28.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.29">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.29.1">
      <span class="ltx_text" id="A3.T7.3.29.1.1" style="font-size:90%;">
       referring QA and detailed image description.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.29.2">
      <span class="ltx_text" id="A3.T7.3.29.2.1" style="font-size:90%;">
       Multi-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.29.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.29.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.30">
     <td class="ltx_td" id="A3.T7.3.30.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.30.2">
      <span class="ltx_text" id="A3.T7.3.30.2.1" style="font-size:90%;">
       LLaVAR
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.30.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.30.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.31">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.31.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.31.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib53" title="">
        53
       </a>
       <span class="ltx_text" id="A3.T7.3.31.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.31.2">
      <span class="ltx_text" id="A3.T7.3.31.2.1" style="font-size:90%;">
       It contains 16K multi-turn conversations, each
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.31.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.31.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.32">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.32.1">
      <span class="ltx_text" id="A3.T7.3.32.1.1" style="font-size:90%;">
       with QA pairs for text-rich images.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.32.2">
      <span class="ltx_text" id="A3.T7.3.32.2.1" style="font-size:90%;">
       Multi-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.32.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.32.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.33">
     <td class="ltx_td ltx_nopad_r ltx_align_right" id="A3.T7.3.33.1">
      <span class="ltx_text" id="A3.T7.3.33.1.1" style="font-size:90%;">
       Multi-Image
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.33.2">
     </td>
     <td class="ltx_td" id="A3.T7.3.33.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.33.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.34">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.34.1">
      <span class="ltx_text" id="A3.T7.3.34.1.1" style="font-size:90%;">
       Understanding
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.34.2">
      <span class="ltx_text" id="A3.T7.3.34.2.1" style="font-size:90%;">
       GSD
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.34.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.34.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.35">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.35.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.35.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib54" title="">
        54
       </a>
       <span class="ltx_text" id="A3.T7.3.35.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.35.2">
      <span class="ltx_text" id="A3.T7.3.35.2.1" style="font-size:90%;">
       It contains 141K pairs of images with text
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.35.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.35.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.36">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.36.1">
      <span class="ltx_text" id="A3.T7.3.36.1.1" style="font-size:90%;">
       describing the differences.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.36.2">
      <span class="ltx_text" id="A3.T7.3.36.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.36.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.36.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.37">
     <td class="ltx_td ltx_nopad_r ltx_align_right" id="A3.T7.3.37.1">
      <span class="ltx_text" id="A3.T7.3.37.1.1" style="font-size:90%;">
       Image
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.37.2">
     </td>
     <td class="ltx_td" id="A3.T7.3.37.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.37.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.38">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.38.1">
      <span class="ltx_text" id="A3.T7.3.38.1.1" style="font-size:90%;">
       Captioning
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.38.2">
      <span class="ltx_text" id="A3.T7.3.38.2.1" style="font-size:90%;">
       VSR
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.38.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.38.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.39">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.39.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.39.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib55" title="">
        55
       </a>
       <span class="ltx_text" id="A3.T7.3.39.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.39.2">
      <span class="ltx_text" id="A3.T7.3.39.2.1" style="font-size:90%;">
       It contains texts describing the spatial
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.39.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.39.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.40">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.40.1">
      <span class="ltx_text" id="A3.T7.3.40.1.1" style="font-size:90%;">
       relations in the image, with 7K training samples.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.40.2">
      <span class="ltx_text" id="A3.T7.3.40.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.40.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.40.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.41">
     <td class="ltx_td" id="A3.T7.3.41.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.41.2">
      <span class="ltx_text" id="A3.T7.3.41.2.1" style="font-size:90%;">
       COCO Caption
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.41.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.41.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.42">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.42.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.42.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib31" title="">
        31
       </a>
       <span class="ltx_text" id="A3.T7.3.42.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.42.2">
      <span class="ltx_text" id="A3.T7.3.42.2.1" style="font-size:90%;">
       It contains image-text pairs with human-written
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.42.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.42.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.43">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.43.1">
      <span class="ltx_text" id="A3.T7.3.43.1.1" style="font-size:90%;">
       captions, with 82K training samples.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.43.2">
      <span class="ltx_text" id="A3.T7.3.43.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.43.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.43.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.44">
     <td class="ltx_td" id="A3.T7.3.44.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.44.2">
      <span class="ltx_text" id="A3.T7.3.44.2.1" style="font-size:90%;">
       TextCaps
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.44.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.44.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.45">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.45.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.45.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib56" title="">
        56
       </a>
       <span class="ltx_text" id="A3.T7.3.45.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.45.2">
      <span class="ltx_text" id="A3.T7.3.45.2.1" style="font-size:90%;">
       It requires the model to comprehend and reason
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.45.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.45.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.46">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.46.1">
      <span class="ltx_text" id="A3.T7.3.46.1.1" style="font-size:90%;">
       the text in images, with 21K training samples.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.46.2">
      <span class="ltx_text" id="A3.T7.3.46.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.46.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.46.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.47">
     <td class="ltx_td ltx_align_center" id="A3.T7.3.47.1">
      <span class="ltx_text" id="A3.T7.3.47.1.1" style="font-size:90%;">
       Image QA
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.47.2">
      <span class="ltx_text" id="A3.T7.3.47.2.1" style="font-size:90%;">
       VQAv2
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.47.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.47.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.48">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.48.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.48.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib57" title="">
        57
       </a>
       <span class="ltx_text" id="A3.T7.3.48.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.48.2">
      <span class="ltx_text" id="A3.T7.3.48.2.1" style="font-size:90%;">
       A dataset for open-ended image question
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.48.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.48.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.49">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.49.1">
      <span class="ltx_text" id="A3.T7.3.49.1.1" style="font-size:90%;">
       answering, with 82K training samples.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.49.2">
      <span class="ltx_text" id="A3.T7.3.49.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.49.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.49.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.50">
     <td class="ltx_td" id="A3.T7.3.50.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.50.2">
      <span class="ltx_text" id="A3.T7.3.50.2.1" style="font-size:90%;">
       OKVQA
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.50.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.50.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.51">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.51.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.51.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib58" title="">
        58
       </a>
       <span class="ltx_text" id="A3.T7.3.51.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.51.2">
      <span class="ltx_text" id="A3.T7.3.51.2.1" style="font-size:90%;">
       It contains questions that require outside
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.51.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.51.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.52">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.52.1">
      <span class="ltx_text" id="A3.T7.3.52.1.1" style="font-size:90%;">
       knowledge to answer, with 9K training samples.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.52.2">
      <span class="ltx_text" id="A3.T7.3.52.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.52.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.52.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.53">
     <td class="ltx_td" id="A3.T7.3.53.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.53.2">
      <span class="ltx_text" id="A3.T7.3.53.2.1" style="font-size:90%;">
       A-OKVQA
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.53.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.53.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.54">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.54.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.54.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib59" title="">
        59
       </a>
       <span class="ltx_text" id="A3.T7.3.54.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.54.2">
      <span class="ltx_text" id="A3.T7.3.54.2.1" style="font-size:90%;">
       It is a successor of OKVQA containing more
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.54.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.54.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.55">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.55.1">
      <span class="ltx_text" id="A3.T7.3.55.1.1" style="font-size:90%;">
       challenging questions, with 17K training samples.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.55.2">
      <span class="ltx_text" id="A3.T7.3.55.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.55.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.55.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.56">
     <td class="ltx_td" id="A3.T7.3.56.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.56.2">
      <span class="ltx_text" id="A3.T7.3.56.2.1" style="font-size:90%;">
       GQA
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.56.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.56.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.57">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.57.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.57.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib60" title="">
        60
       </a>
       <span class="ltx_text" id="A3.T7.3.57.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.57.2">
      <span class="ltx_text" id="A3.T7.3.57.2.1" style="font-size:90%;">
       It contains questions for image understanding
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.57.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.57.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.58">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.58.1">
      <span class="ltx_text" id="A3.T7.3.58.1.1" style="font-size:90%;">
       and reasoning, with 30K training samples.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.58.2">
      <span class="ltx_text" id="A3.T7.3.58.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.58.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.58.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.59">
     <td class="ltx_td" id="A3.T7.3.59.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.59.2">
      <span class="ltx_text" id="A3.T7.3.59.2.1" style="font-size:90%;">
       VizWiz
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.59.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.59.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.60">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.60.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.60.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib61" title="">
        61
       </a>
       <span class="ltx_text" id="A3.T7.3.60.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.60.2">
      <span class="ltx_text" id="A3.T7.3.60.2.1" style="font-size:90%;">
       It contains visual questions asked by people who
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.60.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.60.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.61">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.61.1">
      <span class="ltx_text" id="A3.T7.3.61.1.1" style="font-size:90%;">
       are blind, with 20K¬†training samples.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.61.2">
      <span class="ltx_text" id="A3.T7.3.61.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.61.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.61.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.62">
     <td class="ltx_td" id="A3.T7.3.62.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.62.2">
      <span class="ltx_text" id="A3.T7.3.62.2.1" style="font-size:90%;">
       TextVQA
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.62.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.62.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.63">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.63.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.63.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib62" title="">
        62
       </a>
       <span class="ltx_text" id="A3.T7.3.63.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.63.2">
      <span class="ltx_text" id="A3.T7.3.63.2.1" style="font-size:90%;">
       It contains questions that require models to read
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.63.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.63.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.64">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.64.1">
      <span class="ltx_text" id="A3.T7.3.64.1.1" style="font-size:90%;">
       text in the image, with 800K training samples.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.64.2">
      <span class="ltx_text" id="A3.T7.3.64.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.64.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.64.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.65">
     <td class="ltx_td" id="A3.T7.3.65.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.65.2">
      <span class="ltx_text" id="A3.T7.3.65.2.1" style="font-size:90%;">
       OCR-VQA
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.65.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.65.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.66">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.66.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.66.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib63" title="">
        63
       </a>
       <span class="ltx_text" id="A3.T7.3.66.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.66.2">
      <span class="ltx_text" id="A3.T7.3.66.2.1" style="font-size:90%;">
       It contains questions that requires reasoning about
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.66.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.66.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.67">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.67.1">
      <span class="ltx_text" id="A3.T7.3.67.1.1" style="font-size:90%;">
       text to answer, with 173K training samples.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.67.2">
      <span class="ltx_text" id="A3.T7.3.67.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.67.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.67.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.68">
     <td class="ltx_td ltx_nopad_r ltx_align_right" id="A3.T7.3.68.1">
      <span class="ltx_text" id="A3.T7.3.68.1.1" style="font-size:90%;">
       Video
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.68.2">
     </td>
     <td class="ltx_td" id="A3.T7.3.68.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.68.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.69">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.69.1">
      <span class="ltx_text" id="A3.T7.3.69.1.1" style="font-size:90%;">
       Conversation
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.69.2">
      <span class="ltx_text" id="A3.T7.3.69.2.1" style="font-size:90%;">
       Video-ChatGPT
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.69.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.69.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.70">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.70.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.70.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib64" title="">
        64
       </a>
       <span class="ltx_text" id="A3.T7.3.70.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.70.2">
      <span class="ltx_text" id="A3.T7.3.70.2.1" style="font-size:90%;">
       It contains of 100K video-instruction pairs
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.70.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.70.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.71">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.71.1">
      <span class="ltx_text" id="A3.T7.3.71.1.1" style="font-size:90%;">
       via manual and semi-automated pipeline.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.71.2">
      <span class="ltx_text" id="A3.T7.3.71.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.71.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.71.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.72">
     <td class="ltx_td ltx_align_center" id="A3.T7.3.72.1">
      <span class="ltx_text" id="A3.T7.3.72.1.1" style="font-size:90%;">
       Video QA
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.72.2">
      <span class="ltx_text" id="A3.T7.3.72.2.1" style="font-size:90%;">
       ActivityNet
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.72.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.72.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.73">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.73.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.73.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib65" title="">
        65
       </a>
       <span class="ltx_text" id="A3.T7.3.73.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.73.2">
      <span class="ltx_text" id="A3.T7.3.73.2.1" style="font-size:90%;">
       It contains 200 different types of activities from
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.73.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.73.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.74">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.74.1">
      <span class="ltx_text" id="A3.T7.3.74.1.1" style="font-size:90%;">
       YouTube, with 10K training videos.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.74.2">
      <span class="ltx_text" id="A3.T7.3.74.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.74.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.74.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.75">
     <td class="ltx_td" id="A3.T7.3.75.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.75.2">
      <span class="ltx_text" id="A3.T7.3.75.2.1" style="font-size:90%;">
       Next-QA
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.75.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.75.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.76">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.76.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.76.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib66" title="">
        66
       </a>
       <span class="ltx_text" id="A3.T7.3.76.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.76.2">
      <span class="ltx_text" id="A3.T7.3.76.2.1" style="font-size:90%;">
       It contains¬†52K QA pairs of videos grouped into
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.76.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.76.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.77">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.77.1">
      <span class="ltx_text" id="A3.T7.3.77.1.1" style="font-size:90%;">
       causal, temporal and descriptive questions.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.77.2">
      <span class="ltx_text" id="A3.T7.3.77.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.77.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.77.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.78">
     <td class="ltx_td" id="A3.T7.3.78.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.78.2">
      <span class="ltx_text" id="A3.T7.3.78.2.1" style="font-size:90%;">
       MSVD
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.78.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.78.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.79">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.79.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.79.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib67" title="">
        67
       </a>
       <span class="ltx_text" id="A3.T7.3.79.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.79.2">
      <span class="ltx_text" id="A3.T7.3.79.2.1" style="font-size:90%;">
       It contains videos¬†from YouTube with¬†descriptions,
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.79.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.79.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.80">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.80.1">
      <span class="ltx_text" id="A3.T7.3.80.1.1" style="font-size:90%;">
       containing 1.2K training samples.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.80.2">
      <span class="ltx_text" id="A3.T7.3.80.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.80.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.80.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.81">
     <td class="ltx_td" id="A3.T7.3.81.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.81.2">
      <span class="ltx_text" id="A3.T7.3.81.2.1" style="font-size:90%;">
       MSR-VTT
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.81.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.81.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.82">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.82.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.82.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib68" title="">
        68
       </a>
       <span class="ltx_text" id="A3.T7.3.82.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.82.2">
      <span class="ltx_text" id="A3.T7.3.82.2.1" style="font-size:90%;">
       It contains videos¬†from YouTube with¬†descriptions,
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.82.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.82.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.83">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.83.1">
      <span class="ltx_text" id="A3.T7.3.83.1.1" style="font-size:90%;">
       containing 19K training samples.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.83.2">
      <span class="ltx_text" id="A3.T7.3.83.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.83.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.83.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.84">
     <td class="ltx_td" id="A3.T7.3.84.1">
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.84.2">
      <span class="ltx_text" id="A3.T7.3.84.2.1" style="font-size:90%;">
       iVQA
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.84.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.84.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.85">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.85.1">
      <cite class="ltx_cite ltx_citemacro_cite">
       <span class="ltx_text" id="A3.T7.3.85.1.1.1" style="font-size:90%;">
        [
       </span>
       <a class="ltx_ref" href="#bib.bib69" title="">
        69
       </a>
       <span class="ltx_text" id="A3.T7.3.85.1.2.2" style="font-size:90%;">
        ]
       </span>
      </cite>
     </td>
     <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T7.3.85.2">
      <span class="ltx_text" id="A3.T7.3.85.2.1" style="font-size:90%;">
       It is a video QA dataset with mitigated language
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.85.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.85.4">
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T7.3.86">
     <td class="ltx_td ltx_align_right" id="A3.T7.3.86.1">
      <span class="ltx_text" id="A3.T7.3.86.1.1" style="font-size:90%;">
       biases, containing 6K training samples.
      </span>
     </td>
     <td class="ltx_td ltx_align_left" id="A3.T7.3.86.2">
      <span class="ltx_text" id="A3.T7.3.86.2.1" style="font-size:90%;">
       Single-turn
      </span>
     </td>
     <td class="ltx_td" id="A3.T7.3.86.3">
     </td>
     <td class="ltx_td" id="A3.T7.3.86.4">
     </td>
    </tr>
   </table>
  </figure>
  <figure class="ltx_table" id="A3.T8">
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_table">
     Table 8:
    </span>
    Details of prompt templates used in supervised instruction tuning of SEED-LLaMA.
   </figcaption>
   <table class="ltx_tabular ltx_centering ltx_align_middle" id="A3.T8.1">
    <tr class="ltx_tr" id="A3.T8.1.1">
     <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A3.T8.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      Type
     </td>
     <td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.1.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      Prompt
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T8.1.2">
     <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A3.T8.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      Text-to-Image Generation
     </td>
     <td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.1.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      USER: {caption} Please generation an image.\nASSISTANT: {image}
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T8.1.3">
     <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A3.T8.1.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      <table class="ltx_tabular ltx_align_middle" id="A3.T8.1.3.1.1">
       <tr class="ltx_tr" id="A3.T8.1.3.1.1.1">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.3.1.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         Multimodal Prompt
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T8.1.3.1.1.2">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.3.1.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         Image Generation
        </td>
       </tr>
      </table>
     </td>
     <td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.1.3.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      <table class="ltx_tabular ltx_align_middle" id="A3.T8.1.3.2.1">
       <tr class="ltx_tr" id="A3.T8.1.3.2.1.1">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.3.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         USER: {image1} {instruction} Please generation an image.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T8.1.3.2.1.2">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.3.2.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         \nASSISTANT: {image2}
        </td>
       </tr>
      </table>
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T8.1.4">
     <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A3.T8.1.4.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      <table class="ltx_tabular ltx_align_middle" id="A3.T8.1.4.1.1">
       <tr class="ltx_tr" id="A3.T8.1.4.1.1.1">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.4.1.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         Image Conversation
        </td>
       </tr>
      </table>
     </td>
     <td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.1.4.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      USER: {image} {question}\nASSISTANT: {answer}
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T8.1.5">
     <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A3.T8.1.5.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      <table class="ltx_tabular ltx_align_middle" id="A3.T8.1.5.1.1">
       <tr class="ltx_tr" id="A3.T8.1.5.1.1.1">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.5.1.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         Multi-Image
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T8.1.5.1.1.2">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.5.1.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         Understanding
        </td>
       </tr>
      </table>
     </td>
     <td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.1.5.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      <table class="ltx_tabular ltx_align_middle" id="A3.T8.1.5.2.1">
       <tr class="ltx_tr" id="A3.T8.1.5.2.1.1">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.5.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         USER: This is the first image. {image1} This is the second image.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T8.1.5.2.1.2">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.5.2.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         {image2} {question}\nASSISTANT: {answer}
        </td>
       </tr>
      </table>
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T8.1.6">
     <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A3.T8.1.6.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      Image Captioning
     </td>
     <td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.1.6.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      <table class="ltx_tabular ltx_align_middle" id="A3.T8.1.6.2.1">
       <tr class="ltx_tr" id="A3.T8.1.6.2.1.1">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.6.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         USER: {image} Please provide an accurate and concisedescription of
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T8.1.6.2.1.2">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.6.2.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         the given image.\nASSISTANT: {caption}
        </td>
       </tr>
      </table>
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T8.1.7">
     <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A3.T8.1.7.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      Image QA
     </td>
     <td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.1.7.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      <table class="ltx_tabular ltx_align_middle" id="A3.T8.1.7.2.1">
       <tr class="ltx_tr" id="A3.T8.1.7.2.1.1">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.7.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         USER: {image} {question} Please provide an accurate answer consisting
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T8.1.7.2.1.2">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.7.2.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         of only one word or phrase.\nASSISTANT: {answer}
        </td>
       </tr>
      </table>
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T8.1.8">
     <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A3.T8.1.8.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      Video Conversation
     </td>
     <td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.1.8.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      USER: {video} {question}\nASSISTANT: {answer}
     </td>
    </tr>
    <tr class="ltx_tr" id="A3.T8.1.9">
     <td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="A3.T8.1.9.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      Video QA
     </td>
     <td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="A3.T8.1.9.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
      <table class="ltx_tabular ltx_align_middle" id="A3.T8.1.9.2.1">
       <tr class="ltx_tr" id="A3.T8.1.9.2.1.1">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.9.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         USER: {video} {question} Please provide an accurate answer
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T8.1.9.2.1.2">
        <td class="ltx_td ltx_nopad_r ltx_align_left" id="A3.T8.1.9.2.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
         consisting of only one word or phrase.\nASSISTANT: {answer}
        </td>
       </tr>
      </table>
     </td>
    </tr>
   </table>
  </figure>
 </section>
 <section class="ltx_appendix" id="A4">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix D
   </span>
   Evaluation
  </h2>
  <section class="ltx_subsection" id="A4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     D.1
    </span>
    Benchmarks
   </h3>
   <figure class="ltx_table" id="A4.T9">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 9:
     </span>
     Summary of the evaluation benchmarks.
    </figcaption>
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A4.T9.10" style="width:433.6pt;height:177pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-25.7pt,10.5pt) scale(0.894143566235726,0.894143566235726) ;">
      <table class="ltx_tabular ltx_align_middle" id="A4.T9.10.10">
       <tr class="ltx_tr" id="A4.T9.10.10.11">
        <td class="ltx_td ltx_border_tt" id="A4.T9.10.10.11.1" style="padding-top:1pt;padding-bottom:1pt;">
        </td>
        <td class="ltx_td ltx_align_left ltx_border_tt" id="A4.T9.10.10.11.2" style="padding-top:1pt;padding-bottom:1pt;">
         Dataset
        </td>
        <td class="ltx_td ltx_align_left ltx_border_tt" id="A4.T9.10.10.11.3" style="padding-top:1pt;padding-bottom:1pt;">
         Task
        </td>
        <td class="ltx_td ltx_align_left ltx_border_tt" id="A4.T9.10.10.11.4" style="padding-top:1pt;padding-bottom:1pt;">
         Split
        </td>
        <td class="ltx_td ltx_align_left ltx_border_tt" id="A4.T9.10.10.11.5" style="padding-top:1pt;padding-bottom:1pt;">
         Metric
        </td>
       </tr>
       <tr class="ltx_tr" id="A4.T9.1.1.1">
        <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T9.1.1.1.2" rowspan="7" style="padding-top:1pt;padding-bottom:1pt;">
         <span class="ltx_text" id="A4.T9.1.1.1.2.1">
          <span class="ltx_inline-block ltx_transformed_outer" id="A4.T9.1.1.1.2.1.1" style="width:8.8pt;height:26.4pt;vertical-align:-0.0pt;">
           <span class="ltx_transformed_inner" style="width:26.4pt;transform:translate(-8.81pt,-7.83pt) rotate(-90deg) ;">
            <span class="ltx_p" id="A4.T9.1.1.1.2.1.1.1">
             Image
            </span>
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T9.1.1.1.3" style="padding-top:1pt;padding-bottom:1pt;">
         COCO
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib35" title="">
           35
          </a>
          ]
         </cite>
        </td>
        <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T9.1.1.1.4" style="padding-top:1pt;padding-bottom:1pt;">
         Text-to-Image Generation
        </td>
        <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T9.1.1.1.5" style="padding-top:1pt;padding-bottom:1pt;">
         Karpathy test
        </td>
        <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T9.1.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
         CLIP score (
         <math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T9.1.1.1.1.m1.1">
          <semantics id="A4.T9.1.1.1.1.m1.1a">
           <mo id="A4.T9.1.1.1.1.m1.1.1" stretchy="false" xref="A4.T9.1.1.1.1.m1.1.1.cmml">
            ‚Üë
           </mo>
           <annotation-xml encoding="MathML-Content" id="A4.T9.1.1.1.1.m1.1b">
            <ci id="A4.T9.1.1.1.1.m1.1.1.cmml" xref="A4.T9.1.1.1.1.m1.1.1">
             ‚Üë
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A4.T9.1.1.1.1.m1.1c">
            \uparrow
           </annotation>
          </semantics>
         </math>
         )
        </td>
       </tr>
       <tr class="ltx_tr" id="A4.T9.2.2.2">
        <td class="ltx_td ltx_align_left" id="A4.T9.2.2.2.2" style="padding-top:1pt;padding-bottom:1pt;">
         Flickr30K
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib36" title="">
           36
          </a>
          ]
         </cite>
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.2.2.2.3" style="padding-top:1pt;padding-bottom:1pt;">
         Text-to-Image Generation
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.2.2.2.4" style="padding-top:1pt;padding-bottom:1pt;">
         test
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.2.2.2.1" style="padding-top:1pt;padding-bottom:1pt;">
         CLIP score (
         <math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T9.2.2.2.1.m1.1">
          <semantics id="A4.T9.2.2.2.1.m1.1a">
           <mo id="A4.T9.2.2.2.1.m1.1.1" stretchy="false" xref="A4.T9.2.2.2.1.m1.1.1.cmml">
            ‚Üë
           </mo>
           <annotation-xml encoding="MathML-Content" id="A4.T9.2.2.2.1.m1.1b">
            <ci id="A4.T9.2.2.2.1.m1.1.1.cmml" xref="A4.T9.2.2.2.1.m1.1.1">
             ‚Üë
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A4.T9.2.2.2.1.m1.1c">
            \uparrow
           </annotation>
          </semantics>
         </math>
         )
        </td>
       </tr>
       <tr class="ltx_tr" id="A4.T9.3.3.3">
        <td class="ltx_td ltx_align_left" id="A4.T9.3.3.3.2" style="padding-top:1pt;padding-bottom:1pt;">
         COCO Caption
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib31" title="">
           31
          </a>
          ]
         </cite>
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.3.3.3.3" style="padding-top:1pt;padding-bottom:1pt;">
         Scene Description
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.3.3.3.4" style="padding-top:1pt;padding-bottom:1pt;">
         test
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.3.3.3.1" style="padding-top:1pt;padding-bottom:1pt;">
         CIDEr (
         <math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T9.3.3.3.1.m1.1">
          <semantics id="A4.T9.3.3.3.1.m1.1a">
           <mo id="A4.T9.3.3.3.1.m1.1.1" stretchy="false" xref="A4.T9.3.3.3.1.m1.1.1.cmml">
            ‚Üë
           </mo>
           <annotation-xml encoding="MathML-Content" id="A4.T9.3.3.3.1.m1.1b">
            <ci id="A4.T9.3.3.3.1.m1.1.1.cmml" xref="A4.T9.3.3.3.1.m1.1.1">
             ‚Üë
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A4.T9.3.3.3.1.m1.1c">
            \uparrow
           </annotation>
          </semantics>
         </math>
         )
        </td>
       </tr>
       <tr class="ltx_tr" id="A4.T9.4.4.4">
        <td class="ltx_td ltx_align_left" id="A4.T9.4.4.4.2" style="padding-top:1pt;padding-bottom:1pt;">
         VQAv2
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib57" title="">
           57
          </a>
          ]
         </cite>
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.4.4.4.3" style="padding-top:1pt;padding-bottom:1pt;">
         Scene Understanding QA
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.4.4.4.4" style="padding-top:1pt;padding-bottom:1pt;">
         test-dev
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.4.4.4.1" style="padding-top:1pt;padding-bottom:1pt;">
         VQA acc. (
         <math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T9.4.4.4.1.m1.1">
          <semantics id="A4.T9.4.4.4.1.m1.1a">
           <mo id="A4.T9.4.4.4.1.m1.1.1" stretchy="false" xref="A4.T9.4.4.4.1.m1.1.1.cmml">
            ‚Üë
           </mo>
           <annotation-xml encoding="MathML-Content" id="A4.T9.4.4.4.1.m1.1b">
            <ci id="A4.T9.4.4.4.1.m1.1.1.cmml" xref="A4.T9.4.4.4.1.m1.1.1">
             ‚Üë
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A4.T9.4.4.4.1.m1.1c">
            \uparrow
           </annotation>
          </semantics>
         </math>
         )
        </td>
       </tr>
       <tr class="ltx_tr" id="A4.T9.5.5.5">
        <td class="ltx_td ltx_align_left" id="A4.T9.5.5.5.2" style="padding-top:1pt;padding-bottom:1pt;">
         OKVQA
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib58" title="">
           58
          </a>
          ]
         </cite>
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.5.5.5.3" style="padding-top:1pt;padding-bottom:1pt;">
         External Knowledge QA
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.5.5.5.4" style="padding-top:1pt;padding-bottom:1pt;">
         val
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.5.5.5.1" style="padding-top:1pt;padding-bottom:1pt;">
         VQA acc. (
         <math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T9.5.5.5.1.m1.1">
          <semantics id="A4.T9.5.5.5.1.m1.1a">
           <mo id="A4.T9.5.5.5.1.m1.1.1" stretchy="false" xref="A4.T9.5.5.5.1.m1.1.1.cmml">
            ‚Üë
           </mo>
           <annotation-xml encoding="MathML-Content" id="A4.T9.5.5.5.1.m1.1b">
            <ci id="A4.T9.5.5.5.1.m1.1.1.cmml" xref="A4.T9.5.5.5.1.m1.1.1">
             ‚Üë
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A4.T9.5.5.5.1.m1.1c">
            \uparrow
           </annotation>
          </semantics>
         </math>
         )
        </td>
       </tr>
       <tr class="ltx_tr" id="A4.T9.6.6.6">
        <td class="ltx_td ltx_align_left" id="A4.T9.6.6.6.2" style="padding-top:1pt;padding-bottom:1pt;">
         VizWiz
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib61" title="">
           61
          </a>
          ]
         </cite>
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.6.6.6.3" style="padding-top:1pt;padding-bottom:1pt;">
         Scene Understanding QA
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.6.6.6.4" style="padding-top:1pt;padding-bottom:1pt;">
         test-dev
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.6.6.6.1" style="padding-top:1pt;padding-bottom:1pt;">
         VQA acc. (
         <math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T9.6.6.6.1.m1.1">
          <semantics id="A4.T9.6.6.6.1.m1.1a">
           <mo id="A4.T9.6.6.6.1.m1.1.1" stretchy="false" xref="A4.T9.6.6.6.1.m1.1.1.cmml">
            ‚Üë
           </mo>
           <annotation-xml encoding="MathML-Content" id="A4.T9.6.6.6.1.m1.1b">
            <ci id="A4.T9.6.6.6.1.m1.1.1.cmml" xref="A4.T9.6.6.6.1.m1.1.1">
             ‚Üë
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A4.T9.6.6.6.1.m1.1c">
            \uparrow
           </annotation>
          </semantics>
         </math>
         )
        </td>
       </tr>
       <tr class="ltx_tr" id="A4.T9.7.7.7">
        <td class="ltx_td ltx_align_left" id="A4.T9.7.7.7.2" style="padding-top:1pt;padding-bottom:1pt;">
         SEED-Bench
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib44" title="">
           44
          </a>
          ]
         </cite>
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.7.7.7.3" style="padding-top:1pt;padding-bottom:1pt;">
         Comprehensive QA
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.7.7.7.4" style="padding-top:1pt;padding-bottom:1pt;">
         dim 1-9
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.7.7.7.1" style="padding-top:1pt;padding-bottom:1pt;">
         MCQ acc. (
         <math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T9.7.7.7.1.m1.1">
          <semantics id="A4.T9.7.7.7.1.m1.1a">
           <mo id="A4.T9.7.7.7.1.m1.1.1" stretchy="false" xref="A4.T9.7.7.7.1.m1.1.1.cmml">
            ‚Üë
           </mo>
           <annotation-xml encoding="MathML-Content" id="A4.T9.7.7.7.1.m1.1b">
            <ci id="A4.T9.7.7.7.1.m1.1.1.cmml" xref="A4.T9.7.7.7.1.m1.1.1">
             ‚Üë
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A4.T9.7.7.7.1.m1.1c">
            \uparrow
           </annotation>
          </semantics>
         </math>
         )
        </td>
       </tr>
       <tr class="ltx_tr" id="A4.T9.8.8.8">
        <td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A4.T9.8.8.8.2" rowspan="4" style="padding-top:1pt;padding-bottom:1pt;">
         <span class="ltx_text" id="A4.T9.8.8.8.2.1">
          <span class="ltx_inline-block ltx_transformed_outer" id="A4.T9.8.8.8.2.1.1" style="width:6.9pt;height:25.3pt;vertical-align:-0.0pt;">
           <span class="ltx_transformed_inner" style="width:25.3pt;transform:translate(-9.17pt,-9.17pt) rotate(-90deg) ;">
            <span class="ltx_p" id="A4.T9.8.8.8.2.1.1.1">
             Video
            </span>
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T9.8.8.8.3" style="padding-top:1pt;padding-bottom:1pt;">
         MSVDQA
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib67" title="">
           67
          </a>
          ]
         </cite>
        </td>
        <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T9.8.8.8.4" style="padding-top:1pt;padding-bottom:1pt;">
         Event Understanding QA
        </td>
        <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T9.8.8.8.5" style="padding-top:1pt;padding-bottom:1pt;">
         test
        </td>
        <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T9.8.8.8.1" style="padding-top:1pt;padding-bottom:1pt;">
         Top-1 acc. (
         <math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T9.8.8.8.1.m1.1">
          <semantics id="A4.T9.8.8.8.1.m1.1a">
           <mo id="A4.T9.8.8.8.1.m1.1.1" stretchy="false" xref="A4.T9.8.8.8.1.m1.1.1.cmml">
            ‚Üë
           </mo>
           <annotation-xml encoding="MathML-Content" id="A4.T9.8.8.8.1.m1.1b">
            <ci id="A4.T9.8.8.8.1.m1.1.1.cmml" xref="A4.T9.8.8.8.1.m1.1.1">
             ‚Üë
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A4.T9.8.8.8.1.m1.1c">
            \uparrow
           </annotation>
          </semantics>
         </math>
         )
        </td>
       </tr>
       <tr class="ltx_tr" id="A4.T9.9.9.9">
        <td class="ltx_td ltx_align_left" id="A4.T9.9.9.9.2" style="padding-top:1pt;padding-bottom:1pt;">
         MSRVTTQA
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib68" title="">
           68
          </a>
          ]
         </cite>
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.9.9.9.3" style="padding-top:1pt;padding-bottom:1pt;">
         Event Understanding QA
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.9.9.9.4" style="padding-top:1pt;padding-bottom:1pt;">
         test
        </td>
        <td class="ltx_td ltx_align_left" id="A4.T9.9.9.9.1" style="padding-top:1pt;padding-bottom:1pt;">
         Top-1 acc. (
         <math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T9.9.9.9.1.m1.1">
          <semantics id="A4.T9.9.9.9.1.m1.1a">
           <mo id="A4.T9.9.9.9.1.m1.1.1" stretchy="false" xref="A4.T9.9.9.9.1.m1.1.1.cmml">
            ‚Üë
           </mo>
           <annotation-xml encoding="MathML-Content" id="A4.T9.9.9.9.1.m1.1b">
            <ci id="A4.T9.9.9.9.1.m1.1.1.cmml" xref="A4.T9.9.9.9.1.m1.1.1">
             ‚Üë
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A4.T9.9.9.9.1.m1.1c">
            \uparrow
           </annotation>
          </semantics>
         </math>
         )
        </td>
       </tr>
       <tr class="ltx_tr" id="A4.T9.10.10.10">
        <td class="ltx_td ltx_align_left ltx_border_bb" id="A4.T9.10.10.10.2" style="padding-top:1pt;padding-bottom:1pt;">
         NExTQA
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib69" title="">
           69
          </a>
          ]
         </cite>
        </td>
        <td class="ltx_td ltx_align_left ltx_border_bb" id="A4.T9.10.10.10.3" style="padding-top:1pt;padding-bottom:1pt;">
         Temporal/Causal QA
        </td>
        <td class="ltx_td ltx_align_left ltx_border_bb" id="A4.T9.10.10.10.4" style="padding-top:1pt;padding-bottom:1pt;">
         test
        </td>
        <td class="ltx_td ltx_align_left ltx_border_bb" id="A4.T9.10.10.10.1" style="padding-top:1pt;padding-bottom:1pt;">
         WUPS (
         <math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T9.10.10.10.1.m1.1">
          <semantics id="A4.T9.10.10.10.1.m1.1a">
           <mo id="A4.T9.10.10.10.1.m1.1.1" stretchy="false" xref="A4.T9.10.10.10.1.m1.1.1.cmml">
            ‚Üë
           </mo>
           <annotation-xml encoding="MathML-Content" id="A4.T9.10.10.10.1.m1.1b">
            <ci id="A4.T9.10.10.10.1.m1.1.1.cmml" xref="A4.T9.10.10.10.1.m1.1.1">
             ‚Üë
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A4.T9.10.10.10.1.m1.1c">
            \uparrow
           </annotation>
          </semantics>
         </math>
         )
        </td>
       </tr>
      </table>
     </span>
    </div>
   </figure>
   <div class="ltx_para" id="A4.SS1.p1">
    <p class="ltx_p" id="A4.SS1.p1.1">
     In order to assess the multimodal comprehension and image generation ability of SEED-LLaMA, we evaluate SEED-LLaMA on 10 benchmarks as shown in Tab.
     <a class="ltx_ref" href="#A4.T9" title="Table 9 ‚Ä£ D.1 Benchmarks ‚Ä£ Appendix D Evaluation ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
      <span class="ltx_text ltx_ref_tag">
       9
      </span>
     </a>
     . For the evaluation of image generation, we adopt the CLIP-ViT-L/14 to calculate the CLIP score between the ground-truth image and the generated image. When evaluating SEED-Bench, we adhere to the official guidelines, selecting the option with the highest log likelihood as the response for each multi-choice question (MCQ). For the evaluation on video tasks, we uniformly sample 4 frames for MSVDQA and MSRVTTQA, and 8 frames for NExTQA. For the other tasks, we follow the evaluation procedures in prior works
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib11" title="">
       11
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib22" title="">
       22
      </a>
      ]
     </cite>
     and either submit the results to the official server (VQAv2, VizWiz) or assess them using the official code ourselves.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     D.2
    </span>
    Prompt Templates
   </h3>
   <div class="ltx_para" id="A4.SS2.p1">
    <p class="ltx_p" id="A4.SS2.p1.1">
     We summarize the prompt templates used for evaluating SEED-LLaMA in Tab.
     <a class="ltx_ref" href="#A4.T10" title="Table 10 ‚Ä£ D.2 Prompt Templates ‚Ä£ Appendix D Evaluation ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
      <span class="ltx_text ltx_ref_tag">
       10
      </span>
     </a>
     . As the pre-trained SEED-LLaMA with size of 8B and 14B adopt different LLM (Vicuna-7B and
Llama2-chat-13B), their prompts differ accordingly.
    </p>
   </div>
   <figure class="ltx_table" id="A4.T10">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 10:
     </span>
     Summary of the prompting template for evaluating SEED-LLaMA.
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="A4.T10.1">
     <tr class="ltx_tr" id="A4.T10.1.1">
      <td class="ltx_td ltx_align_center ltx_border_tt" id="A4.T10.1.1.1" style="padding:1pt 3.0pt;">
       Model
      </td>
      <td class="ltx_td ltx_align_left ltx_border_tt" id="A4.T10.1.1.2" style="padding:1pt 3.0pt;">
       Type
      </td>
      <td class="ltx_td ltx_align_left ltx_border_tt" id="A4.T10.1.1.3" style="padding:1pt 3.0pt;">
       Template
      </td>
     </tr>
     <tr class="ltx_tr" id="A4.T10.1.2">
      <td class="ltx_td ltx_align_center ltx_border_t" id="A4.T10.1.2.1" rowspan="3" style="padding:1pt 3.0pt;">
       <span class="ltx_text" id="A4.T10.1.2.1.1">
        <span class="ltx_tabular ltx_align_middle" id="A4.T10.1.2.1.1.1">
         <span class="ltx_tr" id="A4.T10.1.2.1.1.1.1">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T10.1.2.1.1.1.1.1" style="padding:1pt 3.0pt;">
           SEED-LLaMA
          </span>
         </span>
         <span class="ltx_tr" id="A4.T10.1.2.1.1.1.2">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T10.1.2.1.1.1.2.1" style="padding:1pt 3.0pt;">
           8B
          </span>
         </span>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T10.1.2.2" style="padding:1pt 3.0pt;">
       Image Captioning
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T10.1.2.3" style="padding:1pt 3.0pt;">
       {image}
      </td>
     </tr>
     <tr class="ltx_tr" id="A4.T10.1.3">
      <td class="ltx_td ltx_align_left" id="A4.T10.1.3.1" style="padding:1pt 3.0pt;">
       Image QA
      </td>
      <td class="ltx_td ltx_align_left" id="A4.T10.1.3.2" style="padding:1pt 3.0pt;">
       <table class="ltx_tabular ltx_align_middle" id="A4.T10.1.3.2.1">
        <tr class="ltx_tr" id="A4.T10.1.3.2.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.3.2.1.1.1" style="padding:1pt 3.0pt;">
          {image}USER: {question} Please provide an accurate answer
         </td>
        </tr>
        <tr class="ltx_tr" id="A4.T10.1.3.2.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.3.2.1.2.1" style="padding:1pt 3.0pt;">
          consisting of only one word or phrase.\nASSISTANT:
         </td>
        </tr>
       </table>
      </td>
     </tr>
     <tr class="ltx_tr" id="A4.T10.1.4">
      <td class="ltx_td ltx_align_left" id="A4.T10.1.4.1" style="padding:1pt 3.0pt;">
       Video QA
      </td>
      <td class="ltx_td ltx_align_left" id="A4.T10.1.4.2" style="padding:1pt 3.0pt;">
       <table class="ltx_tabular ltx_align_middle" id="A4.T10.1.4.2.1">
        <tr class="ltx_tr" id="A4.T10.1.4.2.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.4.2.1.1.1" style="padding:1pt 3.0pt;">
          {video}USER: {question} Please provide an accurate answer
         </td>
        </tr>
        <tr class="ltx_tr" id="A4.T10.1.4.2.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.4.2.1.2.1" style="padding:1pt 3.0pt;">
          consisting of only one word or phrase.\nASSISTANT:
         </td>
        </tr>
       </table>
      </td>
     </tr>
     <tr class="ltx_tr" id="A4.T10.1.5">
      <td class="ltx_td ltx_align_center ltx_border_t" id="A4.T10.1.5.1" rowspan="3" style="padding:1pt 3.0pt;">
       <span class="ltx_text" id="A4.T10.1.5.1.1">
        <span class="ltx_tabular ltx_align_middle" id="A4.T10.1.5.1.1.1">
         <span class="ltx_tr" id="A4.T10.1.5.1.1.1.1">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T10.1.5.1.1.1.1.1" style="padding:1pt 3.0pt;">
           SEED-LLaMA
          </span>
         </span>
         <span class="ltx_tr" id="A4.T10.1.5.1.1.1.2">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T10.1.5.1.1.1.2.1" style="padding:1pt 3.0pt;">
           14B
          </span>
         </span>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T10.1.5.2" style="padding:1pt 3.0pt;">
       Image Caption
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T10.1.5.3" style="padding:1pt 3.0pt;">
       {image}
      </td>
     </tr>
     <tr class="ltx_tr" id="A4.T10.1.6">
      <td class="ltx_td ltx_align_left" id="A4.T10.1.6.1" style="padding:1pt 3.0pt;">
       Image QA
      </td>
      <td class="ltx_td ltx_align_left" id="A4.T10.1.6.2" style="padding:1pt 3.0pt;">
       <table class="ltx_tabular ltx_align_middle" id="A4.T10.1.6.2.1">
        <tr class="ltx_tr" id="A4.T10.1.6.2.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.6.2.1.1.1" style="padding:1pt 3.0pt;">
          {image} Please provide an accurate answer consisting of only
         </td>
        </tr>
        <tr class="ltx_tr" id="A4.T10.1.6.2.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.6.2.1.2.1" style="padding:1pt 3.0pt;">
          one word or phrase based on the image.\n
         </td>
        </tr>
        <tr class="ltx_tr" id="A4.T10.1.6.2.1.3">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.6.2.1.3.1" style="padding:1pt 3.0pt;">
          Question:{question} \n Answer:
         </td>
        </tr>
       </table>
      </td>
     </tr>
     <tr class="ltx_tr" id="A4.T10.1.7">
      <td class="ltx_td ltx_align_left" id="A4.T10.1.7.1" style="padding:1pt 3.0pt;">
       Video QA
      </td>
      <td class="ltx_td ltx_align_left" id="A4.T10.1.7.2" style="padding:1pt 3.0pt;">
       <table class="ltx_tabular ltx_align_middle" id="A4.T10.1.7.2.1">
        <tr class="ltx_tr" id="A4.T10.1.7.2.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.7.2.1.1.1" style="padding:1pt 3.0pt;">
          {video} Please provide an accurate answer consisting of only
         </td>
        </tr>
        <tr class="ltx_tr" id="A4.T10.1.7.2.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.7.2.1.2.1" style="padding:1pt 3.0pt;">
          one word or phrase based on the video.\n
         </td>
        </tr>
        <tr class="ltx_tr" id="A4.T10.1.7.2.1.3">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.7.2.1.3.1" style="padding:1pt 3.0pt;">
          Question:{question}\n Answer:
         </td>
        </tr>
       </table>
      </td>
     </tr>
     <tr class="ltx_tr" id="A4.T10.1.8">
      <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A4.T10.1.8.1" rowspan="3" style="padding:1pt 3.0pt;">
       <span class="ltx_text" id="A4.T10.1.8.1.1">
        <span class="ltx_tabular ltx_align_middle" id="A4.T10.1.8.1.1.1">
         <span class="ltx_tr" id="A4.T10.1.8.1.1.1.1">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T10.1.8.1.1.1.1.1" style="padding:1pt 3.0pt;">
           SEED-LLaMA-I
          </span>
         </span>
         <span class="ltx_tr" id="A4.T10.1.8.1.1.1.2">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="A4.T10.1.8.1.1.1.2.1" style="padding:1pt 3.0pt;">
           8B &amp; 14B
          </span>
         </span>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T10.1.8.2" style="padding:1pt 3.0pt;">
       Image Caption
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A4.T10.1.8.3" style="padding:1pt 3.0pt;">
       <table class="ltx_tabular ltx_align_middle" id="A4.T10.1.8.3.1">
        <tr class="ltx_tr" id="A4.T10.1.8.3.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.8.3.1.1.1" style="padding:1pt 3.0pt;">
          USER: {image}Please provide an accurate and con-
         </td>
        </tr>
        <tr class="ltx_tr" id="A4.T10.1.8.3.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.8.3.1.2.1" style="padding:1pt 3.0pt;">
          cise description of the given image.\nASSISTANT:
         </td>
        </tr>
       </table>
      </td>
     </tr>
     <tr class="ltx_tr" id="A4.T10.1.9">
      <td class="ltx_td ltx_align_left" id="A4.T10.1.9.1" style="padding:1pt 3.0pt;">
       Image QA
      </td>
      <td class="ltx_td ltx_align_left" id="A4.T10.1.9.2" style="padding:1pt 3.0pt;">
       <table class="ltx_tabular ltx_align_middle" id="A4.T10.1.9.2.1">
        <tr class="ltx_tr" id="A4.T10.1.9.2.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.9.2.1.1.1" style="padding:1pt 3.0pt;">
          USER: {image}{question} Please provide an accurate answer
         </td>
        </tr>
        <tr class="ltx_tr" id="A4.T10.1.9.2.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.9.2.1.2.1" style="padding:1pt 3.0pt;">
          consisting of only one word or phrase.\nASSISTANT:
         </td>
        </tr>
       </table>
      </td>
     </tr>
     <tr class="ltx_tr" id="A4.T10.1.10">
      <td class="ltx_td ltx_align_left ltx_border_bb" id="A4.T10.1.10.1" style="padding:1pt 3.0pt;">
       Video QA
      </td>
      <td class="ltx_td ltx_align_left ltx_border_bb" id="A4.T10.1.10.2" style="padding:1pt 3.0pt;">
       <table class="ltx_tabular ltx_align_middle" id="A4.T10.1.10.2.1">
        <tr class="ltx_tr" id="A4.T10.1.10.2.1.1">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.10.2.1.1.1" style="padding:1pt 3.0pt;">
          USER: {video}{question} Please provide an accurate answer
         </td>
        </tr>
        <tr class="ltx_tr" id="A4.T10.1.10.2.1.2">
         <td class="ltx_td ltx_nopad_r ltx_align_left" id="A4.T10.1.10.2.1.2.1" style="padding:1pt 3.0pt;">
          consisting of only one word or phrase.\nASSISTANT:
         </td>
        </tr>
       </table>
      </td>
     </tr>
    </table>
   </figure>
  </section>
 </section>
 <section class="ltx_appendix" id="A5">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix E
   </span>
   Qualitative Cases
  </h2>
  <div class="ltx_para" id="A5.p1">
   <p class="ltx_p" id="A5.p1.1">
    More examples of multi-turn in-context multimodal generation and compositional image generation are shown in Fig.
    <a class="ltx_ref" href="#A5.F9" title="Figure 9 ‚Ä£ Appendix E Qualitative Cases ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
     <span class="ltx_text ltx_ref_tag">
      9
     </span>
    </a>
    and Fig.
    <a class="ltx_ref" href="#A5.F10" title="Figure 10 ‚Ä£ Appendix E Qualitative Cases ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
     <span class="ltx_text ltx_ref_tag">
      10
     </span>
    </a>
    . Note that generating images with multimodal prompt is not an emergent ability since SEED-LLaMA is fine-tuned on corresponding paired data such as InstructPix2Pix
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib51" title="">
      51
     </a>
     ]
    </cite>
    .
We showcase qualitative examples of text-to-image generation by SEED-LLaMA in Fig.
    <a class="ltx_ref" href="#A5.F11" title="Figure 11 ‚Ä£ Appendix E Qualitative Cases ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
     <span class="ltx_text ltx_ref_tag">
      11
     </span>
    </a>
    . Given various textual descriptions, our SEED-LLaMA can generate realistic images that aligns with the text prompts. We further provide qualitative examples of multimodal comprehension by SEED-LLaMA in Fig.
    <a class="ltx_ref" href="#A5.F12" title="Figure 12 ‚Ä£ Appendix E Qualitative Cases ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
     <span class="ltx_text ltx_ref_tag">
      12
     </span>
    </a>
    , Fig.
    <a class="ltx_ref" href="#A5.F13" title="Figure 13 ‚Ä£ Appendix E Qualitative Cases ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
     <span class="ltx_text ltx_ref_tag">
      13
     </span>
    </a>
    and Fig.
    <a class="ltx_ref" href="#A5.F14" title="Figure 14 ‚Ä£ Appendix E Qualitative Cases ‚Ä£ Making LLaMA SEE and Draw with SEED Tokenizer">
     <span class="ltx_text ltx_ref_tag">
      14
     </span>
    </a>
    . SEED-LLaMA can realize in-context multi-image understanding, real-world knowledge grounding, complex reasoning, story creation and video understanding.
   </p>
  </div>
  <figure class="ltx_figure" id="A5.F9">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="445" id="A5.F9.g1" src="/html/2310.01218/assets/x9.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 9:
    </span>
    Qualitative examples of multi-turn in-context image and text
generation by SEED-LLaMA given multimodal instructions.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A5.F10">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="494" id="A5.F10.g1" src="/html/2310.01218/assets/x10.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 10:
    </span>
    Qualitative examples of compositional image generation by SEED-LLaMA.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A5.F11">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="608" id="A5.F11.g1" src="/html/2310.01218/assets/x11.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 11:
    </span>
    Qualitative examples of text-to-image generation by SEED-LLaMA.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A5.F12">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="678" id="A5.F12.g1" src="/html/2310.01218/assets/x12.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 12:
    </span>
    Qualitative examples of multimodal comprehension by SEED-LLaMA.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A5.F13">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="607" id="A5.F13.g1" src="/html/2310.01218/assets/x13.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 13:
    </span>
    Qualitative examples of multimodal comprehension by SEED-LLaMA.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A5.F14">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="661" id="A5.F14.g1" src="/html/2310.01218/assets/x14.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 14:
    </span>
    Qualitative examples of multimodal comprehension by SEED-LLaMA.
   </figcaption>
  </figure>
 </section>
</article>
